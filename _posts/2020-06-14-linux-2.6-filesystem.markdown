---
layout: post
title:  "Linux2.6：文件系统"
date:   2020-06-14 14:10:00 +0800
categories: Linux
tags: Linux 文件系统
description: 
---

*  目录
{:toc}

***

## vfs

### 源码主干文件列表

核心主干大概如下：

vfs：
* fs.h dcache.h namei.h mount.h 
* file.c filesystem.c inode.c super.c namei.c open.c read_write.c

ext2:
* ext2_fs.h ext2.h
* file.c inode.c super.c namei.c 

cache：
* buffer_head.h bio.h blkdev.h mpage.h 
* buffer.c mpage.c mbcache.c filemap.c

***
### 关于vfs能提出的问题

* 首先，为什么需要vfs？它的来源、起因是什么？它实现了什么功能，解决了什么问题，向外界提供了什么服务？

* 它是什么？它是如何实现的？它内部有哪些核心数据结构？这些数据结构是如何表达我们所关注的状态的，以及这些状态在系统运行中、在进程生灭、文件开关中如何变化，如何维持自洽？

* 在具体的实现中，vfs是如何对自身进行解构的？它在设计中建立了哪些抽象实体，哪些层，为何要如此做？它是如何进行划分的？在vfs和一个具体的文件系统之间，边界在哪里？它们是如何交互的？

* 如果用面向对象的视角来看，可以如何理解它？基类、派生类、基类属性、基类方法、基类虚方法、派生类对虚方法的实现、派生类的扩展方法、派生类的扩展属性、类静态方法、类静态属性……这些面向对象中的思想是如何在vfs中体现的？各个部分之间如何隔离、如何依赖、如何交互、如何配合？如果用面向对象语言来实现，可以如何设计？

* 站在设计者的角度来说，为何会如此设计？他当初考虑过哪些问题，哪些因素？为何是superblock、inode、dentry、file这四个基本对象？文件系统的整体抽象来源于什么？其真实含义是什么？文件呢？

* 如果要实现一个自定义文件系统，我们需要做什么？vfs给我们提供了哪些便利，又设定了哪些限制？这些东西对我们的开发有何影响？好的还是不好的？

* 考虑一个实际文件系统（例如ext2）中的文件，它向外提供的这个整体“对象”，是如何被vfs+ext2实现的？如果以它的各种操作为线，我们去做抽取，会拉出来一些什么？它是如何穿越整个文件系统层的？在它的整个生命周期中，在它的新建、打开、读、写、关闭、删除中，发生了什么？在各个层级发生了什么？

* 你可以在脑海中构建起关于文件系统层的整体图像吗？它们作为一个整体地系统运行、变化、流转，内部有各种对象的生生灭灭、流动不息，外部有各种访问者、交互者，它如何保持这种有序？

* ……

***
### 本地文件系统

**linux0.11文件系统简介**

在最开始，是只有一个本地文件系统的，其中也有superblock、inode、dentry、file这些东西，大家相互配合，工作得非常好。它们作为文件系统的基本部件，共同撑起了文件系统的大梁。它们内部是有层次关系的，各司其职。比如file就负责向外提供服务，用户程序们都是使用文件，而不会直接使用sb或者inode。

内部是怎么样的关系呢？这些基本对象都是什么含义呢？

* sb，超级块对象，描述了一个实际文件系统中的数据，其中放着的都是管理数据，也就是关于数据的数据，俗称元数据metadata，比如有inode位图，块位图，inode数组之类的。我们要管理、使用、修改一个文件系统，超级管理块就是入口。

这里可能需要注意的是，超级块对象中描述的是文件系统中的数据，而并没有描述文件系统本身。在最早的linux中，就只有一个文件系统，各种操作（这些方法加上数据结构一起才是完整地定义了该文件系统）都是写死的，我们自然要没必要对它做抽象，用一个数据结构来特意描述它。

* inode。文件的数据本身。inode不携带交互数据，它关注的是数据本身，是如何能组织起文件对应的数据流，比如如何映射到某个块设备的实际数据块。对于一个数据流，对应一个inode。

* file。文件对象，是交互之后的结果，所以它携带来交互信息，比如打开文件的方式，比如当前的位置。所以如果不同的用户用不同的姿势打开一个数据流（inode），自然就会得到不同的file对象。

* dentry。目录项，这关系到文件如何在文件系统中组织为一个整体，数量大了总不能平铺然后挨个搜索吧？效率高的方式应该是树形的，那就会有层级和目录。所以对于一个文件的访问，会是逐层递归向下的，而在原始文件系统中，目录也是一个文件，其中的数据就是（name，inode_index）的元组。我们在文件系统中找到一个文件，实际上就是根据它的路径和名字（path+name），逐层搜索，直到拿到它的inode，也就获得了拿到数据流的方法。


***
**内部关系和处理流程**

* 它们之间的关系呢？file向外，沟通了文件系统内外。inode代表了数据流，是核心。dentry是联系filename和inode的关键。sb则负责管理inode以及实际的数据。

* 它们各自附带的操作呢？对于文件，大概可以打开、定位、读、写、关闭、删除等。对于超级块，既然要管理inode，那应该要包含对inode的各种处理，比如申请、读、写、删除等，另外还有对于自身和文件系统的一些处理。


当外部程序要使用一个文件时，就可以通过系统调用打开一个文件，处理流程大概是：

* 从当前目录或者根目录出发（初始inode），根据文件路径一步一步搜索、逐层下降，在每一步都要获得目录文件对象的inode和数据，并查询到子目录的inode_index，直到最终找到目标inode。

* file对象持有inode，并且有自己的打开方式和读写位置。这样就完成了打开。

* 之后要读时，就从inode中获得文件数据块号到设备数据块好号之间的映射关系，并将dev+block传递到buffer（缓冲区层），申请buffer_head，同时调用blk_ll_rw完成块设备的读。

这样，一切都很好，相安无事。

***
### vfs的出现

**抽象与设计**

但如果要加入其他的文件系统呢？文件系统并没有唯一的标准，linux有ext2，windows有ntfs，还有很多很多其他各种各样的。

* 自然的想法是用面向对象的思想来处理——虽然文件系统的实现各不一样，但是我们对文件、文件系统的定义本质上是一样的。我们将这种本质提取出来，进行抽象，就能得到一些通用对象。这些通用对象不依赖于具体的实现，而只是从定义出发，说明了它们是什么，它们提供什么功能，而没有确定它们具体要如何实现。

* 这样，接口和实现就完成了分离。使用者只需要使用通用接口，针对接口编程，而不用管文件系统的具体实现。实际的文件系统则负责实现接口。

* 抽象出来的部分，决定了文件系统本质为何的部分，就是“通用文件模型”。这个实现了接口和通用对象的内核软件层，就是vfs。从此，使用者们只需要和vfs打交道，它会遮蔽掉所有的低层实现细节。而实际的文件系统也只需要实现vfs定下的规范接口，因为使用者们一定会如此使用它。一切又恢复了井然有序。


***
**面向对象与归一化**


面向对象的核心是什么？核心目的是什么？继承只是手段，封装和归一化才是真正的目的。在vfs中，这种归一化思想就得到了极致的体现，这种体现不仅仅体现在使用者对文件系统的使用接口上，而是体现在整个vfs系统里。

vfs是基于“通用文件模型”而构建起来的，这种“通用”不仅仅体现在对外的接口上，同时也体现在它内部的实现上。vfs是一个软件层，它基于这些通用对象（核心的就是四大天王，sb、inode、dentry、file）做了大量的处理，比如从概念上独立于文件系统，但为了效率不得不做的缓存，比如统计、锁与同步、整体管理（各种环形双向链表、哈希）等等。这些操作都是基于通用文件模型实现的，可以适用于每一个实际的文件系统。

所以从这个视角来看，vfs不只向使用者，还向文件系统本身提供了大量的服务。如果放在面向对象体系里，我们可以理解为vfs定义了某种“通用文件系统模型”，而其中有大量的操作也是被抽象而出的、基于通用对象实现的，这些方法被实现到了vfs所定义的“基类”里，进而被所有的继承者（实际文件系统）所共享。

同时，vfs作为一个层次，它也有很多“层”上的功能，这不是一个文件系统内部的，而是内核面向所有存在的文件系统所要执行的功能，这是“文件系统层”的整体功能，比如前面列举的统计。

***
**那如何得到vfs的抽象？**

这是一个有意思的问题。文件系统的实现本身就是基于抽象的，原始文件系统里，sb、inode、file、dentry本身就是根据我们数据存储的物理组织方式、根据我们的使用习惯去抽象出来的，它已经很完整了。

但是这种抽象又有着多种可能性。因为使用者关注的文件，本质上就是一个数据流，而文件系统就是组织文件、数据的方式。它本身是没有特定形式的，它是完全开放的。

在这种情况下，如何去得出一个所谓的“通用文件模型”？每一个可以完整地描述这种文件系统逻辑的抽象都可以作为通用文件模型，那我们自然要选择一个成本最小的，也就是本地的。事实上，它本身就是非常优秀的抽象和设计。

然后呢？就是将它定义为某种接口，而不是写死。在c语言中，做法就是在数据结构中附带函数指针ops了。当我们创建一个对象时，同时要定义其方法。这样，使用者就可以用统一的接口和方式使用不同的实际对象了。


***
### 兼容、封装、通用实现

**如何加入其他的文件系统？**

最终目的是让这些外来文件系统可以通过统一的接口向使用者提供一致的服务。围绕这一点，任何能实现这个目的的存在都可以被看作是一个文件系统，不管是原生的、外来的、磁盘的、网络的、虚拟的、还是其他千奇百怪的。

这带来了巨大的开放性、可能性。

对于磁盘，比如我们想要读取一个安装了ntfs文件系统的磁盘，那我们要实现的就是如何根据ntfs磁盘数据，生成我们需要的sb、inode、dentry、file，也许最终磁盘上的数据形态和这个原生骨架长得完全不一样，但没关系，vfs会遮蔽掉所有的转换、实现细节。

比如对于sysfs，它所保证的就是，对于任何一个设备文件，你可以读到你想要的数据，其他你别管我。至于我是否是用磁盘sb组织起来的，还是凭空捏造了一个inode，然后把读操作直接对应到了块设备的驱动，那跟你没关系。


***
**封装**

每一个实际的文件系统，其具体实现都被封装在了它所在的模块内部，而不会被外界所访问。这种封装产生的解藕、隔离，带来了具体实现上的巨大自由，几乎没有任何约束。你只要最后给我这几个结果，其他的随便你自己倒腾。大概就是这样。

***
**如果用面向对象思想深入看看呢？**

以其中一个对象为例，比如inode。

vfs中的inode，它的属性就类似于基类属性，inode.c中所包含的各种函数，大概就类似于基类的实例方法和基类的静态方法。它们会被所有的子类所共享。它所定义的i_ops就是虚方法，等待着子类实现，是和具体文件系统相关。

而派生类里，以ext2为例，它实现的i_ops是对虚方法的实现。它自身文件中的各种函数，则可以看作是它的扩展方法和扩展类方法（它们会在模块内部被使用，但是不会被外部调用）。

但还有一个容易忽略的，那就是ext2中的inode是有扩展属性的，数据结构为ext2_inode_info，其中内嵌了vfs_inode，通过containof宏完成转换，类似于list_head的实现。通过inode的创建，可以初步看到vfs和ext2是如何相互配合的，这是一个小专题，会额外开一个小节，可以到目录里找。

就这样，一个完整的文件系统层实现了！

***
**各种通用实现**

为了能尽可能复用代码，减轻开发者负担，内核甚至提供了很多默认的“通用实现”，比如generic_file_read之类的。之所以可以这样去提供，也在于很多具体方法实现里（比如文件的open、read、write），同样有大量的逻辑是可以基于通用对象完成的，而只有少数逻辑会涉及到特定文件系统的独有操作。

比如namei中，从filename到inode的查询，整个的lookup逻辑就是共通的，这样就可以提供默认实现，对于其中的特定操作，再用函数指针回调函数之类的补一补……

***
### 文件和数据结构

在一个文件的生命历程里，内核中的数据结构、状态会如何变化？

首先，通用模型的骨架其实是根据块设备来设计的，所以在设计里默认有一种这样的倾向——文件系统并不预设数据的所在，它没有规定原始数据必须存在于哪里，它只关注是否可以顺利拿到需要的数据。

对于典型的磁盘文件系统，文件数据在磁盘里，先抛开cache不谈，关于文件系统的数据结构里，是不包含文件数据的，它着眼的只是对文件和文件系统的管理，是如何拿到数据，而不是数据本身。

所以，当我们打开一个文件时，可能会新建inode对象、dentry对象、file对象，它们的建立意味着一条路径被建立，通过这条路径/通道，我们可以顺利地获取到data stream。文件系统本身所关注的，就是这条通道的管理。

当我们要读写数据时，就是利用这条通道，完成data从数据源到用户buf之间的转移。内核中的文件系统层，核心就是维护这条通道的运转，而不大在乎数据的来源、去向、含义（瞎诌中……）。


vfs基于这些对象做了哪些管理？

我没怎么理过，其中关系似乎错综复杂……有一些最基本的，以inode为例，要维护它的各种标识，比如dirty、lock、uptodate，还有它的count，基于它们实现同步、资源管理、数据有效判断等复杂的管理逻辑。同时应该是有统计的，比如系统中的inode总数，进程打开的文件数。可能还要以某种方式将它们组织起来，比如hash、list之类的。还有很多外围的关系，比如cache、设备文件之类的。

***
**目录项缓存**

这在linux0.11中并没有做，当时的namei处理一定很慢，要多次访问磁盘，循环解析搜索才能拿到最终的inode。一种更合理的方式当然是建立一个缓存来加速这个搜索过程，这就是目录项缓存的由来了。

如何实现呢？我没看过，不过可以猜猜。

既然是缓存，肯定有映射和搜索，方法应该是哈希或者红黑树。对于目录项，应该是没有顺序遍历之类的要求的，用红黑树似乎没多大必要，用哈希更为简单。哈希的键呢？文件路径名？似乎也可以。

内存方式呢？既然很多，那应该是有slab缓存的。是分散管理，要的时候再申请，还是类似于0.11中对于buffer的处理，集中申请一大块内存，安排上多少多少个，一起管理呢？好像都行，我感觉用slab来管效率应该也ok，还没那么麻烦。

在多cpu环境中，需要做每cpu缓存吗？不太清楚，对这一块的实现不熟悉。缓存本身肯定是面向所有cpu的，而空目录项的每cpu缓存slab本身就提供了吧？

如何同步呢？目录项是没有实体对应的，它是读出目录文件的数据后生成的，所以它应该是不需要写回的，用完可以直接丢弃。如果要更改目录，那就是修改目录文件了，不走这一条路。

还有一个问题是，如何缓存呢？比如我访问了一条新的路径，实际上是访问了一串的文件（每级目录都是文件，都有目录项对象），那只缓存最终的文件，还是缓存路径上的每一个文件呢？我猜是都缓存，访问一个缓存一个……因为使用习惯里，对于文件的访问也是有局部性的；而它最终的映射方式里，应该就是简单的filename到inode，而不会去解析中间的目录的。

***

### inode创建：VFS和EXT2是如何互相配合的

随着源代码一路看下去吧——

```c
/* 
 * ext2/namei.c
 * 在vfs的inode的ops中，有create函数指针。需要ext2完成具体实现，
 * 它会在创建新文件时被调用，功能是在给定目录节点下创建一个新的节点。
 */
struct inode_operations ext2_dir_inode_operations = {
  .create   = ext2_create,
  .lookup   = ext2_lookup,
  .link   = ext2_link,
  .unlink   = ext2_unlink,
  .symlink  = ext2_symlink,
  .mkdir    = ext2_mkdir,
  .rmdir    = ext2_rmdir,
  .mknod    = ext2_mknod,
  .rename   = ext2_rename,
        /* 其他属性 */
};


/*
 * ext2/ext2.h
 * vfs中的通用模型是inode，ext2中对它的属性进行了扩展，将inode内嵌入一个
 * ext2_inode_info对象中，可以用EXT2_I()内联函数将inode地址转换为
 * ext2_inode_info地址，实现方式list_head的方式一样，都是用container_of宏。
 */ 
struct ext2_inode_info {
  __le32  i_data[15];
  __u32 i_flags;
  __u32 i_faddr;
        /* 其他字段…… */
  struct inode  vfs_inode;
};

static inline struct ext2_inode_info *EXT2_I(struct inode *inode)
{
  return container_of(inode, struct ext2_inode_info, vfs_inode);
}


/*
 * ext2/namei.c
 * ext2_create()中，调用了ext2_new_inode()获得一个新的vfs inode，
 * 并修改了它的一些属性，比如i_op,i_fop,i_mapping->a_ops。
 * 值得注意的是，虽然拿到的是inode，但实际上是一个ext2_inode_info对象的一部分，
 * 是内嵌在它数据结构中的inode地址。
 */
static int ext2_create (struct inode * dir, struct dentry * dentry, int mode, struct nameidata *nd)
{
  struct inode * inode = ext2_new_inode (dir, mode);
  int err = PTR_ERR(inode);
  if (!IS_ERR(inode)) {
    inode->i_op = &ext2_file_inode_operations;
    inode->i_fop = &ext2_file_operations;
    if (test_opt(inode->i_sb, NOBH))
      inode->i_mapping->a_ops = &ext2_nobh_aops;
    else
      inode->i_mapping->a_ops = &ext2_aops;
    mark_inode_dirty(inode);
    err = ext2_add_nondir(dentry, inode);
  }
  return err;
}

/*
 * ext2/ialloc.c
 * 通过vfs的通用接口new_inode(sb)拿到inode，做大量的初始化。
 * 初始化可以分为两部分，inode的，和外围扩展的ei的。最后返回inode。
 */
struct inode *ext2_new_inode(struct inode *dir, int mode)
{

  sb = dir->i_sb;
  inode = new_inode(sb);

  ei = EXT2_I(inode);
  sbi = EXT2_SB(sb);
  es = sbi->s_es;
  
  进行大量的初始化、状态更新、判断……

  return inode
}

/*
 * fs/inode.c
 * vfs通用方法，传入sb，返回inode。处理过程可以分为两步：
 * 1.通过调用alloc_inode拿到inode。
 * 2.更新文件系统层中的各种管理信息，比如将inode放入合适的链表，更新统计信息等
 */
struct inode *new_inode(struct super_block *sb)
{
  
  static unsigned long last_ino;
  struct inode * inode;

  spin_lock_prefetch(&inode_lock);
  
  inode = alloc_inode(sb);
  if (inode) {
    spin_lock(&inode_lock);
    inodes_stat.nr_inodes++;
    list_add(&inode->i_list, &inode_in_use);
    list_add(&inode->i_sb_list, &sb->s_inodes);
    inode->i_ino = ++last_ino;
    inode->i_state = 0;
    spin_unlock(&inode_lock);
  }
  return inode;
}

/*
 * fs/inode.c
 * 同样为vfs中的统一处理，传入sb，返回inode。主要处理同样分为两步：
 * 1.调用sb->s_op->alloc_inode(sb)拿到inode，这个函数是ext2自己实现的。
 * 2.inode通用字段的初始化。
 */
static struct inode *alloc_inode(struct super_block *sb)
{
  static struct address_space_operations empty_aops;
  static struct inode_operations empty_iops;
  static struct file_operations empty_fops;
  struct inode *inode;

  if (sb->s_op->alloc_inode)
    inode = sb->s_op->alloc_inode(sb);
  else
    inode = (struct inode *) kmem_cache_alloc(inode_cachep, SLAB_KERNEL);

  if (inode) {
    struct address_space * const mapping = &inode->i_data;

    inode->i_sb = sb;
    inode->i_blkbits = sb->s_blocksize_bits;
    inode->i_flags = 0;

    /* ……各种操作 */

    mapping->a_ops = &empty_aops;
    mapping->host = inode;
    mapping->flags = 0;

    /* ……各种操作 */

    inode->i_mapping = mapping;
  }
  return inode;
}

/*
 * ext2/super.c
 * 最终的内存申请函数，通过slab拿到一个ext2_inode_info对象，甚至一些状态，并
 * 返回 &ei->vfs_inode.
 */
static struct inode *ext2_alloc_inode(struct super_block *sb)
{
  struct ext2_inode_info *ei;
  ei = (struct ext2_inode_info *)kmem_cache_alloc(ext2_inode_cachep, SLAB_KERNEL);
  if (!ei)
    return NULL;
#ifdef CONFIG_EXT2_FS_POSIX_ACL
  ei->i_acl = EXT2_ACL_NOT_CACHED;
  ei->i_default_acl = EXT2_ACL_NOT_CACHED;
#endif
  ei->vfs_inode.i_version = 1;
  return &ei->vfs_inode;
}

```
这是一条很有意思的线，上面是从外到内，逆序回溯的。

如果顺序梳理的话——

* ext2_alloc_inode()，ext2中实现：通过slab拿到ext2_inode_info对象，设置一些字段，并返回&ei->vfs_inode，也就是返回一个inode对象。
* alloc_inode()，vfs中实现：拿到inode对象后，对其做各种初始化，包括它自身的字段，以及相关的address_space对象。（这里有i_mapping和i_data两个字段，对于普通文件和设备文件，其指向不同）
* new_inode()，vfs中实现：对新创建的inode对象做整个文件系统层上的统一管理，比如加入某些链表，更新统计信息之类的。
* ext2_new_inode()，ext2中实现：重点在于初始化扩展部分（ei）的属性，比如决定文件数据逻辑块号和设备数据块号之间对应关系的i_data等。
* ext2_create()，ext2中实现：注入到inode_ops里，作为ext2对vfs inode通用模型的方法实现。

层层调用，vfs/ext2相互调用、紧密配合，最终实现了一个完整的文件系统功能。其他的通用对象的各种方法，实现和方式都和这个类似。

***
### 一些主题

**文件系统的注册**

注册一个文件系统，是将这个文件系统type放到文件系统里，这个操作一般是在文件系统模块的init函数中完成的。先有这个type，才能挂载这个type的文件系统，内核才知道如何处理它。

一个文件系统type需要提供一些什么？核心就是name、get_sb。拿到sb对象，就拿到了相关的数据和方法，之后的其他对象、操作都可以顺藤摸瓜拿到了。

***
**文件系统的挂载**

挂载呢？linux的操作系统真是一个神奇的操作，可以玩得非常花里胡哨。比如可以用挂系统来隐藏文件，一个系统可以挂多个地方，甚至还可以挂到自己的目录底下来进行套娃……

这种挂载的开放性一方面带来了自由，一方面也增加了实现的复杂度，在各种各样的操作里，几乎都要考虑到这个目录项是否是一个挂载点，是否需要做文件系统的切换……

这个里面的很多细节就不深究了，比如操作系统对于文件系统的整体管理，如何组织在内核的数据结构里等等。

***
**根文件系统**

大概意思是先搞了个空目录的虚拟文件系统，然后在里面再挂载上实际的根文件系统，这样就只需要做一次初始化，之后可以各种切换不同的根文件系统……同样没有深究。

***
**ext2和ext3**

非常粗略地浏览了一番。和原初还是有很大的不同。

ext2，引入了块组，预分配等提高效率、健壮性的功能。

ext3，加入了非常重要的日志功能。日志功能其实很好理解，本质上是用二次冗余来实现操作的完整性、原子性，进而得到数据的一致性。当然，日志的实现是很复杂的……

***
## 页高速缓存

### 关于页高速缓存的问题

同样可以先提问题。

* 为何会引入页高速缓存？它解决了什么问题？提供了什么服务？向谁提供的？对外接口是怎么样的？它是如何被使用的？在系统中它处于何位置？它的存在环境是怎么样的？

* 它的实现原理是怎么样的？它有哪些核心数据结构？内部状态如何变化？

* 在linux0.11中，只有缓冲区高速缓存（bh），为何后来引入了页高速缓存？两者有何区别？现在是如何让两者共存的？

* 页高速缓存和vfs、和文件系统之间的关系是怎么样的？和设备之间的关系又是怎么样的？

* 为何只有块设备要引入缓存，字符设备不用？为何在处理块设备文件时，要有那么多的特殊处理？

* 页高速缓存和内存子系统是何关系？如何交互？它所持有的内存页是如何动态变化的？是位于内核地址空间还是用户地址空间？低端区还是高端区？会生长到多大？如何申请？如何释放？

* 页高速缓存是如何管理自己的数据页的？如何保持数据的有效？如何同步到磁盘？如何搜索脏页？什么时候写回到磁盘？定时检查？内存警告？还是内核线程？

* 在文件系统的操作中（比如文件的读写），是如何利用页高速缓存的？处理流程是怎么样的？页高速缓存是如何从设备中读取数据的？

***
### 从缓冲区缓存到页高速缓存

**页高速缓存**

在linux0.11中，只有缓冲区高速缓存，没有页高速缓存。也就是说，缓存的基本单位全部是块，和磁盘之间的通信也是以块为单位的，这样的效率肯定不高。

新实现中，缓存是基于页的。页是个内核管理数据非常自然的单位，因为这是内存的基本单位。而页高速缓存，就是内核实现的，用内存作为磁盘数据的缓存，从而减少磁盘访问次数，提高系统效率。所以以页为基本单位，就是自然的，有很多便利，比如它的申请、释放等等。


同时，和缓冲区高速缓存的另一个不同是，bh本质上是面向设备的数据块的，而页高速缓存是面向文件的。0.11中所有的buffer都集中放置在一起，然后以dev+block为key实现哈希。而新的实现里，页是分散的，不会集中分配，而是在需要的时候经过伙伴系统向内核申请。拿到的页也不再是统一管理，而是归属于文件，也就是inode，inode有i_mapping字段，是address_space类型，描述的就是文件的高速缓存。

所以所有的缓存页，都一定是属于某一个文件的，这个文件可能是普通的磁盘文件，也可能是设备文件，但一定是文件。当我们访问一个文件的数据时，就会先从它的缓存开始找起，如果有，就直接用；如果没有，就会触发底层的读，同时写进缓存，再返回。

为何要用这种方式来组织？应该是效率。从内存使用上来说，在需要的时候再动态分配是最好的，提前分配一个区，不但没用的时候浪费严重，而且也不好动态扩展，不能完全利用内存。而且如果都放在一起，那页数多了以后（一定会多，如果内存中有4g的缓存，4k大小的页，那就是1M数量的页），搜索效率一定会降低，哈希虽然是O(1)，但那是负载不重的时候。这么多数量的页，你得弄个多大的槽才能控制碰撞数？而且这个数量是动态增长的，还要做动态扩容，各种麻烦……

以文件来分成一坨一坨的呢？这非常自然，因为既然是对文件数据的缓存，那自然对缓存的访问是从文件的访问开始的，文件持有缓存的入口也就再合适不过了。

***
**一个文件的高速缓存是如何组织的？**

2.6的实现里是基树。为何用基树而不是哈希？或者其他的？我不知道哈哈，但我可以猜猜。文件很特别，它可能本身非常大，也可能非常小，还可能动态地变大变小。如果用哈希实现的话，如何合理地选用槽数呢？还要做动态修改？基树在动态调整深度方面是非常优秀的，而且在访问时也非常稳定。（胡诌中……）


***
**页高速缓存是如何与内存子系统交互的？**

首先建立缓存以后，它指向什么？那当然是page页描述符（老容易直接想成是内存页地址，页描述符它不是更香吗？）同时，page中也早就安排好了关于cache的字段，有mapping和index。

内存申请大概是走伙伴系统，一次一个页的申请。申请之后由cache持有，在上面读和写，在合适的时候检查dirty位，并写回到磁盘。

一个问题是，page的count应该如何设置？当一个count被加入到一个mapping里时，count应该要加一，以表示page在使用，不能被其他人占用。如果多个mapping都指向它（共享文件），那count就会大于1。当page从mapping中移除时，则会减一。可能这里最需要注意的是，page从mapping中移除，并不需要文件被关闭。无论文件是否被关闭，任何一个cache页都是可以从mapping中移除的，只需要把该写回的页写回就可以了。

这种写回和移除发生在什么时候呢？可能的情况应该有不少。比如过去的时间很长了，比如内核线程的定时刷新同步，比如内存不够了等等。

申请的内存，位于哪个区？首先一定是放在内核地址空间里，用户程序是看不见页高速缓存的（当然可以有技术直接传送到用户buf，但这是两回事）。放在低端区似乎没有多大必要，而且这东西会越长越大，低端区迟早容不下它，所以应该是放在高端内存区比较靠谱。


***
**缓冲区高速缓存**

在2.6中，缓冲区高速缓存是基于页高速缓存实现的。保留它是有必要的，因为一个页可能包含多个块，而这多个块可能在磁盘中不连续，那就需要一个块一个块地读，在维持映射关系时也更为复杂。

对于在磁盘中连续存储的页，这种映射关系是简单的：page中有mapping和index，可以轻易转换为dev和起始block号。对于不连续的页，则要对其中每一个块都维护一个buffer_head，这在读取的时候就麻烦得多，效率低得多了。所以ext2实现了预分配，一次性给多个连续的块，一读读一长段。

除了这种不连续存储之外，还有一些数据必须一块一块地读。比如一个磁盘文件系统的超级块对象，在读取它之前，文件系统都还没建立起来，它也不属于任何一个普通文件，所以只能以设备文件的方式去读取它。这个里面有挺多细节的，没有深究。


总的说来，页高速缓存应该是比较简单、比较好理解的，但里面有很多细节。

***
### 内存映射

为何要搞内存映射？其他文件不知道，但是对于可执行文件，这种做法的好处是非常多的。典型的是，在调用exec()装载一个新的可执行文件时，我们只需要根据文件头和段表、重定位表啥的，分配线性区，并关联到文件，而不用实际载入，也不用实际分配内存。只有等到真正访问时，触发page_fault后，才会在缺页异常中完成内存分配。当可执行文件很大时，这种做法对内存效率是非常重要的。

可以大概梳理一下这个过程：

* 映射一个文件，在进程的虚拟地址空间中分配线性区，映射到这个文件。线性区建立以后，对其中地址的访问就是合法的。但是这个映射关系的建立并没有真正载入文件，也没有分配内存页，而仅仅是一种合法声明，当然，页表项自然也是空着的。

* 当进程访问这个线性区中的地址时，因为页表项无效，就会触发缺页异常。在异常处理函数中，我们会做非常复杂的判断，比如当前是内核态还是用户态，这个地址是否位于合法的线性区，读写权限是否正常等等。

* 如果判断不合法，进程会被杀死。杀死的过程一般是通过信号机制完成的，典型的就是断错误信号。如果是内核错误，那可能要停机。

* 如果判断合法，那就要区分是否为匿名线性区。如果是匿名线性区，则申请一个空白页，重置页表，然后重新读。如果是映射到文件，那就要触发文件的装载，需要分配新的物理页，载入文件数据，重置页表，再重新读。


这是典型的利用异常机制来提高资源使用效率的例子。要注意的是，内存映射和页高速缓存，虽然都是将文件的数据装到内存页里，但是两者不同：

* 内存映射是基于页高速缓存的。文件数据还是要被装载到页里，然后映射到用户的线性地址，那这个物理页在哪里？它当然可以就位于页高速缓存里。如果没有特别的要求，我们当然没有必要重新复制一份。

* 基于高速缓存，可以容易地实现映射的共享。当有多个进程映射同一个文件时，线性地址都映射到高速缓存中的数据页就可以了。

* 如果某个进程要修改呢？那就重新申请一个页，将数据复制过去，然后更改自己的页表，再重新访问。也就是结束了共享，以后它就访问自己的私有页了。


这个过程有一些有意思的，值得讨论的点：

* 为何可以这么共享，又为何可以这样私有化？内存映射之后，进程访问的是不是文件，而是它的线性空间内容。这对于进程而言，区别非常大。比如对于文件的写肯定是要写回的，对于线性空间内容的写，应该是不要写回的。比如对于文件的访问，是要到内核中绕圈的，但是对于自身空间的访问呢？是直接通过页表的，不需要陷入内核。

* 所以内存映射以后，进程本质上是对自身线性空间的访问，只是可能要到文件中拿取数据而已。一旦页分配，页表被填充，就完全不用从那边绕了。

* 那共享呢？其实是内核玩的小手段，就类似于新fork出来的子进程会共享父进程的数据页一样。只是恰好这些数据是从文件中读取出来以后，自然地就存在于页高速缓存里，所以就拿它作为共享的副本。同时页高速缓存是联系到inode的，而不同的进程映射到同一文件时，指向的inode是同一个，所以自然地就开始了……

***





