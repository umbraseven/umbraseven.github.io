---
layout: post
title:  "Linux2.6：内存管理"
date:   2020-06-04 09:10:00 +0800
categories: Linux
tags: Linux 内存
description: 
---

*  目录
{:toc}

***

内存管理是一个综合性的话题。可以分为几个部分：页框管理、内核线性地址管理、进程线性地址管理，以及从线性地址到页框物理地址之间的映射关系、寻址方式。


## 内存寻址

内存可以有多种寻址方式，这在《操作系统公开课》里已经有相关的论述，比如对于分段、分页，实模式、保护模式等的讨论。

在linux2.6中，页表的分级不再是二级，而是四级。在64bit系统中，pgd/pud/pmd/pte/page，分别是9/9/9/9/12，一共为48位；如果是32bit系统，分别是10/0/0/10/12。

***
### 实模式与保护模式

这里的模式主要指的是计算机的编址、寻址模式。

**实模式**

最开始，计算机只能硬编码，程序员写什么地址就是什么地址，后来出现了“实模式”。在实模式中，程序中用到的地址仍然为真实的物理地址，但是通过将地址分段的做法，实现了重定向功能。实模式的编址规则如下：

真实地址 = 段基址\*16 + 段偏移

这个计算是通过cpu底层实现的，段基址\*16就是将它左移4位，最后的结果是一个20位的数，最大寻址空间为1M（2^20），最大段长度为64KB（2^16）。

实模式最大的缺点是不安全。程序员可以任意修改自己的段基址，可以访问任意的物理地址，甚至包括操作系统的内核空间。

***
**保护模式**

保护模式的运行机制为：
* 程序员访问的是逻辑地址，而不再是真实的物理地址。逻辑地址中包含两部分：段选择符；段内偏移。通过段选择符，可以在GDT（全局描述符表）中找到对应的段描述符。
* 段描述符为8字节，其中包含了32位的段基址，20位的段限长，12位的参数配置，包括各种权限、模式配置。32位地址线，最大寻址空间为4G。
* 因为GDT是由操作系统来管理的，所以可以对进程权限、能访问的地址等进行控制，并可以为不同进程定义不同的权限等级，实现保护。
* 操作系统刚启动时，用实模式运行，需要调用某些命令切换为保护模式。


***
## 页框管理

### page描述符

物理内存的每一个页框都会对应一个page描述符，长为32字节，约占总内存的1/100，存放在一个很大的数组mem_map[]中。

所以，拿到了page描述符，就能根据它的地址和mem_map[]的地址，计算得到它在mem_map中的索引pfn，再平移12位（page_shift)，就得到了它的物理地址。

那么，在page描述符中，需要放一些什么字段？要回答这个问题，我们先需要思考，我们要如何来管理一个页框。

首先是它的物理地址和虚拟地址。物理地址可以直接换算拿到，不需要存储；虚拟地址则有任意可能，所以只能存着。

其次是它的标志和属性，比如一些保护位、count计数、是否在高速缓存之类的。

还有就是管理数据结构，一般是通过插入一个双向链表头。


还有一些问题，比如：

我们需要知道是哪个进程拥有它吗？事实上，这是不可能的，因为页可能被多个进程所共享。同样的，进程地址空间的虚拟地址也不会被记录，page里存放的虚拟地址仅仅是内核地址空间中的虚拟地址。


我们在什么时候需要使用、如何使用page描述符？

page描述符只是用于页框的管理，当我们真正要访问这个页框时，肯定是通过线性地址-mmu-物理地址-地址总线的方式去访问，而不是通过page描述符。事实上，mem_map[]存在于低端内存，位置应该早已固定，全程不会改变。

那我们如何通过page描述符去管理页框呢？

page描述符的最简化版本，其实是linux0.11中的一个int数，表示page的count。所以，page最根本的用途，就是对于页框进行计数。这样在分配内存时，就可以区分哪些页框是空闲的，可以用于分配。其他字段则是根据需要慢慢增加的。

同样，在内核中需要传递这些页框时，相对于直接传递页框地址，传递page描述符是等价的，而且更为方便。它包含了页框的物理地址。

物理地址虽然不能用来寻址，但是它可以用来填写页表表项。所以一个可能的物理内存分配流程是：

申请内存-返回page-根据page计算得到物理地址-填入pte-通过页表/mmu访问。

***
### 分节点和分区

物理内存在属性上并不是完全一致的。

比如在numa模型中，各个cpu在内存寻址上有一定的独立性（这缓解了内存总线竞争的问题），但这也同时导致了，不同的cpu都有自己的优势区域。我们要尽量让它的访问落在自己的区域里，可以大大提高访问效率。

如何实现呢？就是要将内存访问不同的区域，每一个区域是一个节点。


在一个节点内部，所有的内存都同样处理吗？也不行。还要再分区。

一个原因是dma访问。在某些架构里，dma只能访问最低的16M物理内存（它不经过mmu），所以对这一部分内存要做特别的保留。

另一个原因则是内核的线性地址有限（32位系统），所以要分为normal和highmen区。


在常用的分区模式中，每一个区都有一个zone数据结构进行管理。它的字段中，很重要的一个是水位“watermark”，标志着这个区里物理内存的耗损量。

在分配物理内存时，需要综合考虑各个区的水位情况，让每个区都有足够的存量。

***
### 分配页：alloc_pages()

这是分配物理页框的底层函数，几乎所有其他的内存分配函数都是基于它来实现的。

传入参数：order、zonelist、gfp_mask。


如何响应内核的申请，分配给它足够多的连续物理页框呢？

首先是要分节点分区，不会跨节点、跨区分配。在每一个区里，都有各自的管理数据。我们需要在分配函数中传入zonelist，在分配中会按序扫描，根据水位等信息，综合决定使用哪个区。

其次是在一个节点里，如何避免外碎片（页框散乱分布不连续）呢？一个好用的方式是伙伴系统，尽可能在每一次分配中，保持整块的内存。

这个函数返回的是page而不是线性地址，所以它可以用于申请任意区的内存。

***
### 伙伴系统：buddy system

伙伴系统是一种管理物理页框的算法。在每一个zone里，都会维持一个数组，类似于：page* buddy_list[order]

记录在每一个order上的page空闲链表。当进行内存分配上，尽量在低的order上分配，如果不得不超出，就对高等级order进行切分，并将剩余部分进行下移。同样在释放的时候，进行循环结合，尽可能组成高等级order（大块）的整块内存。


***
### 每cpu页框缓存

伙伴系统在避免外碎片、维持页框环境上很好，但问题就是它太慢了，有很多循环，大动干戈。

为了尽量避免消耗，内核对单页框进行了缓存——一次性从伙伴系统上拿出一堆单页，放到缓存池里。当申请单页时，直接从缓存池里拿；释放时，放回缓存池。如果有必要，可以动态地增减缓存池，和伙伴系统进行交互。

同时，为了尽可能提高硬件高速缓存的效率，这种缓存最好是每cpu的。并且在设计时，应该尽可能利用“热”的页（hot page），也就是一个页越晚被释放，就越可能仍然存在于硬件高速缓存里。

这就是每cpu页框缓存。


值得注意的是：

* 每cpu缓存同样是从伙伴系统里拿到页框的，但是不是通过alloc_pages，而是rmqueue_bulk()，可以一次性拿一堆。但是值得注意的是，这个过程并不会更改页的状态，比如count之类的，而仅仅是从伙伴系统中，转移到了每cpu缓存池里。

* 每cpu缓存同样是分区的。事实上，每个zone除了有buddy的空闲链表，还有pageset，当申请单页时，pcp = &zone->pageset[get_cpu()].pcp[cold]，大概就是这样。

***
## 内核地址空间

### 内核页表：3-4G

**进程和内核分享空间**

在32位系统中，内核所拥有的线性地址区间是有限的，只有1G；其他3G则留给了进程地址空间。为什么？

为什么要让内核和进程分享，而不是各自拥有完整的4G呢？

如果各自拥有4G，那么当一个进程陷入内核时，就必须有页表的切换，而进程陷入内核是如此地频繁，这会导致很多的性能损失。另外，如果使用不同的页表，那么内核就完全无法访问进程的地址空间了，而在某些系统调用中，内核是必须要访问的——比如某些流设备中，要通过系统调用传给内核buffer地址，让内核将拿到的字符流写入buffer。毫无疑问，进程所拥有的buffer地址一定是进程空间中的线性地址，一定是要通过进程的页表才可以访问。

有意思的是，即使这个线性地址所映射到的物理地址位于低端内存区，也就是物理映射区，内核也没办法通过内核页表进行访问——因为不通过进程页表，内核也不知道这个线性地址所对应的物理地址是多少。


***
**内核页表**

既然空间一分为二，页表当然也不同，内核页表对应3-4G空间的映射。内核页表的初始化是在进程0中完成的，相应的内存数据结构是init_mm。

进程页表拥有内核页表的拷贝。注意，这很重要，它拥有内核页表的拷贝。但是这种拷贝只有第一层。比如对于10/10/12的分布，pgd刚好占用一页，其中有1024项；pgd的每一项都指向一个页表pte，其中有1024项；pte中的每一项则指向一个大小为4k的物理页。

那么对于进程来说，它的pgd仍然是完整的一页，其中有1024项。它的前3g（768项）是独有的；后1g（256项）则是从init_mm中复制过来的，和内核页表指向同样的pte。注意，并没有复制pte，pte仍然只有一份，但是所有进程页表都指向它。

这个复制是在初始化时完成的。


***
**使用页表**

在这样的分布下，当进程处于用户态时，所产生的线性地址都落在0-3g内，可以直接使用页表进行访问。当进入内核态时，线性地址落在3-4g内，同样可以直接用进程的页表进行访问，不需要切换页表，从某种意义上，可以视为“进程在内核态中执行”。

但是仍然有一些问题。

首先是内核线程的问题。内核线程没有用户空间，怎么办？内核线程既然只使用内核空间，那任意进程的页表都足够让它使用了，所以当切换到内核线程时，根本就不需要切换页表，只需要简单地将active_mm指向上一个进程的页表即可。事实上，内核线程的mm字段为空。

其次是内核页表的同步问题。当我们在一个进程中更改了内核页表，如何保证它能及时同步到所有进程中去呢？解决办法是，当我们修改内核页表时，都是直接修改原始文件init_mm，而不修改当前进程的内核页表。如果触发了访问错误，再来检查当前进程的内核页表部分是否需要同步。而在释放内核页表的项时，并不会删除掉pte表格，而只是将其对应项清0。如果一个进程访问一个已经清0的项，那表示有编程错误（因为内核自身不应该有这样自相矛盾的操作，即使是在不同的进程空间中访问，它们的逻辑也是统一的，不应该访问已经释放的页）。

另外一种不需要切换页表的情况就是当多个进程共享地址空间时（也就是多线程模型时）。


***
**一个疑惑：为什么内核产生的地址在3-4g？**

其实这个问题和编译、链接、装载相关，就像可以控制启动程序产生的地址是0x7c000，代码地址在0x8048……之类的一样，在编译链接的重定位阶段，可以让内核主程序位于0xc0000000的内核空间内。

（这一部分需要先复习一下相关知识，再来补充完善）


***
### 低端内存区

因为内核只有1g的线性地址，又要管理可能多达4g的物理内存，那怎么办呢？这就决定了一定要预留出来一部分线性地址区间，来做动态映射。（这也是动态资源管理的通用思路——需要的时候分配，使用完了就回收，而不是静态分配，静态映射好。）

内核空间可以分成两个大块，0-896M和896-1024M，前者一般称为低端内存区，后者一般称为高端内存区。

对于低端内存区，采用静态映射的方法，在内核运行全程都不会改变，也就是3g+896M的线性地址，连续、顺序地映射到0-896M的物理地址。这种方式的好处是在分配、访问它们时，不需要更改内核页表。因为更改内核页表的开销很大（会刷新tlb，污染缓存）。

对于高端内存区，则要采用动态的方式进行管理，后面再讨论。



这里需要仔细分辨的一点是：映射不等于分配。

对于低端内存区，虽然在构建内核页表时就已经完成了映射，但并没有分配。这个思想在linux0.11中就有体现——初始内核页表映射了全部的16M内存，但只有前面装着代码数据的部分被标记为占有，这是通过将mem_map[]中的count置1来实现的。在现代linux系统中，仍然是这样。映射只是表征着建立了静态的页表，等到真正分配时，仍然需要扫描mem_map数组，查找空闲页，然后修改它的属性，标记为占有。

所以事实上，虽然内核完整地映射了低端物理内存区，但这并不代表内核占有了它们，也不代表内核需要占有它们。事实上，用户进程仍然可以申请它们（和linux0.11类似，如果加上高端内存区，我们可以视为内核页表映射了全部的物理地址空间，但这不代表用户进程就没有内存可以用了）。

当用户进程申请了低端内存之后呢？会修改用户进程的页表，会修改mem_map中的page描述符表征它被占有。此时虽然在内核页表中仍然存在该映射，但内核并不会通过该页表来访问，因为内核并没有申请和分配这一块物理内存，相应的线性地址自然也不会被使用——当然之所以可以这样做，是因为内核信任自己，这种方式对于用户空间就不行。


***
### 高端内存区


**内核和进程对不同内存区域的使用**

所谓的高端内存区，既指线性地址，也指物理内存。但是在物理上，内存的高低没有任何区别；但是因为线性地址的限定（线性地址低端区只能映射到低端内存区），所以在使用上就有了区别。

对于进程而言，低端内存或者高端内存区别不大。因为进程线性地址足够，空间足够大，没有什么静态映射、动态映射，所以它使用任何位置的内存，都不会造成进程页表的颠簸。只有对于内核而言，两者才有显著的区别——使用低端内存区时，可以完全不用更改页表；使用高端内存区时，则会导致页表的更新和tlb的刷新，而这又会导致潜在的内核寻址缺页错误。

所以如果是进程申请内存（在缺页错误里），一般是这样的：page = alloc_page_vma(GFP_HIGHUSER, vma, address)。可以看到传入的标志是highuser，也就是表示可以使用highmen，分配的时候就会优先分配高端内存。


***
**永久映射/临时映射**

永久映射区很小，临时映射区也不大，它们位于线性地址的尾部。

永久映射区的管理很完备（其实我还没大明白为什么会如此设计）。它一般只有一个页表，也就是对应4m空间，1024页。它的管理方式是哈希表，可以将pfn映射到其中一项，所以可以根据物理地址或者page描述符，得到它的线性地址。同时它也为高端内存区里唯一可以用page_address()查询线性地址的区域。

临时映射区是用完即仍，即无效化。每一种内核成分都可以在其中找到自己的一项，注意，一类指向同一项，所以用完后就失效，因为会被不断覆盖。临时映射区的优点是任意时刻，对于任何成分，都绝对可用。因为它默认当你进入时，上一个使用的数据已经无效了。

因为这个特点，所以在使用临时映射区时不能被打断，它使用在中断、可延迟函数、持有自旋锁等情况之下。


永久映射和临时映射使用的函数是kmap和kmap_atomic。值得注意的是，它们只映射，不分配。所以在使用的时候，可能是通过alloc_page分配，再用kmap完成映射。


***
**非连续内存区：vmalloc**

非连续内存区的特点是，可以将不连续的物理地址，映射到连续的线性地址。这和永久映射区不同，在永久映射区，是一次申请一页，集中放在一个页表内。而非连续内存区，往往是针对要一次性获得多个连续的线性页面，但又不想占用连续的大块物理内存。

和永久映射相对，也有一个vmap专门用来映射。而vmalloc则是实实在在的分配+映射。

值得注意的是：

vmalloc可能会失败。因为vm_area有限，可能需要等待别人释放。vmalloc分配的物理地址没法用page_address获得线性地址。


***
## slab

### 内核如何使用内存？

一个有意思的问题是，内核要使用多少内存？如何使用内存？

内核使用的第一部分内存，是内核的静态code和数据，当我们启动操作系统时，这一部分的数据会被装载。意外的是，这一部分可能是很小的。事实上，ubuntu4.0的linux_kernel只有几个M……它们应该是被直接装载在物理内存的开始区域（不是很确定，因为这会占用dma区，这个位置应该可以调整的，比如mem_map结构的初始化，应该是在扫描整个内存之后完成的，它很大，肯定不会放在dma区里）。

内核使用的大量内存都是随着内核的运行动态分配的，会有哪些呢？



首先在代码部分，光有kernel应该是不行的，应该还有其他的模块需要加载。这些模块似乎是用vmalloc加载在非连续内存区。

其次是数据部分。有大量的管理数据都需要动态分配，比如内存管理、进程管理、文件管理、io及设备管理等等。典型的有task_struct、mm_struct、页表、file、inode、page，还有各种缓存区。

在内核中，似乎大部分申请内存都是通过slab层完成的，因为很多申请都是重复式的。当然slab层的底层所依赖的是伙伴系统，会一次性拿到连续的多个页框。但也有例外，比如页表恰好是一页，所以是直接通过alloc_page申请的。


我们可以想象，这些内存是随着内核的运行，进程的不断开启，逐渐分配的。它们会在伙伴系统的控制下，散步在整个低端内存区（slab运行于低端内存区），并且尽可能保证大块内存的完整。而进程运行时，则尽量在高端内存区分配，以保留低端内存区（因为随时都会有新的进程运行，会打开新的文件，会需要申请新的低端内存）。


实际上，内核以非常有限的形式使用高端内存区。比如在加载模块时，会用到非连续内存区（可能是因为模块比较大，而且运行不会非常频繁）。


***
### slab缓存

slab缓存的具体实现方式这里就不多介绍了，内核用它来管理小块内存的分配和释放，减少内碎片。

slab系统可以分为专用和通用缓存。专用缓存是专门为某一种对象建立的，使用kmem_cache_create/alloc/free/destroy系列函数操作；通用缓存则是根据2^^order大小建立的，使用kmalloc/kfree函数分配和释放。

slab被内核广泛地使用，大部分内存申请，都是通过slab完成的。

slab专供内核，专门应用于低端内存区。所以它定义的slab_flag里面，是没有highmen的。常用的是slab_kernel和slab_atomic，对应于gfp_kernel和gfp_atomic。


***
### slab关键数据结构和函数

slab层大概可以分为三级：

高速缓存cache——slab链表——对象object

每一种对象对应一个高速缓存；一个cache内有会slab list3，分别是full/free/partial list，代表着其中的slab是满的/空的/部分满的，注意slab可以在这几个链表中移动，根据它的状态随时调整；一个slab会拥有一片区域，可以存放固定数量的对象，这个区域大小可能包含数个页框，slab区域中对象如何排列，也就是从对象索引index到地址的映射是固定的。

开始的时候slab为空，放在free_list中，在第一次使用时会将其移入partial_list，在全部使用完后会移入full_list。

如果所有的slab都使用完了（也就是free/partial list都为空），但仍然还在申请该对象，则会从伙伴系统申请新的空间，建立新的slab，并放入free_list。

同时，为了提高效率，还为slab层建立了每cpu缓存。它的运行机制非常简单：一次性从slab中拿出n个对象（在slab中标记为已使用），将它们的地址存放于一个数组构成的栈里；当cpu需要使用时，就从栈顶（数组尾部）弹出一个地址；当cpu释放时，则重新压入栈。实际上这种数组有两个，分别对应cold和hot。

那slab层如何管理自己的对象，记录其是否空闲呢？同样用一种非常简单而巧妙的方式：在slab描述符，有一个用数组指针实现的，简易版的“空闲链表”——如果slab可以容纳100个对象，那这个数组就有100项；数组中存放的是下一个空闲对象的索引；在slab中会存有最“新”的对象的索引。

比如最新的对象是20，在array[20]中存放的数字是50，在array[50]中存放的是38，那么这个空闲链表就是20-50-38……用这个索引再从slab区域里获得对象的真实地址。当我们分配走一个对象时，就把链表后移，将最新索引从20变为50；当我们释放一个对象时，则把它插入链表头部，比如释放33，则令array[33]=50，再将hot_entry=33，则形成了33-50-38……的空闲链表。

值得注意的是，这样实现时，还保持了冷热性，越热的对象位于空闲链表的越前面，以充分利用硬件高速缓存。

***
**kmem_cache**

其中有各种管理数据，比如对象大小，slab包含多少页框，如何着色，从slab移到本地cpu缓存的对象个数等等。

其中最重要的是kmem_list3。它中间有三个链表，list_full/list_free/list_partial，还有一些管理数据，比如空闲对象的数量。


***
**slab**

字段不多，加入list3的list，在使用对象的个数inuse，第一个对象的地址s_mem，着色偏移colouroff，空闲链表头索引free……


***
### 一些细节

**从伙伴系统获取内存**

大概的调用流程是：

kmalloc() - kmem_cache_alloc() - cache_alloc() - cache_alloc_refill() - cache_grow() - kmem_get_pages() - alloc_pages() - page_address() 


***
**对齐与着色**

为了提高高速缓存的使用效率，一般都会将对象对齐（对于小对象，通过对齐和确定合适的size，这样对象就会存在于一行之内，而不是跨行）。

但是这可能引起硬件高速缓存的颠簸——高速缓存是按行来分布的，每一行映射的物理内存是固定的，我举个可能不合实际的例子：

高速缓存每行是1k，一共有1k行，对应1m空间。那么对于第一行，它会映射到哪些物理内存呢？1m/2m/3m/4m/5m/6m……的第一行，这是固定的。

那如果所有的slab都是从slab区域的开始处排列对象，那就有可能导致大量对象映射到硬件高速缓存的同一行里，导致这一行非常的忙，即使另外的行正空闲着。这就造成了缓存的颠簸。


为了避免这一情况，引入了着色的概念。实际上它就是在对象排列时，在区域的开始处引入一个偏移。这样第一个对象的位置不再是页首，而是某个随机分配的值，不同的slab会不同。

为什么会有这样一个空间呢？因为slab对象往往都不能填满整页，免不了会有空隙利用不上。这样排布对象时就可以在这个空隙中移动，但是因为要对齐，所以移动的单位是buffer_line_size。

如果空格足够，会将slab描述符也塞入到这个空间里面来……


***
## 进程地址空间

### 线性地址与物理内存、线性区

进程地址空间毫无疑问是线性地址，那它如何使用物理内存呢？当我们建立进程的地址空间时，实际上只是开了一个空头支票，给出了某种承诺，而并没有分配实际新的物理内存。进程地址空间内的几乎所有物理内存，都是在缺页异常中完成的。所以一次分配一页，不需要物理连续。

值得注意的是，进程面对的、能看到的，始终只有线性空间，而没有物理地址。它也没有任何方法，可以直接拿到物理地址。即使拿到了物理地址也没有任何用，因为没有任何方式可以直接访问。


那进程如何管理自己的地址空间呢？page和mem_map只是用来管理物理页框，页表也只是完成一些映射和权限检查，它们都不是从进程的视角进行地址空间的管理。

现代的方式是使用线性区，它和分段有一定的相似性，和编译链接装载阶段的分段则联系更加紧密。通过使用线性区，可以将内存分为不同的区域，并且限定其范围、权限。有了这些约束条件，就可以在访问线性地址时进行对比，判断访问是否合法。

事实上，建立页表时，其权限初值就是复制的线性区的权限；而发生page_fault时，则要根据线性区的地址范围进行比对，以判定是真的缺页，还是访问了一个错误的地址。

如果是真的缺页，对于匿名区，则分配一个零页即可；对于已经映射了某个文件的页，则会在线性区中找到ops->nopage，用这个方法将文件数据载入到物理页内。（为什么这个方法可以载入？如果这个方法是某种通用的，则它必须存储某些静态变量，比如页对应的数据位于哪个文件的哪个数据块）


***
### 关键数据结构

关键数据结构有：
* 内存描述符mm_struct
* 线性区描述符vm_area_struct

对于mm_struct，既然要管理整个进程的地址空间，它的字段可想而知有一些必须项：
* 页表地址 pgd
* 各种链表 mm_list
* 各种锁
* 线性区管理相关字段：红黑树根、计数、各个区（比如堆、栈、代码、共享库等）的信息……


对于线性区描述符，则有：
* 地址区间
* 权限、属性
* 管理字段：比如链表、锁、计数……
* 红黑树相关
* ops操作方法：比如nopage的处理方法

***
**红黑树**

为什么要使用红黑树而不是普通的链表？

链表在遍历时最为高效，但是链表在修改、查询时都比较慢。而对于线性区，会有大量的搜索操作，比如确定一个地址位于哪个线性区内。如果是链表，那就是O(n)。

对于红黑树，它的插入、删除、查询都是O(logn)的。就是用起来稍微麻烦一点。

***

### 相关系统调用

内核提供了一些系统调用给用户态程序管理线性区，典型的就是brk，可以调整堆的大小。另外还有mmap之类的，可以映射文件到内存。


***
 




