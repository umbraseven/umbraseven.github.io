---
layout: post
title:  "Linux2.6：中断、信号、同步"
date:   2020-06-04 14:10:00 +0800
categories: Linux
tags: Linux 中断
description: 
---

*  目录
{:toc}

***

## 中断系统


相关源文件：

irq.c/h

interrupt.h

softirq.c

workqueue.c/h

handle.c

manage.c

entry.S

i8259.c

***
### 为何引入中断机制？

因为cpu和外设之间速度的严重不匹配。要让它们可以协同工作，有哪些办法？轮询？那毫无疑问太慢了，所以只能引入中断机制。我先忙我的，等我准备好了你再通知我，我再进行处理，虽然这可能会打断我现在的控制路径。

在这样的要求下，会对内核设计提出哪些新的要求？在没有中断的情况下，一切都是同步的，每一个控制路径都是无限往下执行，直到时间片到期（虽然这样效率很低）；有了中断以后，它们可以让自己陷入睡眠进行异步等待，它们会主动放弃cpu，即使时间片还有剩余。

没有中断的时候，除了调度和cpu异常之外，控制路径不会被打断、交错；有了中断以后，在任何位置都有可能被打断，执行一段中断处理程序，再恢复执行。

这将使得我们如何设计中断处理程序？尽可能的简单、快。尽可能不对原来的进程执行路径产生影响，不在中断里做调度，不引发异常。

***
### 分类：中断与异常

这是定义很混乱的一片区域，intel有它cpu视角的定义，linux有它os视角的定义，最后就是一片混乱。

intel的定义里，所有的中断可以分为中断intr和异常excption。两者可以简单的如下区分：后者是cpu自己引发的，比如因为编程错误、计算错误、软件中断等等，肯定发生于一条指令之后，所以它是时钟同步的；前者呢，则更多的是外部的硬件中断，它可能在任何时候出现，是异步的。

总的来说，我们的讨论和处理，也主要是以这种方式进行区分。

在异常的下面，我们又可以做一些细分，比如故障fault、陷阱trap、中止abort、编程异常。这也是intel从cpu的角度来区分的，因为相应的处理方式不同——故障一般是可以修复的，该指令需要被重新执行，比如缺页错误page fault；陷阱是需要跳过去的，指令不会重新执行；中止则一般是无法修复的，需要杀死程序；编程异常则一般是由软件触发的，一般作为软中断。

但是在linux里，分类又不一样。linux一般是根据两个维度来划分为四象限：是否为硬件中断；用户态是否可以访问。

依据这个原则，idt中的中断描述符（gate）大概分为了这么几种：intr、trap、system、task等。对于trap和intr的处理的一个显著不同是：intr处理会自动清IF标志，禁用本地的可屏蔽中断。

***
## 中断处理


在内核的处理里，中断和异常从同一种流程开始（依赖于硬件），但是在软件设计上被导向不同的方向。

### 进入中断

当cpu检测到一个中断或者异常时，会执行同一套操作：
* 确定中断号。
* 根据idtr寄存器，找到idt，找到对应表项。表项中有几个重要字段：段选择子、偏移、type、dpl等。
* 根据gdtr寄存器，找到gdt，找到段选择子对应表项。
* 权限检查。
* 判断有没有发生特权级变换，这是通过比较cs中的cpl和gdt表项的dpl来确认的。如果发生了特权级变换，则要进行栈的切换，从用户栈切换为内核栈。具体操作是从进程的tss里拿到esp0，给ss/esp寄存器赋值，这就切换到了内核栈，再将旧的ss/esp值保存到内核栈上。
* 如果type是fault，则用引起异常的指令地址装载cs/eip寄存器，因为这个指令需要重新执行。
* 保存eflags、cs、eip到栈上（内核栈）。
* 如果异常有出错码error code，则将它压入栈。
* 装载新的cs/eip值到寄存器。这是经过idt/gdt查表后得到的。

所有利用idt处理的中断/异常，都是上面的流程。其中有一些值得讨论的点：

***
### 权限检查

权限检查需要考虑一些特别的因素。首先是idt作为所谓的“gate”，就是拿来穿越的，本来一个中断对应一个处理函数，可能只有32位，但是gate描述符是64位。其中16位段选择子，32位偏移，16位属性，其中就有属于它的type和dpl。

所以最后访问的地址的段，其实还是gdt所决定的，idt只是一个“gate”，作为中转。

权限检查的规则很复杂。比如在用户态下，有些门可以访问，有些门不能访问（用户程序可能会用int指令伪造某些中断）；在内核态下呢，同样是有些门可以访问有些门不能访问。如何设计规则呢？

基本的规则是，对于idt，必须cpl<=dpl；对于gdt，必须cpl>=dpl。也就是说，当cpl为3时，我们可以通过设置idt中的dpl，让它可以通过某些特定的门，这就是system门，它的dpl为3，而trap/intr的dpl都为0，用户态无法触发；同时，在gdt中，内核段的dpl为0，用户段的dpl为3，这样使得，如果不经过中断机制，用户程序是无法直接访问内核段的，而内核状态下，也无法通过中断机制跳转到用户代码去。

***
### 栈的切换与嵌套

这种处理方式里，已经隐含了嵌套的可行性。我们只有在检查到有特权级变化的时候，才会将栈从用户态切换为内核态，否则就简单地维持它不变，只切换eflags/cs/eip。同样，当我们通过iret或者sysexit之类的返回时，我们也可以通过确认特权级，决定是否要切换ss/esp。

这样的处理方式可行吗？

其实控制路径的切换，就是这么的简单。我们在编程里，其实早就面对过类似的控制路径切换，那就是函数调用。在函数调用里，我们并没有切换esp，而是用一个call指令切换了eip，之后通过ret指令返回。而关于寄存器的保存之类的，则是通过某些调用协议约定好，手动编程处理的。

这就是控制路径的切换。我们在执行一个函数的过程里，跳转到另外一个函数去执行，执行完之后再跳转回来。

在中断里，不也是一样吗？我们暂停当下的工作，通过手动编程保存好各种通用寄存器/段寄存器的值，再切换到另外一段的代码去执行。至于esp，如果同样是在内核段，我们可以什么都不做，就类似于调用函数一样，当中断处理程序执行完毕后，它自然会恢复之前的样子。

我们在熟悉进程切换的概念后，反而容易将控制路径的切换看得过于“重”，其实它本身是非常轻巧的。

在这种设计下，中断是允许嵌套的，就类似于在内核栈上函数的嵌套调用一样，每次中断触发都是一个call，最后通过iret返回；在处理过程里，它会保证栈的一致性。


***
### 中断机制的设计

虽然中断允许嵌套，但并不是中断异常可以随意互相打断。它们有着特定的规则。

总的规则是：

中断可以打断中断、异常、内核线程执行流、用户进程执行流，也就是基本可以打断一切，因为它本身在任何时刻都有可能出现。

异常则只会在用户程序里触发，唯一的例外是页错误，它可能在内核态被触发，但它不会嵌套，因为页错误处理程序不会再次引发页错误，而且所有的中断处理程序不会引发页错误。所以，我们可以认为异常主要就是打断用户态控制流的。

中断的这种特性决定了中断的设计理念：因为我们无法预知它会打断什么，所以我们应该尽可能地将它执行完，恢复到原来的控制路径。要允许中断进行嵌套，在一个中断发生时，允许其他类型的中断进入，提高硬件效率。

所以中断处理程序要尽可能短，在这样的要求下，我们将中断处理分成了两个部分，前半部分处理紧急的事情，比如有严格时限的任务，或者和硬件相关的任何；其他的则全部放到后半部分。上半部会在中断发生时就开始执行，其间会禁用掉全局的同类型中断；下半部则在之后的某个合适的时间执行（会尽快，但没有保证），会开放同类型中断的接入。在最理想的设计里，是将所有的工作都放到下半部。


***
### 中断的各种保护机制

中断的“禁用”有很多种方式。

第一种是禁用掉本地cpu的所有可屏蔽中断（典型指令cli）。比如一个异常处理程序中，要访问和中断处理程序有共享的全局变量，或者持有一个

第二种是禁用掉某条irq线，在所有cpu中都禁用掉该类型的中断。这在中断处理里往往是必须的，因为硬件只有一个，在处理中往往不可重入。

还有各种保护机制：

比如在内核中广泛应用的自旋锁。

***
### 异常处理

它们的入口、整体流程都是定义在汇编文件里（比如entry.S），因为其中会涉及到save/restor寄存器的工作，或者会手动在栈里布置某些参数，最后还要调用iret等等。但是它们大多是同一个模式，套路都是类似的，所以这些代码一般都可以重用。（具体流程就省略了，非常简单）

真正的处理函数则一般是高级C函数，命名一般是do_handle_name。

***
## irq子系统

### irq子系统

中断（intr）的处理要比普通的异常处理复杂得多。

引起这种复杂性的原因有很多，比如中断子系统必须是开放的，从而允许硬件扩展；其次硬件的种类繁多，要想办法做到兼容，硬件的数量也是不可知的等等。

但是中断仍然是用idt框架，在其中预留了一段给中断使用（一般是从32开始）。它的整体流程仍然是清晰的：根据中断号，找到它的描述符irq_desc，并依次调用其中的irqaction。但是实现起来是复杂而繁琐的。  

在中断处理里，引入了irq的概念。我们将所有的硬件中断请求都统一为irq，用统一的方式处理。实际上，就连idt中的描述符和它们分别指向的执行函数，也都是用统一的方式处理的。

irq有它的硬件中断号n，可以通过idt找到它的中断处理函数interrupt[n]，其中的执行逻辑都是一样，最终调用do_irq()，只是都会将自己的中断号n写入到参数regs里，这样在do_irq()中可以进行分辨。

从某种角度来看，irq子系统和systemcall子系统是类似的，都是对于同一种类型的中断，用一个模式来处理，加入一些面向对象思想，尽量实现代码的重用以及接口的简洁。


***
### irq系统的核心数据结构

irq_desc_t/irqaction是两个核心的数据结构。do_irq()仍然是用一套模子来处理，不同irq之所以处理的不同，就在这些数据结构上。

每一条irq线，都会对应一个自己的irq_desc。在desc里，除了action链表，还有关键的hw_irq_controller，以及lock、status、count等控制字段。aciton是响应irq需要执行的操作，它之所以是链表，是因为irq可以共享。

hw_irq_controller则是irq的硬件控制程序，比如8259A就定义里自己的一套，其中有name和一套预定义了格式的函数指针，用来对irq线做通用操作，比如enable、disable、ack、end等等。其中有着非常典型的面向对象设计思想，它就像一个虚基类或者一个纯粹的interface，不同的硬件可以继承接口，有自己的实现。而对于上层，则可以用统一的方式进行处理。

irqaction则是我们要真正做的处理，其中有关键的handler处理函数，有next指针，设备名字name，dev_id等等。


***
### do_irq()

核心函数是do_irq()，内部会有各种封装调用。具体流程可以参考书本，我们只讨论一些有意思的点：

**cpu同步**

其中有很复杂的cpu同步机制，因为当一条irq线正在被处理时，它不应该再在其他cpu上被处理。但是因为某些复杂原因（APIC？），其他cpu可能还会接到，所以要通过各种标志进行判断和同步。

中间还有可能发生中断的丢失，需要挽回。

因为中断处理程序不会在多cpu上并发，它自己又不会被自己中断，所以handle一定是串行执行的。这很重要，这样使得handle不需要是可重入的，它一定不会重入。


在进入中断处理程序之前，cpu已经清除了IF标志，也就是禁用了本地中断；在中断处理程序的开始，我们会检查irq->status的标志，如果不需要屏蔽，就会打开中断，以允许中断嵌套，提供系统效率。

在irq处理的开始，就会通过hw_irq_controller->ack()方法，将irq线禁用。对于8259A，就是将irq线对应的那一位置1，写入硬件端口。

***
**request_irq()**

作为一个驱动程序开发者，如何在实际编程中使用irq系统？

假设我们的计算机新加入了一个设备，在我们将设备接入之后应该可以得到它的设备id（应该是动态的）。之后我们可以通过reqeust获取一个irq号，将这个设备的irqaciton注册到这条irq线上。


***
**处理器间中断IPI**

这是cpu之间进行通讯的一种方式，非常有用。比如可以强制其他的cpu调用某一个函数，或者重启一次调度，或者强制刷新页表缓存tlb。


**irq处理相关**

比如APIC的多cpu分发，比如irq的共享、动态分配等等。

***
### 多内核栈的切换

如果我们启用了多内核栈（也就是将内核栈定义为4k），那么硬中断和软中断都会在另外的cpu栈上执行。这些区域是内核初始化时就已经分配好的，可以直接使用的，大小都是4kb。

在这种情况下，我们就有可能要做栈的切换。需要注意的是，这种栈的切换和中断刚发生时可能发生的用户栈-内核栈切换不同，从用户栈切换到内核栈时，这是cpu在硬件机制上自动完成的，一定会被执行的；而irq的内核栈-中断栈切换，则纯靠手打。（在新的内核实现中，switch_to的栈切换也是手动完成的）

实现的方式很简单：通过比较irq_ctx/curr_ctx来确定现在是否已经在中断上下文了，如果是则什么都不做（类似于在原始内核栈上的中断嵌套一样）；如果不是，则保存原来的esp值，做一些乱七八糟的处理，然后重复赋值esp寄存器，并且执行结束之后恢复。

我们可以想象软中断也是用类似的方式处理的。（所以，切换栈真的是再简单不过的事情了，就和切换指令指针eip一样简单）


***
## 下半部

下半部的实现方式多种多样，核心思想都是一样：尽量减少中断处理程序的执行时间，让它快速处理完，快速退出，重新打开这条irq线。所以就在它的最后，用某种方式将可以延迟的工作放到“可延迟函数”里，并唤醒它，让它可以在以后的某个时刻得到执行。

常见的方式有：软中断、tasklet、工作队列。

***
### softirq/tasklet/workqueue比较

三者之间有着明显的区别：

* softirq是静态建立的，不支持动态添加。tasklet/workqueue都可以动态添加。softirq支持多cpu上的并发，所以需要对数据做严格的保护；tasklet严格串行，不会重入。

* tasklet是基于softirq实现的。所以它们两个都是运行在中断上下文，不允许睡眠；而workqueue则是基于内核线程实现的，运行在进程上下文（但同样没有用户空间），可以睡眠和调度。

* 如何选择？大部分情况下，能用tasklet就用tasklet，它性能足够好，接口简单，不需要可重入，好处理。如果需要调度睡眠，那只能用workqueue。如果触发频率非常高，必须cpu并发（比如网络），那就用softirq。


***
### softirq

它最大的特点就是可以并发，所以效率高。它大概的实现机制是：定义某个softirq数组，其中保存着softirq_action数据结构，中间实际上就是action和data；当我们需要延迟执行时，就唤醒相对应的softirq，比如将对应的bit置位。

内核会在某些特定的时刻调用do_softirq()，它会扫描bitmap，执行需要处理的softirq。特别注意的是，它会在处理的开始将bitmap复制到局部变量，然后清零，这样其他cpu上的中断处理程序就可以再次唤醒，再次执行了。所以其中的action->handler必须是可重入的。

在哪些时刻会调用do_softirq()呢？典型的，硬件中断处理程序执行结束之后，也就是do_irq()结束的位置。或者在内核中显示调用了local_bh_enable()之类的。

当然其中有很多细节略去不谈。我们只讨论其中一个有意思的设计冲突：

软中断不会被软中断打断，但是可能被硬中断打断。软中断本身是可能重复触发的，比如网络子系统，那当我们扫描完一轮以后，是再次检查这个bitmap呢，还是直接回去，等待下次触发呢？

如果我们选择再次扫描，那么当系统负载很重的时候，可能会一直重复触发，导致用户进程饥饿；如果我们直接返回，那可能用户进程没什么事干，软中断也是在闲置着等待，cpu时间被浪费（因为至少要等待一个时钟滴答）。

采用的方式是专门的内核线程ksoftirqd。当我们执行了几个循环发现还没有完，就唤醒这个内核线程（它的优先级非常低，nice值为19），然后返回。这样可以保证用户进程不饥饿，而如果用户不忙呢，又能执行软中断不至于cpu跑idle。

***
### tasklet

softirq虽然强大，但是缺点明显。比如：使用复杂，只能静态创建，不支持模块；cpu并发，需要可重入，这会导致很复杂的数据保护；数量有限，通常情况下只有32个，2.6.11实际只使用了6个。

tasklet是基于softirq实现的，要简单很多。它利用了6个坑中的两个，提供了两种不同的优先级。它的大致思路是：我们可以把需要延迟执行的函数打包成一个tasklet_action，然后注册到softirq里的对应槽去（注意每个cpu都有一个自己的softirq系统），并将它唤醒。

这样在以后的某个时刻，内核调用do_softirq()时，就会依次处理它的tasklet_action链表。同时taskle提供了锁和标志位，使得可以在cpu之间做同步，以保证同时只在一个cpu上执行。


***
### workqueue

工作队列的最主要的特点就是运行在进程上下文，所以可以睡眠和调度。但是因为涉及到调度，毫无疑问它的开销最大，性能最差。

它的大致思路是：将需要执行的延迟函数打包为一个work，放到一个workqueue里。这个workqueue会创建一族内核线程（每个cpu一个），有活干了就唤醒它们，让它们在work_list里找活干。

同时，内核提供里一个缺省的队列：event，对于任务量不大的工作，没必要单独建一个线程，可以直接使用。

值得注意的是，这样方式的处理里，work也是串行执行的，因为它有一个pending字段，表示它是否已经被加入队列了。


从这里我们可以看出，对于同一种中断，我们是用对象的思路来处理的——它们的data和func被打包，我们如何让它们的执行串行化呢？每次这种类型的中断到来，我们都会使用同一个对象（对象可以包含状态state，用于判断它是否已经被唤醒/调度/加入/执行……），而不是去调用一个没有状态的函数。

所以我们可以想象，以工作队列为例。如果我们在某个硬件中断里将一个work加入了event；当这种类型的中断再次到来时，我们还是使用这个work，而不是重新创建一个对象。

***

## 系统调用

### POSIX、系统调用、C库

系统调用和c库到底是什么关系？我觉得需要厘清一些观念。

首先，c库和c语言接口其实是两回事，比如系统调用都进行了c形式的封装，但这和c库是两个概念。

其次，系统调用是面向内核的，是内核提供的服务接口；c库则是面向用户程序的，是编程接口，是api。两者的角度不一致，c库可以视为一个抽象层，在不同的操作系统提供的系统调用基础上，可以提供统一的api。这种规范之一就是posix。

所以，c库是更上面的一层，是在操作系统/系统调用和用户程序之间的一层，它会充分利用系统调用，但也有自己的用户态实现。


***
### 新的实现：sysenter/sysexit

为什么要搞新的实现？因为之前用int软中断实现的方式实在太慢了。为什么慢？因为软中断机制里有很多的权限检查（从idt到gdt），而且int指令似乎要断流水。

既然系统调用是如此频繁，那值得开发一个专用指令来实现它，这就是sysenter的由来。


那sysenter是如何实现的呢？基本想法就是绕过中断，用专用通道，而实现的功能仍然是类似的——保存各种状态，从用户态跳到内核态执行系统调用处理函数。流程如下：

* 在用户态进行某个函数调用（会将返回地址入栈）。
* 保存edx/ecx/ebp到用户栈。将用户态堆栈地址esp压入ebp。
* 调用sysenter指令，它会将内核态的cs/ss/eip载入相应寄存器，会将本地tss压入esp。（注意不是压入栈，而是载入esp寄存器）切换至内核态，特权级变为0。继续往下执行，也就是从eip指定的地址开始执行，这里是sysenter_entry()函数的线性地址，这是在内核初始化时设置的。
* 从tss获得esp0，载入到esp中。至此为止，cs/eip/ss/esp全部替换成了内核量。
* sti开中断。
* 保存用户态状态到内核态：ss/esp/eflags/cs/eip。其中ss/cs的值固定，esp保存在ebp中，eflags可以直接读，eip则设定为一个固定值：sysenter_return函数的线性地址。


这里需要特别注意的是，sysenter_return是运行在用户态的，当我们执行完其中代码，触发ret时，它会弹出用户栈中压入的返回地址，也就是第一步中压入的地址，从而继续执行用户程序代码。

另外，在恢复ebp的值时，使用的指令是：movl (%ebp) %ebp，这里的ebp初始值是用户栈地址，栈顶保存的就是压入的ebp的值。所以这是在内核态访问用户栈，为什么可以直接这样访问？因为共用一个空间，共用一个页表结构。那段寄存器呢？两者是否需要访问不同的段，通过不同的gdt表项进行访问？在平坦模式中，就用内核数据段去访问进程空间也是办得到的，毕竟都是0-4g拉满……

如果是在linux0.11中，那可能就要get_fs_byte之类的操作，去获取用户数据了。


***
### 参数传递和验证

sysenter方式中，参数传递和int方式差不多，也是save_all。如果参数太多，就指定一片内存区域；如果参数过长，就传指针。

用户传进内核的参数需要经过仔细的验证，尤其是地址。

验证的方式有两种，一种是判断它是否不属于内核，这样比较简单；另一种则深入判定是否合法，这需要搜索线性区。

现在好像更流行的方式是第一种，简单判定。因为其他的错误会在之后的页访问里暴露出来。

***
## 信号管理


### 信号机制

为什么要引入信号系统？

外部硬件中断是外设与内核之间的通信，系统调用是进程和内核之间的通信，异常是cpu和内核之间的通信，那信号呢？是进程与进程之间的通信。

进程之间通信有着天然的限制，比如在单cpu系统上，始终只有一个进程在执行，那这种通信必然只能是异步的，因为在给对方发送信号时，对方必然不在线。那么很自然的做法是，将这种信号保存在对方的进程描述符中的某个数据结构里，等对方被唤醒时去查看，并确定要如何处理。这就基本上是信号机制的雏形了。

所以信号是另外一种形式上的中断。但是它是异步的、和进程密切相关的。而硬件中断是同步的、与内核相关的。但信号同样需要打断正在执行的逻辑流，调入到信号处理程序，执行完后再返回。


***
### 信号的核心数据结构

因为信号是异步的通讯，所以需要设计一套数据结构和流程来保证一切可以顺利进行。作为发送者，产生信号的一方，几乎是不需要保存信号状态的；而作为接收者，处理信号的一方，则必须可以存储自己被挂起的信号。

显然，和进程相关的信息存储在进程描述符中是最方便的。在进程描述符中和信号处理相关的核心字段有：signal/sighand/pending。

p->signal
p->sighand
p->pending

其中，signal是整体管理字段，其中有各种计数、标志，以及共享挂起信号表（这是整个线程组所共享的，都需要处理的挂起信号）。sighand则是进程的信号处理函数，线程组会共用，用同一种方式处理。pending则是挂起信号表。

其中，在sighand中，有一个sigaction数组，每一项中都包含了sa_handler，这是具体的信号处理函数，用户可以自己注册；在pending中，包含一个链表和一个掩码。

signal:
shared_pending
count/	live/flags…

sighand:
sigaction[64] action
count/siglock

sigaction:
sa_handler
sa_flags	
sa_mask

sigqueue:

如图：（源自《深入理解linux内核》）

![d56cf3798ed799dd40b49a74a62c493a.png](evernotecid://DB3D5FB5-8235-49A0-BD47-308B3511E2EB/appyinxiangcom/21050468/ENResource/p3869)



***
### 信号处理流程

信号机制可以分为两个阶段：产生和传递（处理）。

**信号的产生**

产生信号是由发送者完成的，目标是写对方的进程描述符，在挂起信号队列中加入产生的信号。但是这个过程的逻辑异常复杂，因为有很多因素需要考虑，比如：是发送给单一进程，还是整个线程组；信号是什么类型，能否被捕获，是否被忽略；发送者的身份是什么，对方的状态是什么，是否需要唤醒……

产生信号的一些典型函数：
* static int specific_send_sig_info(int sig, struct siginfo * info, struct task_struct * t)
* int group_send_sig_info(int sig, struct siginfo * info, struct task_struct * p)
* static int specific_send_sig_info(int sig, struct siginfo * info, struct task_struct * t)


***
**信号的传递**

传递阶段是由接收者完成的。发送者会将接收进程的进程标志：TIF_SIGPENDING 置位，以此来标示有信号需要处理。对信号的检测发生在内核态返回用户态之时。

值得我们注意的是，信号处理程序是用户态的（stop和exit也是吗？），实际处理的方式是在系统调用/时钟中断等位置返回时，用通用处理函数的地址替换到用户返回地址，写入到内核栈中保存的eip中，在处理完之后，再用某种流程返回到用户位置继续往下执行。其中会涉及到用户栈的手动布置，这在2.6中和0.11中本质上没有什么区别，仅仅是判断的逻辑更为复杂。

在通用信号处理函数中，会从pending/shared_pending中根据掩码和sigqueue确定信号码，从sighand中确定处理方式，然后依次执行。

对于非实时信号，同一个信号只会处理一次，也就是当一个信号在执行时，会忽略到后续到来的同种信号。同时在一个信号处理完成之前，不会被打断之后再次开始处理同类信号，所以信号处理函数不需要可重入。（貌似在信号处理时还是可以被中断，可以调用信号调用，这会导致比较复杂的处理逻辑……）


***
### 关于多线程和posix

多线程：是作用于所有线程，还是单一线程？如何区分？如果是所有线程，用什么方式发送？用一个什么存在来代表整个进程？还是把所有线程都通知一遍？线程组内部如何通信，知道彼此的存在？当进程退出时如何保证全部退出？

实际上，posix详细地给出了规范……当然比较复杂，这里不详细展开了，以send_signal为例：

ret = send_signal(sig, info, p, &p->signal->shared_pending); //线程组

ret = send_signal(sig, info, t, &t->pending); //单线程

***
## 内核竞争

### 执行线程的交错

内核中有多种执行线程：中断处理程序、可延迟函数、异常处理程序（包含了为用户程序提供服务的系统调用）、内核线程等。这些执行线程之间是可能交错执行的，这就带来了数据竞争的可能。

* 执行线程交错的第一种核心方式就是打断——
  * 中断处理程序可以打断一切，包括其他的中断处理程序；可延迟函数可以打断异常处理程序和内核线程（这两者可以都看作是进程的内核态，之后不分开讨论），但不能打断中断处理程序，也不会打断自己；异常处理程序则只能打断异常处理程序，而无法打断中断处理程序或者可延迟函数。
  * 但是这种打断都是可以被禁止的——禁本地中断、禁本地可延迟函数、禁本地抢占。这都是为了防止在本地cpu上，出现执行线程的交错。

* 执行线程交错的第二种方式就是并发——
  * 在多处理器系统上，不同的cpu可以执行不同的执行线程，它们之间的执行顺序是未定义的，它们有着几乎所有的共享资源——内存、内核空间、硬件io……并发的组合也是几乎所有可能的组合，除了同类tasklet只有一个会同时执行（实际上进程肯定也只有一个cpu在执行）。

***
### 打断

我们来详细看一下内核中的各种“打断”。

**中断处理程序**

对于中断处理程序，它可能在用户态触发，也可能在内核态触发；不管从哪里来，它都会由cpu的硬件机制响应，粗暴地进入中断处理程序，打断一切。它运行在中断上下文，不代表任何进程执行，它不能等待、睡眠、调度，在它执行结束之前，不会切换到可延迟函数或者异常处理程序，所以也就不可能发生抢占或者调度。

值得注意的是，在中断处理程序里可能进行所谓的“延迟调度”，实际上只是设置了抢占标志，并不真的发生调度。它们会在中断处理程序返回时检查和执行。

但是中断处理程序会被其他的中断处理程序打断，形成中断嵌套；但是它自身并不会嵌套，也就是同一个中断处理程序同一时间只会在一个cpu上运行。所以中断处理程序不要求可重入。这种机制是通过禁用中断线、各种irq检查提供的。


***
**异常处理程序**

对于异常处理程序，它运行在进程上下文，代表某个进程执行。它可以等待、睡眠、调度。它几乎不会触发新的异常，除了page fault之外。所以它一般是从用户态进入，不管是通过系统调用还是其他异常，它的姿态一般是提供某种服务，或者作为内核线程单独执行。

它的优先级最低，可能被各种打断。首先是中断处理程序，可以像打断用户态程序一样简单打断它，但是其间并不会发生进程切换，只是开启一个新的内核执行线程，一个新的逻辑流；其次是可延迟函数，它们可能在不可预知的任何时候执行，比如中断返回时，比如从tick里触发；最后是代表其他进程的内核程序的抢占。

值得注意的是，对于进程，它的调度、睡眠、等待、抢占，都是发生在内核态。这是自然的。

***
**可延迟函数**

对于可延迟函数，它一般由中断处理程序唤起，在一个“合适”的时候执行。这个所谓“合适”，就是任何“合适”的时候。它同样运行在中断上下文，不代表任何进程，不可以等待/睡眠/调度。它同样可能被中断处理程序粗暴地打断，但它不会被“抢占”。

***
### 抢占

抢占是如何发生的？抢占三要素：允许位、抢占标志、检查点。

* 允许位是preemt_count，只有当它为0时才表示允许抢占。它可以分为三段，在进入中断、可延迟函数时，相应的段会自增，使得不为0，也就是禁用了抢占；同样我们也可以显示调用preemt_disable，它也会增加它的值；而当我们调用preemt_enable，则会就地自减并检查，如果为0且抢占标志被设置，则会触发调度。

* 抢占标志，它是一个简单的标志位，只作用于本地cpu，它可以在各种地点、被各种程序、甚至各个cpu设置。比如在中断处理程序里，当时钟tick发现当前进程的时间片到期，就会设置抢占标记；当wakeup一个process，发现它的优先级比current更高，同样会设置抢占标记；甚至于进程自身也可以设置它来进行延迟调度，比如在fork里，在wakeup里；甚至cpu之间还可以相互设置，比如wakeup一个进程的时候，发现某个cpu比较闲，就把进程放到它的队列里去，设置它的抢占标记，再给它发送个处理器间中断，这样在中断返回时就会触发重新调度，让新唤醒的进程立即得到执行……

* 检查点，最典型的检查点就是各种“返回”的时刻，它们本来就意味着执行线程、执行状态之间的切换。所谓延迟调用，就是为了不强行中断当前的执行状态，所以延迟到这种中断点来做，无疑是自然的。比如中断返回点、内核态（异常）返回点等（事实上它们并没有本质区别）。

***

从中断/异常返回时，有一个通用流程。它们能做的一个大的区分，就是从内核栈的CS来分辨是要返回到内核态还是用户态，如果是返回到用户态，则一定会检查抢占标记；如果是返回到内核态，则会先检查是否允许抢占。

这里值得注意的是，在这个流程里，是无法区分/不会区分返回的内核态所真正执行的线程的——不管是中断/可延迟函数，还是普通的内核线程，都是会执行这个流程。当然如果有中断/可延迟函数的嵌套，那它们永远不会允许抢占，自然也就不会在这个时候发生调度。


那如何禁止抢占？

* 破坏三要素。首先毫无疑问就是禁用抢占，这样即使到了检查点，也不会发生调度；其次是禁用中断，这样检查点就不存在了（没有中断返回点，我自己也不返回，就不会有检查点）。

* 但有一些细节是无法忽略的。在多cpu环境中，抢占标志是无法保证不被设置的，因为它可以被其他cpu设置；同时禁用中断只能禁用掉本地中断，当然这会同时禁止掉其他cpu发过来的处理器间中断。

* 但是单纯地禁用中断在这种环境里变得不那么可靠：如果我们只是禁用中断，而不显式禁用抢占的话，如果其他cpu设置里我们的抢占标志，而在我们的调用里，又触发了preemt_disable/preemt_enable函数对，那么在enable时会进行检测，发现preemt_count为0且抢占标记被置位，这样当前进程就会主动发起一次调度（虽然此时的task_state为running），这样虽然没有检查点，但也触发了重新调度。


***
### 保护：面向数据

当交错的执行线程共享了数据时，就可能导致潜在的竞争。竞争的含义是结果的不一致性，比如数据的最终状态依赖于执行线程的执行顺序。

为了防止数据竞争，我们需要对数据进行保护，以实现执行线程之间的同步——让它们不再各行其是，而是通过某种机制有序地对数据进行访问，从而防止竞争。所谓“同步”，就是彼此之间有商有量，有某种秩序而非混沌。

***
**面向数据而非代码**

这也许是所有的保护机制中最重要、最基础的一条。要想真正理解它并不是一件容易的事。当我们在做所谓“保护”的时候，我们真正在保护的是什么？（也许可以从有限状态机，从图灵机，从过程式语言等方面进行阐述，但我现在还有些迷糊）

来看一个实例。

**在什么场合需要同步？**

如上面讨论，在多个执行流，共享某些数据时。程序的执行可能看作是状态和计算，在图灵系语言，状态是至关重要的；如果一个计算会影响到某种状态，那就是有副作用的。从这个角度来看，可以将这些执行线程对共享变量的作用视为某种“副作用”，它们之间会相互影响。相反，如果它们只是读这些变量而不做任何更改，也就是不改变状态，没有“副作用”，那毫无疑问它们是安全的，不需要做任何同步。

当线程存在副作用时，如何让一切变得正常起来？我们需要做的就是，保证结果的一致性。如果有一致性，那这种一致性的结果就将是我们所预测的，也就是“正确的”。

* 比如几个执行线程一起完成任务，通过一个共享变量count对完成的任务进行计数。
  * count保存在内存中，当我们要对count进行更新时，典型的操作是，将它从内存中读出来，加上变化量，再写回内存。这个操作涉及到两次访问内存，不是原子的——内存总线对cpu的同步中，如果不锁内存线，那么它只会对各个cpu的单次访问进行排序。
  * 所以在多cpu的环境中，可能会出现这种情况：两个cpu同时访问count，内存总线进行排序；cpu1拿到count的值，比如为100，回去加上10；cpu2紧接着拿到count，值为100，回去加上20；cpu1先计算完，再次访问，写回count，110；cpu2再次写回，值为120。而我们预期的结果是130，而不是110或者120。
  * 我们可以通过将数据的访问“原子化”来完成数据保护。当一个cpu访问count时，拿到的值要么是100，要么是110，120，而不会是数据访问的某个中间结果。这样，不管这两个cpu以何种顺序访问count，它们的预期结果永远是130。

* 如果我们还有另外一个线程，负责周期性地将count保存到某个系统状态里去呢？
  * 事实上我们仍然对count进行原子操作保护就足够了——这个执行线程将count读到局部变量，并进行存档。在它保存的时候count可能已经变化了，但这不影响。

* 但是如果这个线程的任务是，根据count的数量，分配对应的task[]，并将count清零呢？
  * 乍一看似乎count和task有了关联，它们之间要维持某种有序，也就是要把对它们的操作“原子化”。但其实是不需要的，原子性仍然只要对count，那就是“读出count的值且将它置0”这个操作可以原子地完成。根据读出来的值分配task，而其他线程可以继续更新count就足够了。

* 如果我们将线程的任务改成：每一次更新count时，要同步更改某个数据queue的长度（这和链表的维护是一致的：每次更新链表时，更新链表中元素的数量nr）。
  * 此时，count和queue之间是要始终保持同步——任何执行线程在访问到它们时，它们都有着一致性。那么对它们的操作就需要是原子性的，才不会出现一者更改一者没有更改的混乱。在这种情况下，我们就需要新建一个数据结构，类似于：
```c
struct c_q{
	unsigned count;
    queue_t q
    lock_t lock
}
```
  * 让它们成为一个完整的数据结构，并且通过某种锁来保证同步。对于这个锁的要求是，在可能的各种交错里，始终只能有一个执行线程同时访问到它，以此来保证对它访问的原子性。

我之前在心里总会犯嘀咕，让数据访问原子性就足够了吗？执行线程就能和平相处了吗？事实上这正是程序设计需要保证的——既然这些执行线程之间能共同持有某些状态，那我们需要保证也就应该仅仅是这些状态的一致性。如果几个执行线程对状态的使用本来就会出现冲突，那么它们如何能共享呢？


从这个角度出发，我们可以窥探到很多数据结构、算法，甚至是大型程序的设计哲学。——在我现在看来，计算往往是围绕数据而展开的，所以数据结构的设计至关重要。

所以关于数据竞争的保护也由此展开——保证某种原子性。它是面向数据而非代码的。

***
### 如何实现这种原子性呢？

* 某些操作自然地就具有原子性，比如读某个变量（单次内存访问）；cpu提供了了某些原子指令，比如lock，它会锁住内存总线，比如xchg，它能原子地交换值（如果是多条指令实现，那么在中间的任意时刻都可能发生抢占，形成数据竞争）。

* 对于复杂的数据结构，我们依赖于各种“同步原语”（实际上，所有的同步都依赖于同步原语，原子操作就是其中一种），在不同的条件下， 我们需要选择不同的方式。比如原子操作，是最底层的，可以应用于多cpu的，绝对的（不考虑分布式）。

所以，我们最后使用的方法（同步原语），是要根据我们的交错条件，来选择尽可能合适的：正确是基本前提，然后是尽可能高的并发、效率、简单。

***
## 同步

防止数据竞争有两个核心思路：线程禁止、线程同步。

线程禁止，就是在我执行时，不让某些可能产生交错的线程执行。没有并发的线程自然就没有竞争。

所谓同步，就是当一个执行线程在访问数据时作出某种标记；当其他线程尝试访问时，发现这个标记后进入等待，直到数据可用。

当然等待的方式各种各样，但不外乎两类：站着等和躺着等。站着等就是什么都不干的忙等待，cpu空转；躺着等则是陷入睡眠，资源可用来再通过某种机制进行唤醒。

相对应的，在同步原语里，忙等待对应的方式就是自旋锁，所谓自旋，就是原地死循环；睡眠对应的方式就是信号量，在down里进入睡眠等待，在up里被唤醒。

但特别的是，在执行线程里，中断处理程序和可延迟函数都是不允许睡眠的，所以它们天然地拒斥信号量（暂时不讨论不陷入睡眠的try持有）。

所以我们可以自然地得出以下这些同步的原则：

***
### 自旋锁

对于单处理器系统，中断上下文不应该等待任何资源，不应该持有任何锁或者信号量——它们不能被睡眠，而忙着等自己毫无疑问是个笑话，这会导致死锁。

所以自旋锁的出现是因为cpu之间做同步——通过短暂的空转避免一次大动筋骨的进程切换是值得的；在中断上下文中也可以持有，因为持有自旋锁的代码段应该都是短暂的。

这就是自旋锁的特别之处，持有自旋锁的进程，是不允许等待、睡眠、调度、抢占、返回用户态的。它必须原子性地快速处理完并返回。如果它和中断上下文的线程共享了数据，它还必须显式地禁用中断/可延迟函数。因为在同一个cpu上两次持有自旋锁会导致死锁。

事实上，自旋锁是内核中使用最多的锁。因为中断共享数据时，没得选。而且自旋锁本身也可以非常灵活，如果粒度控制得好，是很灵活高效的。

***
### 信号量

和自旋锁相对的是信号量。和自旋锁相对应的是信号量，它的主要特点是如果不可用就进入睡眠；还有一个特点是它可以不互斥，而是可以设定信号的“量”，虽然在大多数场合，都是使用互斥形式（量为1）。

信号量只能使用在进程上下文里。但是因为它可以睡眠，所以可以用在长代码段，它也不禁用抢占，不会影响调度和响应时间。另外有意思的是，持有自旋锁的时候是不允许持有信号量的，因为可能陷入睡眠；而持有信号量的时候是可以持有自旋锁的。


在多处理器系统中，我们可以分析各种各样的执行线程交错场景。对于一个数据结构——

* 如果它被一个进程私有，或者一种tasklet使用，或者某个中断处理程序访问，那它不需要做任何同步。因为它们不可能发生交错（任意时刻都只有一个存在）。

* 如果它被多个中断处理程序共享，那在访问时必须禁止本地中断（本地cpu）+自旋锁（其他cpu），前者是线程禁止，后者是同步。

* 如果它被多个异常处理程序（进程内核态）持有，如果数据处理过程复杂，则考虑使用信号量，不需要做其他同步；如果使用自旋锁，则会自然地禁止抢占。

* 如果它被中断处理程序和异常处理程序共享，在中断处理程序中需要持有自旋锁，在异常处理程序中需要禁止本地中断，并持有自旋锁。如果有多个中断处理程序共享，那中断处理程序中也需要禁用本地中断。

* 可延迟函数的讨论和上述类似。


自旋锁和信号量只是基本形式，在它们的基础上可以做很多扩展。比如——

***
### 其他同步方式

**读写锁**

它既可以基于自旋锁实现，也可以基于信号量实现。它的基本思路是，多个读者之间不互斥；写者与其他读者、写者互斥。所以它可以通过某些巧妙的计数机制来标志当前读者/写者的数量。

当读者访问时，只要写者为0，就可以直接进入，然后更新读者的值；当写者访问时，会等到读者和写者都为0才进入。

这种方式下，读者可以并发，能显著提高访问效率；但是写者会一直等待，可能造成饥饿。

***
**顺序锁**

另外一个变种则是顺序锁，它给写者比读者更高的优先级。当读者访问时，不会做任何的同步；当写者访问时，则要做同步处理，并且在进入和退出时都增加count。这样，当count为奇数时表示有人在写，为偶数时表示无人在写。

读者在访问的开始和结束都要访问count并且对比它们的值，一旦不同就表示在它读的期间发生了写，读者就要重新读，如此循环直到读取成功。

在这种方式下，写者可以快速地完成写，但可能让读者反复读。它适用于有大量读者和少量写者，同时写者速度很快的场合。


信号量则有互斥版本，以及后来独立的“互斥量”。互斥量使用起来比信号量更为简单，但条件更为严苛——互斥量要求持有和释放是在同一个进程上下文，而信号量则允许我持有它释放。

***
**每cpu变量**

最好的同步方式就不需要同步，因为任何同步都会造成性能的损失。每cpu就是这样一种思路，将只和本地cpu相关的变量独立出来，每个cpu一份，这就就不用担心并发的问题了。

但是在访问每cpu变量时，需要禁止抢占和中断，防止本地执行线程交错引发数据竞争。

***
**屏障：barriers**

编译器会进行各种优化，比如如果我们用volatile修饰一个变量，那每一次对它的访问都会从内存，否则它会尽量从寄存器里获取它的暂态值。（这和缓存机制是类似的，对于某些硬件端口，它的值是可能从外部被改变的而寄存器缓存的值并不知道已经被更新了，就会引发数据错误）

除了这种访问的“捷径”，编译器还会根据它的“理解”，对指令进行各种优化排序。但是在某些时候，这种排序会导致潜在的竞争。我们可以通过设置屏障barriers()来防止：它所保证的是，在barrier之前的指令都执行完毕后，才会执行barrier之后的。

***
**完成量/补充原语：complete**

这是为了防止临时信号量的微妙潜在竞争而引入的——当A创建了一个临时信号量并等待，B将它唤醒后，A可能立马就将这个信号量释放了，此时回到B后，如果继续访问该信号量，就会导致悬空。

（我也没太搞懂这个）

***
**读-拷贝-更新：RCU**

据说可以实现写者的并发执行，大概的思路是，读的人直接读，写的人先复制一份数据的拷贝，更改完了以后再改变数据指针。但是关于它我有很多疑问：这不同样会导致多个写者时的竞争吗？它们可能在同一时刻读入，然后各写了一份自个的。


***
### 锁的设计原则

* 面向数据而非代码。
* 按序申请，防止死锁。
* 防止饥饿。
* 力求简单。
* 选择合适的粒度，保持尽可能高的并发。

另外，各种软件锁，最终都是基于硬件提供的原子指令实现的。事实上，通过软件也可以实现，但是会非常非常繁琐。而原子指令就是一个指令原子地执行，中间不会发生打断，所以它可以做的事一定是非常少的，对于复杂的数据结构的同步操作不可能放在一个指令里。

所以，原子指令的一大用处就是用来实现各种锁，再用锁来保护各种功能临界区。当然原子操作本身是个例外，atomic是基于原子指令直接实现的同步原语，能这么做的原因也是因为int或者bit非常简单，在指令级可以完成同步。


***




