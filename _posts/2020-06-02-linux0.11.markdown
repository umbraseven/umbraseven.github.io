---
layout: post
title:  "Linux:0.11"
date:   2020-06-03 09:10:00 +0800
categories: Linux
tags: Linux 
description: 
---

*  目录
{:toc}

***
参考书目：《Linux内核完全注释》

## 引导和初始化

### head.s

关于boot、setup、head的一些说明可以查看笔记——《操作系统公开课》笔记。


head的主要功能有：
* 重新设置idt、gdt（数据就在head.o的数据段，每个表格大小都是256\*8）；对于idt，暂时都是指向一个只显示信息“未知中断”的哑处理函数；对于gdt，手动设置了前四项，后面都填的0。

* 加载全局描述符表寄存器时，指令是 lgdt gdt_descr，其中包含的是gdt表的地址和大小，一共6字节。

* 设置页目录表和页表，开启分页模式。上一部分的代码位于文件开头，中间用.org指定为空，直到0x5000（20KB）处才是后面的代码。因为前20KB会拿来放页表结构，pg_dir指向0x0。

* 设置页表初值时，对于前面的16M（0-0xfff000），完全就是线性地址和物理地址一一对应设置的。总共占4个页表。

* 开启分页模式。手动设置cr3，cr0的值，然后用ret返回。这是因为改变分页处理标志后，需要用转移指令刷新预取指令队列（因为处理器的流水线机制，也即是让预取的作废）。


***

## main.c

main.c的核心函数时main()，它在system模块中紧接着head模块，完成初始化工作。这些工作可以分为两部分：内核的初始化；初始进程的启动。前者属于内核部分，初始化的目的是为进程的启动做好准备；后者属于内核之外，但也属于操作系统的部分，目的是利用内核提供的功能做一些准备，给用户的登录、用户进程的运行做准备。


### 内核初始化：main()

让我们想想，head模块运行结束之后，计算机进入了一个怎么样的状态？它的内存中已经装入了内核的数据段、代码段，其中包括着各个模块，比如head、main、kernel、fs、mm等等。它们在内存的前端顺序排列，占用大小不会超过640kb。

在这些内存中，代码是只读的，是已经固定好的、不需要初始化的；某些全局变量在声明时已经赋予了初值，是可以直接使用的。但很多内核用于管理的关键数据结构还是空白的，需要初始化。这就是内核初始化需要做的事——填充内核中的关键数据结构，让各个模块都为运行做好准备，并联系为一个整体。

那内核中有哪些关键数据结构？我们可以从内核模块一个一个来数：

* 进程管理模块。首先是task[]，全部要清空；gdt表的初始化（在head中完成了一部分）；需要手动构建的进程0（init_task）的ldt、tss、task_struct等数据结构。
* 中断处理模块。idt表的初始化，包括trap、intr、system等类型的gate设置。其中包含了非常关键的系统调用处理和时钟中断处理。
* 缓存区管理。初始化bh数组，组织成空闲链表，并初始化哈希表。
* 内存管理。内核页表的初始化（这在head中做了），mem_map[]的初始化。
* 硬件设备和终端。比如块设备管理的request队列初始化，字符设备的dev table、fn table初始化等。（其中还要注册硬件中断处理函数）
* 文件系统。初始化根文件系统，初始化inode_table，file_table等，并将该设备载入内存。（这是mount_root()中完成的，事实上它是以系统调用的形式被进程1调用，延后完成的。）
* 其他，比如硬件的某些初始化（通过outp指令），时间的初始化等。

当这些初始化全部完成以后，内核就进入了就绪态，可以将控制权交给进程了。如何完成这种交换呢？从高特权级到低特权级，只能通过iret实现。所以在运行初始化进程的时候，必须手动布置好堆栈，然后用iret命令，跳跃到用户态，成为进程0。

在这信仰之跃后，我们可以认为内核的运行方式从此发生了根本性的改变。在跳跃之前，内核是主动的、主宰式的；在跳跃之后，它不再持有什么，而是等待被各种中断——系统调用、异常、硬件中断等等——唤醒，并执行相关的处理。

这是一种极有美感的放弃。这也让我们深思，操作系统到底为何存在，计算机到底为何存在？计算机存在目的是为了处理某些任务（task），这些task不是内核，而是用户进程。所以计算机的核心目的，就是用户进程的执行；而操作系统的核心目的，是为了让众多进程可以有序、高效的执行。它扮演的从来都是一个管家、仆人的角色，是一个低到尘埃里的角色。它不是什么上帝，不是什么主宰，相反它是谦卑甚至卑微的。它尽心尽力地想提供最好的服务，却还是时时防止某些调皮的进程的恶意捣蛋——这很像家长宠溺着家里的一窝孩子，还要时时提防熊孩子搞破坏。

我觉得这也是进行操作系统开发的人应该秉持的一种情感、一种角度、一种信念——以一个父母、管家、仆人的姿态，去面对可能执行的每一道进程，而不是主宰者。


***
### 初始化进程：task 0

内核做好准备以后，就放弃读控制流的掌控，托付给“进程”。这就是闲散的进程0，只有它是手动构建的，它fork出进程1，进程1再发展出其他各种进程。


**task 0**

进程0是通过一个宏 move_to_user_mode() 实现的（下面有代码）。我们可以仔细地来讨论其中发生了一些什么。

首先是初始化进程0的相关数据结构。
* ldt。进程0虽然是从内核跃过来的，但它也是货真价实的进程，它同样运行在用户态，所以当然也需要ldt表。它的ldt表是手动构建的，数据段和代码段的基址都是0，限长640kb，dpl为3。也就是可以用0x0f和0x17访问。
* task_struct。这是在内核中单独划出来的一页，作为进程0的内核栈页面，一端则存放ts结构。
* tss。因为跃进来时不需要使用tss，所以tss的初始化很简单，大部分是0，只要将页表地址pg_dir、内核栈地址PAGE_SIZE+(* long)&init_task之类的赋值好就行。
* 注册进gdt，载入ldtr，tr之类的。

```c
/* ldt */
  {
  0x9f, 0xc0fa00}
  , \               // 代码长640K，基址0x0，G=1，D=1，DPL=3，P=1 TYPE=0x0a
  {
  0x9f, 0xc0f200}
  , \               // 数据长640K，基址0x0，G=1，D=1，DPL=3，P=1 TYPE=0x02
}

```
然后是人为地布置堆栈。
* 在跃入进程0之前，系统所使用的栈都是在sched模块中手动预留出来的一页，我们可以称之为系统栈。而iret命令不会管你当前的esp指向的是不是内核栈，它只需要从esp位置可以找到五虎将（ss/esp/eflags/cs/eip）就可以。（事实上，cpu在硬件层级的处理上不可能区分什么内核栈不内核栈）
* 在栈上手动压入五虎将。这几个数据非常有意思：ss的值是0x17，表示访问的是用户数据段，也就是以dpl=3的特权级，访问ldt表中的第三项ldt[2]；esp的值，就是压入五虎将之前的esp当前值，也就是说iret之后仍然使用这个栈作为进程0的用户栈，连位置都不带改；cs的值是0x0f，和ss类似；eip的值则指向iret之后的下一行代码，也就是根本没发生跳跃……
* 从上面这个过程可以看出来，iret根本就不想玩什么跳跃，它只想在原地蹦跶一下，把自己的权限级别转换为dpl3，以光明正大的通过ldt进行内存访问。


系统栈 & 进程0的用户栈：
```c
/* ldt */
  {
  0x9f, 0xc0fa00}
  , \               // 代码长640K，基址0x0，G=1，D=1，DPL=3，P=1 TYPE=0x0a
  {
  0x9f, 0xc0f200}
  , \               // 数据长640K，基址0x0，G=1，D=1，DPL=3，P=1 TYPE=0x02
}
// SCHED.C 
long user_stack [ PAGE_SIZE>>2 ] ;

struct {
    long * a;
    short b;
    } stack_start = { & user_stack [PAGE_SIZE>>2] , 0x10 };

//HEAD.S
 startup_32:
    movl $0x10,%eax
    mov %ax,%ds 
    mov %ax,%es
    mov %ax,%fs
    mov %ax,%gs 
    movl $0x10,%eax     # reload all the segment registers
    mov %ax,%ds     # after changing gdt. CS was already
    mov %ax,%es     # reloaded in setup_gdt
    mov %ax,%fs
    mov %ax,%gs
    lss _stack_start,%esp
    xorl %eax,%eax
1:  incl %eax       # check that A20 really IS enabled
    movl %eax,0x000000  # loop forever if it is not
    cmpl %eax,0x100000
    je 1b
```

最后是进程0的执行。
* 跳跃为进程0之后，它在原位置继续往下执行，也就是还在main()中。
* 它执行系统调用，fork出进程1；自己则陷入一个死循环。
* 死循环中，它不断pause()，但永不睡眠，仅仅是为了检查是否有其他进程可以执行。——这是无比崇高的做法，我永远都愿意守着cpu，但从来不会占据它。




***
### 初始化进程：task 1

进程0的主要作用就是fork出task 1，它负责执行init()，并在其中进入死循环永不返回。os内核之外的初始化准备工作，主要就是由task 1负责完成。

它的主要工作有：
* 触发系统调用setup，执行硬盘的初始化，其中包括加载根文件系统和文件系统的初始化。

* 打开tty0，并dup()两次，让fd0/1/2全部指向这一个设备。

* fork出task 2，并等待task2执行完毕。

* 在task2中，启动bash，并根据参数文件/etc/rc，执行某些初始化工作后退出。

* 进入死循环：创建一个子进程，并等待它结束。

* 子进程：重新打开终端tty，并运行bash。



***


### shell与交互

**shell**

进程1会进入死循环，为终端创建一个子进程，启动一个shell，与用户进行交互。

shell是什么？shell是一类程序的总称，是操作系统的壳，是终端用户（是“人”的用户，不是用户程序）与计算机之间的交互平台。用户可以以shell为接口，启动各种用户程序。

shell的种类有很多，比如常用的bash，它的启动路径可以被用户自定义，保存在用户参数里。一个用户通过一个shell启动的所有进程，一般可以视为一个会话session，如果一个session的leader终止，那这个session中所有的进程都会终止。比如当一个用户logout时，它启动的所有进程都应该终止掉。

所以，会话>进程组>进程。每一个进程创立时，都可以加入某个进程组。比如当我们在shell输入一个管道命令：find xxx | grep | echo >> testfile……之类的，它们就会被归结为一个进程组，会有一个组长。这种进程组的方式，方便我们可以一键将它们杀死，比如ctrl+c。

shell本身是一个非常复杂的程序，是一个脚本执行程序。它需要理解我们在终端中的输入，执行对应的命令，大概都是fork、execv、waitpid之类的。


**用户进程**

我们可以讨论下用户进程的启动、运行过程。当我们通过进程1启动一个shell时，从fork得到的page_table全部被丢弃掉；而此时ldt限定的线性地址段，比如从128-192M，在页表中还是无效的；但是我们在进程数据结构中说明了如何找到它们——如果是在code/data范围内的，可以从curret->executable中读取数据，申请并绑定一个新的物理页，并将数据写入。如果是范围外的，则申请一个空页。

所以，从shell开始，再也访问不到内核模块区域了——它的64m空间中所包含的代码和数据，都是被exec中载入的shell程序所决定的。它也不再共用之前的用户栈了，新的用户栈总在线性地址的末端。由shell再启动的子进程也是一样，不再能直接访问到内核区域。它们对于内核代码、数据的访问，必须经过系统调用接口。


**超级用户与普通用户**

超级用户的权限，和内核的权限是完全不同的两个概念。内核权限是代码级的权限，超级用户权限则主要体现为用户对系统中文件的访问权限，或者执行某些系统调用的权限。内核权限是基于cpu的特权级来组织的，是基于硬件的；超级用户权限则完全是程序设计上的，是基于软件的。



***
## 进程管理

### 重要的数据结构

```C
struct task_struct {
/* these are hardcoded - don't touch */
    long state;    /* -1 unrunnable, 0 runnable, >0 stopped */
    long counter;
    long priority;
    long signal;
    struct sigaction sigaction[32];
    long blocked;    /* bitmap of masked signals */
/* various fields */
    int exit_code;
    unsigned long start_code,end_code,end_data,brk,start_stack;
    long pid,father,pgrp,session,leader;
    unsigned short uid,euid,suid;
    unsigned short gid,egid,sgid;
    long alarm;
    long utime,stime,cutime,cstime,start_time;
    unsigned short used_math;
/* file system info */
    int tty;        /* -1 if no tty, so it must be signed */
    unsigned short umask;
    struct m_inode * pwd;
    struct m_inode * root;
    struct m_inode * executable;
    unsigned long close_on_exec;
    struct file * filp[NR_OPEN];
/* ldt for this task 0 - zero 1 - cs 2 - ds&ss */
    struct desc_struct ldt[3];
/* tss for this task */
    struct tss_struct tss;
};



struct sigaction {
    void (*sa_handler)(int);
    sigset_t sa_mask;
    int sa_flags;
    void (*sa_restorer)(void);
};


```
***


### 嵌入式汇编

基本的可以读懂就行，没必要纠结过多细节。

其实主要就是能搞清楚基本的汇编指令（这不难），然后搞清楚嵌入式的基本语法规则，尤其是输入、输出的指向，那些字母分别代表什么寄存器之类的。


***
### 段寄存器

* 启用分段机制后，对内存的访问总是要经过段。为了能快速访问，避免每次都到内存中访问描述符表，cpu有段寄存器和内存。

* 其中最核心的是三个：cs、ss、ds。cs是指令段寄存器，搭配eip。ss是栈段寄存器，搭配esp。ds则是默认的数据段寄存器。另外还有三个辅助的：es、gs、fs。

* 当我们访问一个数据时，比如movb，如果不特别指定，默认使用ds段寄存器。

* 每一个寄存器除了16位short段选择子之外，还有一个隐藏段，其中是在GDT/LDT中读出来的表项，比如段基址、限长等。所以重新设置一个段寄存器是有代价的，那会使得背后的缓存失效，需要重新载入。但是如果表项发生了改变，则一定要注意用某些指令触发缓存刷新。

* 按照linux0.11的习惯，ds、es一般指向内核数据段（0x10），fs则一般指向用户数据段（0x17）。段选择子最低二位为dpl（0-3），第三位指明是gdt/ldt，而且表项的第0项一般弃置为空。所以对于内核，cs段一般为0x08，ds/ss段为0x10（此时低三位全为0，表明dpl=0，访问的表为gdt）；对于用户，则分别为0x0f和0x17（dpl=3，表为ldt，低三位全为1）。因为每一个进程都有自己的ldt，所以都是0x17，不会冲突；而所有进程共享内核空间，所以都用0x10也不会冲突。

* 在编译期处理c代码时，默认是不会改变段寄存器的。如果是一个数据移动，那编译出来大概是movb %1, %2。如果要在内核区和用户区移动数据（只有内核可以做到），这种方式不行，因为它们位于不同的段，而mov默认的寄存器都是ds。此时就需要利用嵌入式汇编，比如get_fs_byte/put_fs_byte，手动指定为：movb %0, fs:%1。

* 我们使用段寄存器的逻辑是，尽量不要假定它们处于“正确”的值，而是手动设置值，在使用完后将它们恢复原样。所以在很多内核代码中，都有对ds、es、fs的push、mov、pop操作。

* 对ds、fs等的更改，除了显式赋值以外，某些指令也会隐性地更改它们的值。比如call，ljmp等，就会导致cs的值发生改变。

***
### gdt/ldt/idt/tss

这是几个容易搞混的表，整理一下。可能在linux不同的版本中实现都不一样，这里就只以0.11为例。

* gdt：全局描述符表，也就是全局段表。linux内核只有两个段：代码段和数据段；每一个进程私有的ldt、tss本身也是段，也会存到gdt里。（所以在linux早期版本，进程的数量是有限制的，因为gdt放不下）

* ldt：局部描述符表，也就是进程私有的段表。一个进程一般也只有两个段：cs，ds/ss段。因为ldt[0]一般置空，所以一般对应ldt[1]/ldt[2]。某种意义上，ldt可以看做gdt的二级表项。

* tss：任务状态段，同样是一个进程一个。它们也是一个单独的段，所以在gdt里也有一个单独的项。

* idt：中断描述符表。同样是一个单独的段，在gdt里有表项。idt是全局共用的，内存里仅此一份。idt中的门描述符也有很多种，中断门、陷阱门、任务门等等。在描述中有段选择子和偏移值，也就是可以通过访问门，跳转到另外的段，其中就包括特权级的改变。

* 每个进程的ldt[]/tss，实际是存储在该进程的内核页中（task_struct与kernel stack共用的那一页），它们本身就是ts的字段之一。至于为什么要把它们作为单独的段放入gdt，可能是考虑到方便查找吧。这样安排的话，就可以根据该进程在task[]中的序号n，直接用宏_TSS(n)找到它的tss入口了。

* 对于每一种表，cpu都有对应的寄存器缓存，GDTR/LDTR/IDTR/TR。


**从gdt访问idt**

在平坦模式下，访问起来的地址转换是自然的、容易理解的。在内核中，基本是它处于物理内存的哪个位置，就用对应的地址去访问，cs/ss等寄存器仅仅作为一些权限检查之类的，而不会影响最终地址。

但是如果非平坦模式，比如访问tss、idt。它们也是作为一个单独的段，在gdt中占据一个表项。那在访问它们的时候，gdt中表项的base指向的是idt、tss等表的开头，那我们使用的逻辑地址，实际上就是在表中的偏移地址，而非线性空间、或者物理空间中的地址了。


***
### 创建：fork.c

何为创建一个进程？从实际的、工程的角度来看，就是创建一个可以被os调度、可以运行的实体。

为了达成这个目标，我们需要组织一些数据、状态。类似于bh之于buffer，inode之于文件。能描述进程的各种状态、属性，并能以之为节点、句柄，访问到进程的各种数据的描述符，就是进程描述符，task_struct。在linux0.11中，创建进程的第一步，就是从内核数据段中分配一个空闲的物理页，将它作为该进程的内核栈，并且在它的页首放上这个进程的ts。（内核栈是从高地址往下生长的，所以拥有的空间不足一页，否则会损坏进程描述符的数据）

从调度的观点来看，只要一个进程有着自己的ts结构，在一些内核管理数据结构中有注册（比如task[]，gdt[]），并且ts中的核心数据有效（比如tss，ldt），这个进程就可以被调度了——调度的过程，就是在task中找到合适的task，赋值给curret指针，然后用task->tss替换掉old.tss（利用ljmp指令），当重新执行时，就进入了另一个逻辑流了。（类似于执行到某一刻时间静止，用相机拍下暂停一刻的所有状态，记录为tss，在需要的时候再重新导入，恢复时间的流动）开始执行后，对用户内存的访问则要经过ldt。（在linux0.11中，因为线性地址是分段的，所以ldt中base的不同，就直接导致了会访问到不同的页目录项，进而对应到不同的页表，不同的物理页）

所以，在fork时，我们要做的工作无非也是这些：创建ts，先直接将父进程的进程数据全部拷贝给它（* p = * current，这个里面就包含了打开的文件列表filp[]，信号处理函数表sigaction[]等等，反正很多）；然后设置一些不太重要的父子不同的字段属性，比如时间片、父子关系等。

接下来就是重头戏之一的tss，但要注意的是，这里并不是复制current.tss，而是用压入栈中的值，为什么？因为在通过系统调用陷入内核时，tss是不会刷新的，只有在进行进程切换时，才会被自动刷新。但是其中有几个特别的点：
  * tss可以认为是逻辑流（控制路径）的“断面照片”。在linux0.11做进程切换时，进程都是在内核态下，所以保存、载入的tss状态，都是内核控制路径的“断面照片”，同样在恢复时，也是恢复到内核态。
  * 经过系统调用从用户态陷入内核态时，同样有控制路径的切换，copy_process()传入的一串参数，就是这个断面照片里的各种字段属性。此时将它们赋值给tss，当这个新创建的进程首次被调度时，将会直接回到用户态。（如何突破权限检查？权限不就是设置cs的值吗……）
  * 在设置tss时，有一些比较特殊的字段，首先是eax，其中会存放返回值，对于父进程，会返回子进程的pid号，对于新进程，会返回0；其次是ss0/esp0（另外的1/2没用到），指向的是当前进程的内核栈，两个进程的内核页面不同，所以这个也需要单独设置。
  * 另外，在复制* p = * current时，其实只复制了内核页面前面的task_struct结构，而没有复制栈内容。因为每一次就进入内核栈，都是从tss中取ss0/esp0，也就是默认内核栈永远为空。而新进程会直接返回到用户态，下次陷入内核时也不会用到当前内核栈中的内容，所以不用复制。

还有一个重头戏就是内存，在linux0.11中主要体现为ldt和页表。
  * linux0.11中，每个进程都会占用虚拟内存的一个段（64M），所有进程共用一个虚拟空间，所以这和之后的还不一样。此时，相当于所有进程共用一个页表结构，只是会占据不同的页目录项。所以cr0的值是不用更改的，只需要在ldt表项中设置不同的base就可以（根据进程号nr可以算出来要偏移多少，数据段的limit是64M）。
  * 这样设置完成之后，对于两个不同的进程（比如刚fork完的父子进程），调用同一个movb a,b命令时，可以得到我们预期的结果：如果ds指向内核数据段，那就是0x10，共用gdt，ok；如果ds指向用户数据段，那就是0x17，此时会查询进程自己的私有段表ldt，从而让线性地址落在进程独有的虚拟空间内，进而访问到对应的页目标项，页表项，以及正确的物理地址。
  * 所以在copy_mem()中，一个主要的工作就是设置新进程的ldt；另外一个主要工作，则是复制页表结构，这是通过调用函数copy_page_table()完成的。在复制完之后，父子进程的所有页都会被标记为只读。

最后，将进程的状态设置为running，等待下一次调度的来临。我们可以推理一下可能的过程……
  * 父进程在sys_fork中，调用完copy_process()之后落寞地独自返回，进入system_call的通用流程——验证下state和count看是否到头了，再处理下信号，恢复下内核堆栈和寄存器状态，最后迎来了光荣的、冗长的iret。
  * iret之后，用户态“断面照片”被载入到了cpu的各个部件里，父进程又回到了用户程序之前被截断的一瞬间——pid=fork()，当它继续往下执行，想往栈里写点啥的时候，立马就会发现这一页的属性为只读，触发page_fault，调用do_wp_page()，通过验证mem_map[]的值，发现count大于1，所以就分配一个新的页，将它们都copy过去。然后重新写……
  * 执行了一会，父进程累了，休息了，开始调度。好巧不巧，新进程终于有了上场的机会，它上场的时间点，同样是那个pid=fork()……但是它们俩除了血缘，已经没有任何关系了。

***
### 装载：exec.c

只能fork毫无意义——fork一万个子进程，也是执行和父进程一模一样的逻辑流。我们需要有一个方式，让进程载入新的程序映像，成为一个真正意义上的“新”进程。这就是exec要干的事。（linux0.11和新版本的linux在exec处理时区别很大，0.11的格式是a.out而非elf，也没有共享库与动态装载，但是总体流程是类似的）

exec()的参数里，最关键的是执行程序名字filename。要做的第一步，就是根据这个filename找到对应文件的inode。这里会涉及到各种权限判定（比如可执行权限），以及最后调用namei。

拿到inode后，我们需要对文件的类型进行判定，比如验证它是脚本文件还是.out文件还是elf，还是其他。这是通过读取它的文件头，进行某些对比（比如magic number）来实现的。注意文件头是在文件数据的开头，也就是inode.zone[0]所指向的block中。用ex = * ( (struct exec * ) bh->data)来直接将数据赋给ex头。

对脚本文件的处理暂时不讨论，大概是如果看到magic number为'#!'，则将这一行后面的部分作为寻找脚本程序的dirname；然后载入脚本程序，将这个脚本文件作为类似参数输入这样。你问参数可以这么大吗？参数和环境可以最多占用128K，其中有很复杂的处理操作（我看得头有点晕）。

找到inode，复制好参数，布置好堆栈之后，剩下的就是一开始就不能回头的启动操作了。这里的大概思路和fork一样，围绕task_struct来构建。
  * 首先要将current->executable替换为新找到的inode，之后触发page fault需要从磁盘读东西时需要访问它；注意丢弃之前的exec inode前要iput掉，否则会一直持有。
  * 再重新设置ts的某些字段属性，比如clear所有的信号处理句柄，将有close_on_exec标志的file关闭掉等等。
  * 之后是重头戏，重新构建内存映射，同样可以分为两部分：ldt和页表。根据ex提供的各种段长，重新设置ldt的base/limit，重新设置ts中的start_code,end_code,end_data,brk等；释放掉之前ldt[1-2]指向的页表结构。
  * 这里尤其值得注意的是，完全释放掉页目录项，将其清零就可以，而不用构建它——在page fault中自然会构建。其次，同一个页目录项被释放两次是ok的，如果它p位为0，则会自动跳过。
  * 最后，将程序入口地址（保存在ex中，不一定是main，这是由编译过程决定的）置入内核栈的eip位置，将栈地址置入内核中中的ss位置（这会在iret时用到），最后return，进入system_call的通用流程。


***
### 退出：exit.c

进程退出需要做的是善后事项，主要可以分为两部分：释放持有的所有资源；进程组的相关处理。

* 释放资源。资源可以分为两部分，一是进程所持有的，也即可以通过ts访问到的，主要是内存和磁盘，也即为页表结构、文件、inode，这部分一般有进程自己释放；二是ts本身以及所占的作为内核栈的页，一般由其父进程释放，因为其中可以包含一些进程的退出状态之类的，处理操作包括从task[]中将其删除，并free_page(p)。

* 进程组的相关处理（我现在对于进程组、会话之类的还比较模糊）。比如要tell_father()，如果有子进程则要给它找个继父，如果自己是会话leader则要杀死整个会话之类的。

* exit.c里还有两个重要函数的实现：kill和waitpid，两者都以pid为参数，对各种情况做了解析，很巧妙。

***

### 调度:sched.c 

调度相关的有几个问题：什么是调度，如何完成调度，什么时候触发调度。


**什么是调度？**

就是进程切换，从一个执行实体切换到另一个执行实体，其中也必然包含着控制路径的切换。在大部分情况下，这些控制路径都是内核态的，但也有用户态的情况，比如一个新进程第一次执行时。


**如何完成调度？**

如果将各个进程看做一束平行向前的线，每一次只有一根可以流动，那调度就是对断面进行拍照和重新载入。那么调度可以分为两个重要部分：find_next/switch_to。

* find_next。可以有不同的控制算法实现，比如时间片、轮转、优先级等等；可以考虑的因素则有响应时间、吞吐量、IO密集或者计算密集等等。linux0.11中使用了一个非常简单但又非常好用的算法。

* Switch_to。调度的关键就在于保存和恢复瞬时状态。进程有哪些状态？寄存器、内存，反应为数据结构，就是tss和页表结构。另外还有内核的一些控制数据，比如current指针。所以在switch中，核心工作就是用ljmp触发tss切换工作；而页表结构的切换则通过改变current和ldtr的指向来实现。

* ljmp，它的操作数是64位的。包含段选择符和偏移地址，其中32位的偏移地址被弃置不用。所以需要传入一个{long a, long b}tmp 的结构。只要将b设置为tss描述符段选择子就可以。


**什么时候执行调度？**

调度函数schedule()会在两个时候被调用：进程主动放弃时，或者因为时间片到期，在时钟中断处理函数do_timer()中被调用。

进程主动放弃又可以分为两种情况：一种是从内核态返回用户态时（比如systemcall返回之前），如果发现自己的state不为0，或者count为0，则进行调度；另一种是因为io阻塞或者等待信号，虽然count还有剩余，但主动放弃执行，调用schedule()。

时钟中断处理则是每一个时钟滴答都会触发一次（滴答所对应的时间是通过对时钟模块进行初始化时设置的），在do_timer()中会做很多事，比如判断用户定时器是否到期，如果到期就调用注册的回调函数fn。当然更重要的还是判断当前进程的count，决定是否需要调度。

但是，尤其要注意的是，linux0.11为非抢占式内核，这意味着在内核代码中不能发生抢占。所以如果传入的cpl为0，就不执行调度。这意味着，一个进程陷入内核后，理论上可以一直执行，只要它不经过system_call的通用流程进行返回。

这种方式的好处是内核简洁，竞争保护很容易做（参考fs中的讨论）；但缺点也很明显 ，那就是实时性差。不管优先级多高，中间会有多少的时延是不可控的。

和这种方式相对应的是抢占式内核，也就是在内核中可以发生抢占，只要有机会，不管count是否执行完，高优先级进程一旦进入，就会立即得到执行。所以高优先级进程的执行时延是可控的，不会多于一个时钟滴答。好的实时系统，甚至可以做到微妙级。

linux在2.4之后，也引入了内核抢占，但不是完全的，可是在比较长的内核路径中，插入了可抢占点，具体实现我暂时还不太了解。

***
### 通信：signal.c

**信号通信机制**
信号是进程之间的一种异步通信机制。进程会在合适的时候查看自己的信号表，并进行处理。一般可以分为两种情况：一是从内核态返回用户态时（不管是systemcall直接返回/调度后返回，还是do_timer的直接返回/调度后返回）；另一种则是在schedule()中，内核唤醒等待信号的、处于“可中断睡眠”中的进程。

这种异步信号通信机制有几个要点：
* 约定一个信号表，在所有的进程间达成共识：有哪些信号存在，并是什么序号。比如用一个32位的long表示，就可以对应到每一个bit了。
* 每个进程都可以有自己的信号处理方式。比如可以采取忽略、捕获、默认三种方式。当然对于某些关键信号（比如kill），是不可以忽略和捕获的，防止内核拿进程没办法。
* 进程可以注册自己的处理函数，来“捕获”某些信号。这需要内核提供一个接口给用户来注册，比如sys_sigaction()系统调用之类的。
* 进程应该可以选择屏蔽哪些信号（某些不能屏蔽的除外，比如kill），那就还需要一个32位的block，也即阻塞标志。

同时还需要考虑一些信号处理中的细节，比如：
* 当有多个信号同时达到时，依次执行，还是执行其中一个？其他的忽略掉？还是先保存着，等到下次从内核返回时再执行？
* 信号程序是用户注册的，属于用户程序，在用户数据段，所以肯定是在用户态执行，使用用户态堆栈。那也就意味着对于内核而言，它和普通的用户程序没什么区别。那它就可能因为时间片到期被调度，也可能因为硬件中断而陷入内核。在这些情况下，是否会导致一个信号处理函数还没被执行完，就引发了其他的信号？包括同一个信号，或者其他信号？是否应该允许这样的嵌套，或者重入？


**do_signal()**
* 在linux0.11的实现中，一次只处理一个信号，按序号优先级来，先找到先执行。

* 如果要处理信号，内核代码（实际上就是systemcall中，ret_from_sys_call的那一段）会找到信号序号nr后，调用do_signal()（此时所有的寄存器状态还在堆栈里，相当于都是它的参数）。

* do_signal()的核心任务不是执行处理函数handler，因为此时是内核态，它做的事情包括：
  * 修改内核栈中存放的eip，修改为handler的入口地址，使得iret后，直接开始执行handler；修改内核栈中存放的esp，因为要手动更改用户栈，需要扩张一些空间。
  * 修改进程的用户栈，压入sa_restorer和它的参数们；再压入old_eip的值。这样，当handler执行结束，调用ret时，会跳转掉栈顶地址，也就是sa_restorer的程序入口开始执行；当restorer执行完毕（在它内部一定需要手动清理栈，相当于自己清空压入的参数，从而使得栈顶返回地址暴露出来），调用iret时，会跳转到正确的old_eip地址，然后一切回到正轨。


* 如果只有一种信号，并且同一时间只触发一次，那事情也许可以到此为止了。但是因为有多种信号，而且要考虑信号的重入问题，所以进程注册的，不只是一个简单的handler，而是一个sigaction。其中除了handler、restorer字段，还有mask和flag。具体的处理细节就不深究了，总之会根据这些设定，在处理程序的开始和结尾，对某些信号进行阻塞。这也使得在这个版本里，似乎同一种信号只处理一次，如果重复到来就被丢弃了；而其他的信号即使被阻塞，那也只是没有处理，而不会被丢弃。（相当于signal中会有置位，但是block使得在扫描时不考虑它）

***

### 一些程序小tips

**序号n转换为bit位**
>current->blocked = new_mask & ~(1<<SIGKILL-1) 

**在段间复制数据**
>put_fs_byte(* from,to);
>from++, too++;

**取数组中元素**
> struct sigaction * sa = current->sigaction+signr-1;
这等于 sa = current-> &sigaciton[signr-1]，但毫无疑问后者是绕了个弯路。

**更改栈上的数据本身**
>* (&eip) = sa_handler;
很玄妙。这个操作之所以合理，是因为eip是传入的参数，并且这个参数栈是手动构建的，我们明确地知道它的位置；而且我们需要在函数内部更改栈上数据的值，而非仅仅引用它——就像在平时的函数调用中所做的一样。
不妨设想，如果直接用eip=sa_handler，那编译回来的程序大概率是从栈上&eip位置读数据到某个寄存器比如ebx，然后令ebx=sa_handler，以备后用。而栈上是不会去赋值的，因为没有意义，那里的数据是在函数返回之后会被丢弃的。

**一个复杂的定义**
>void (* signal(int signr, void(* handler)(int)))(int);
>
>typedef void (* handler)(int)
>handler signal(int signr, handler hd)

前者和后者的含义一样。所以，当涉及到以函数指针为另外函数的参数或者返回值（尤其是返回值）时，最好还是typedef一下，否则太难读了。

**printf族**
因为在内核中无法使用c标准库（c标准库是通过调用系统调用来实现的，这显然无法在内核使用），所以在内核中要直接搞一套，但是代码逻辑还是一样的。

这个族里有负责底层实现的vsprintf，有内核专用输出printk，还有输出到文件和字符串的fprintf/cprintf……



***
## 中断处理

### 中断处理流程

所有的中断处理流程在硬件层级上都是一样的：
* 在cpu上触发中断以后，将当前寄存器值cs、eip、eflags、ss、esp压入current->tss->(ss0:esp0)；
* 根据中断号n，从idt[n]中得到中断描述符，解析，访问gdt，最终得到处理程序地址，并载入cs:eip；
* 将在第一步中因为压入数据而有了变化的内核栈地址赋值给ss:esp，并刷新掉eflags。

上面的这一串操作，是在中断发生时，由硬件自动完成的；当执行完中断后，可以经由iret指令一键复原。所以所有类型的中断处理程序，都是以iret结尾的（我当前了解，肯定有误）。也就是说，中断处理入口程序一定是在.s程序里。在linux0.11中，主要包含在asm.s和systemcall.s中，其他的比如还有rs_io.s等。

由此我们也可以知道，中断处理程序是一定会陷入内核的，不管是系统调用，还是cpu发现的执行异常/陷阱，还是外部硬件中断，都一定会导致进程进入内核态。

但同时，并不是只有用户态才能引发异常，内核态中同样可以引发异常和中断。也就是说上面的过程同样可以在内核态中上演——

但是这会引发的问题是，这会导致内核控制路径的嵌套，如果中断处理程序都是在内核栈中执行，那如何处理呢？将内核态当前的五虎上将（cs/eip/eflags/ss/esp）又放到内核栈里？但是因为内核栈有东西，所以要从当前内核栈的esp位置开始放，相当于函数嵌套调用时的栈帧？这就需要触发中断和iret时可以自动判断有没有权限的切换了……又或者，当处于内核态时，对五虎将会用另外的方式处理？我暂时还不确定。


### 中断程序种类

**三种门**
在linux0.11中，中断门分为了三种：intr、trap、system。在system.h中，有三种门的初始化宏，其中的区别是：
>trap：type=15，dpl=0；
intr：type=14，dpl=0；
system：type=15，dpl=3。

dpl=3意味着任何程序都可以调用，其中主要有int3（调试用），overflow，int0x80等。dpl=0则意味着用户程序无法直接访问，这可以防止用户程序的恶意模仿。

**初始化**
初始化分为两步：

第一步，在head.s中，将idt全部初始化为一个哑中断函数。

第二步，在各个初始化程序中分别完成，比如在traps.c的traps_init()函数中，初始化了大部分trap/system；在char_dev/serial.c/rs_init()中，调用了set_intr_gate(0x24,rs1_interrupt)，在sched.cd中的sched_init()初始化函数中，则初始化了时钟中断和系统调用中断……

从这里我们看出三者的一些分别，intr主要指向的是硬件中断，trap则指向一些cpu执行中遇到的陷阱和异常，system则是面向用户程序开放的，比如系统调用占用int80和调试用的int3。

在初始化时，和硬件相关的初始化，都是在对应的设备驱动子程序中完成的。


**处理**

中断处理内部也有着形式上的类似性，比如都会涉及到一些寄存器的保存恢复操作；一般来说，除了入口程序之外，真正执行逻辑的程序名字叫do_xxx，比如do_timer。

但是它们之间也有着明显的区别，我大概会将它们分为以下几类：

第一种是系统调用systemcall，它无疑是特别的。它只能在用户态时被调用，需要在根据调用号查询对应的实际处理函数。它还有一个特别的地方是，在处理完返回的时候，会检查它的state和count，并可能引发调度，这是程序的一个处于主动放弃和被动调度之间的调度点，大概可以命名为被迫主动调度。而在返回到用户态之前，它还要做信号处理。

第二种是timer类。在时钟中断中，也可能会引发调度，这和系统调用返回之前是类似的，但在时钟中断中会判断进中断之时的cpl，如果为内核态就不调度。同时因为可能是在睡眠中醒来，时钟中断返回后，同样需要处理信号。

第三种则是普通类。它们就是普通的保存寄存器，调用do_xxx，恢复，iret退出。不涉及任何的判断、调度、信号处理。


***
## 内存管理：memory.c

### 抽象概念

可以查看之前的笔记：《操作系统公开课》

补充：
（《程序员的自我修养》笔记）
**内存**

内存的图像是由多进程的图像所引出的。随着进程概念的引入，一台计算机要为不同的用户提供服务，需要同时运行多个程序。此时，内存的几个问题随之而出：
* 如何进行保护隔离，使得程序之间互不打扰？
* 如何尽可能内存复用，提高使用效率？
* 如何进行重定位，使得编程的人不用关心底层地址分布？

在这种需求之下，分段机制出现了。它解决了第一个问题和第三个问题：它引入虚拟地址和保护模式的概念，程序需要使用多少内存，就申请多少内存，用段基址和段限长的方式，保证不会访问越界；同时段基址的可更改，又可以实现载入和swap的重定位问题。

但是第二个问题仍然存在，因为段很长，管理粒度很粗，会导致内存中产生大量的、谁都不愿意吃的“面包屑”；另外，当进程多、内存需要swap时，每次都需要放进、移出整个的程序，效率很低。

为了解决这些问题，分页机制出现了。分页机制通过大大减小管理的粒度——从不定长的段变为固定大小的页——一方面防止了面包屑的产生，这个时候的面包是切成一整片一整片的了；另一方面，扩展了虚拟地址的概念，变为不受限制的虚拟地址空间。

因为在页机制中，不再需要底层硬件空间的连续性，它可以用离散的方式进行管理，这产生了非凡的效果——没有被使用的虚拟地址段可以完全不映射；没有正在被使用的虚拟地址段不需要放到内存，而是可以存储到磁盘里。

分页机制向每一个进程所保证的是，你总是可以在你需要的时候，拿到你某个内存地址中的内容。至于它是在内存里，还是在磁盘里，中间会不会导致缺页陷入内核交给操作系统来处理，不需要你关心。

在这种形态里，内存和磁盘几乎完美地结合了，内存成为了磁盘的高速缓存，理论上我们似乎拥有了近乎无限潜力的内存空间。


**虚拟地址空间**
这是一个很大的话题，我们只能尽可能简略地聊一聊。进程虚拟地址空间是随着分页机制的出现而出现的内存管理方式，有着很多优点，其中最显著的几个特点是：
* 每个进程独占一个地址空间。在linux0.11的时候，一个程序只能分到64m的虚拟空间；而现在是完全的独占。这使得对地址空间进行隔离、保护、约束都很方便。
* 虚拟内存空间只依赖于映射结构（页表结构），而不依赖于实际的物理内存；对进程只需要保证在它需要的时候，可以拿到它想要的数据就可以了。
* 将内存以“页”为单位进行管理。
* 上述两者可以得到很多强大的特点：程序要求的虚拟空间的连续性经过页表映转换后，可以映射到离散的物理内存；页表这种转换机构配合swap机制，可以让页内容对应到磁盘，在需要的时候从磁盘换入物理内存，相当于是让物理内存作为了磁盘的缓存，而让计算机拥有了极大的内存空间。
* ……

***
### 主要函数

* get_free_page()
  * 传入参数：无；返回：物理页地址。
  * 物理页申请，和线性地址无关。
  * 搜索men_map[]，找到空闲页，计数加一；将页面清零。
  
* free_page()
  * 传入参数：物理页地址。
  * 物理页释放，和线性地址无关。
  * 判断是否地址等是否出错；计数减一。

* put_page()
  * 传入参数：物理页地址、线性地址。
  * 将指定的物理页，映射到线性地址处。这是连接线性地址和物理地址的底层函数。
  * 判断线性地址是否有效，如没有分配页表则需要先申请内存页初始化为页表；再讲指定的物理页地址放入页表项中。

* copy_page_tables()
  * 传入参数：起始线性地址、长度。
  * 复制指定线性地址段所对应的页表结构。
  * 判断地址、长度等是否合法；复制页目录项（因为共用一页内存，不需要申请新页，只需要填充新项）；如果某一项存在，则需要复制页表，申请新页，初始化为页表，逐项判断、复制；将新旧两个页表中的页属性都设置为只读（R/W位清0）；在mem_map[]中将所有页面的计数加一。

* free_pape_tables()
  * 传入参数：起始线性地址、长度。
  * 释放指定线性地址段所对应的页表结构。
  * 判定参数是否合法；从目录项开始，进入页表，逐项检查，释放物理页；释放各个页表所在的物理页；刷新页表高速缓存。

* get_empty_page()
  * 传入参数：线性地址。
  * 申请物理页，映射到指定的线性地址处。
  * 先调用get_free_page()得到物理页地址；再调用put_page()将两者映射起来。

* do_no_page()
  * 缺页异常中断处理函数。
  * 要进行情况判定：如果缺页内容位于数据/代码段，则尝试进行页面共享操作，如果失败则从设备中读入；如果大于这一段，或者进程刚建立，则新申请一页物理内存。（swap情况呢？）

* do_wp_page()
  * 写保护异常中断处理函数。
  * 先判断地址区域是否合法（比如代码区就不允许，这个可以根据段/线性区性质进行判定）；执行写时复制操作。
  * 写时复制：先查看物理内存页的引用数，如果为1，则直接修改页表R/W位，刷新缓存，返回（这说明另外的进程已经触发过写时复制，它是最后一个拥有了的）；如果引用数大于1，则重新申请一页并映射，并将原物理内存页的引用数减一。

* mem_init()
  * 在main.c中的main()函数中被调用。
  * 因为初始页表结构已经在head.s中设置，所以这里只需要初始化mem_map[]就可以。

***

### 一些细节与讨论

* invalidate()
  * 实现为：__asm__("movl %%eax, %%cr3"::"a"(0))
  * 对cr3重新赋值0，触发页表缓存刷新。内核编程中导出都有这样的“小手段”。

* 地址换算
  * 在上面的处理函数中，有很大一部分工作都是将线性地址和页表索引之间的换算、物理地址和mem_map索引之间的换算。
  * 在地址、索引等的计算里，大量的用到这几个操作：>>/<</&/|，因为位操作比加减乘除都要快。
  * 中间要特别注意的是，在页表区域，线性地址和物理地址是一一对应的，且pg_dir=0，这很重要。

* 汇编代码
  * get_free_page()就是用汇编实现的。但理论上应该是可以用c语言编写的，可能是为了效率吧。毕竟有两个循环操作（一个是搜索mem_map，一个是对页面清0）。

* 页表缓存刷新
  * 当页表映射失效时，一定要刷新。比如释放了，或者执行了异常处理。
  * 当新增了页表映射时，可以不用刷新。

* 共享机制
  * 这是为了同一应用程序开启不同进程时，可以有代码/数据的共享。（数据段也可以用写时复制的形式共享）
  * 当发生缺页异常时，就会先尝试着进行共享：先判定是否在共享区域内；对这一可执行文件对应的进程（链表），挨个尝试是否可共享……

* 只能调用一次的限定
  * 某些函数只能在系统初始化被调用一次。
  * 用static静态变量的方式进行限定：初始化为1，用它进行判定，如不为1则返回；在第一次调用时清0。（此时还没有开启多线程，所以没有竞争风险）

***
### 从全局的角度看mm模块

从全局来看，首先这个模块更多的是向内核的其他模块提供服务（比如中断处理、进程空间管理等），而非直接面向用户程序。

用户程序如果要申请一块内存，只会在进程空间的线性区中划出来一块；在之后的实际访问中，触发缺页异常后，才会进入mm模块，进行底层处理。

但linux0.11里还没有实现虚拟内存。


**补充 2019.11.9**
* 在copy_page_table后，会有两个不同的页表项指向同一个物理页。页表项中是没有count计数这个说法的，只有rw位；物理页数组mem_map[]是char，是可以计数的，所以可以mem_map[this_page]++。

* 在free_page_table时，不但会将页表对应的物理页都释放掉，将页表清零，也会将页表项本身释放掉，就是目录项也会被清0，* dir=0；但是特别值得注意的是，这样被释放掉之后，是可以直接访问的，但是会触发page fault，引发do_no_page。因为虽然页目录项都不在了，但是访问的地址本身是合法的（在segment limit之内）。


***

## 块设备

可以分为两大块：
* ll_rw_blk.c，其中包含对外接口ll_rw_blk()，主要给buffer.c使用。
* hd/floppy/rmdisk等不同种类的实际块设备的驱动。


### ll_rw_blk.c

**核心流程：**

* 在buffer.c中调用ll_rw_blk()。
  * 传入读写标志rw、buffer head数据结构bh。
  * 在调用完后，会进行等待wait_on_buffer(bh)，bh中含有b_block标志。

* 执行ll_rw_blk()
  * 流程：进行一些判定，并调用make_request()。传入主设备号major，rw，bh。

* 执行make_request()
  * 流程：对bh进行判定，比如buffer是否可用；在request[]找可用的槽；将bh转换为req；调用add_request()，dev（根据major在blk_dev[]中找到的），req。
  * 如果没有找到可用的槽，则进入等待，sleep_on(&wait_for_request)。注意传入的参数是指向链表的指针。

* 执行add_request()
  * 流程：一些判定；如果当前设备空闲，则直接运行request_fn()后返回；如果有任务在执行，则将req加入电梯队列后返回。
  * 电梯队列：以特殊的方式构造链表，使得顺序访问符合电梯规律。比如当前队列是[30,35,40,60,10,20]，那50会被插入40/60之间，15会被插入10/20之间，5则会插入60/10之间。所以插入时有两种可能性：前<插入<后，或者前>后且插入>前或者小于前。
  * request_fn()是用宏实现的分发机制，是面向对象思想，可用根据不同的设备类型，调用不同的函数。

* 不管是request_fn()，还是加入电梯队列，都不会等待IO，也即是不会阻塞在IO上，而是立刻返回。所以整个的流程只有两处会陷入睡眠：
  * buffer.c中，调用ll_rw_blk()之后的wait_on_buffer()，在其中sleep_on(&bh->b_wait);
  * make_request中，没有槽时的sleep_on(wait_for_request)。
  * 以上两者，都会在end_request()中被唤醒。而且是一次性唤醒整个链表上所有等待的进程。



**关键数据结构：**
```c
struct blk_dev_struct {
    void (*request_fn)(void);
    struct request * current_request;
};

struct task_struct * wait_for_request = NULL;

struct request {
    int dev;        /* -1 if no request */
    int cmd;        /* READ or WRITE */
    int errors;
    unsigned long sector;
    unsigned long nr_sectors;
    char * buffer;
    struct task_struct * waiting;
    struct buffer_head * bh;
    struct request * next;
};

struct buffer_head {
    char * b_data;          /* pointer to data block (1024 bytes) */
    unsigned long b_blocknr;    /* block number */
    unsigned short b_dev;       /* device (0 = free) */
    unsigned char b_uptodate;
    unsigned char b_dirt;       /* 0-clean,1-dirty */
    unsigned char b_count;      /* users using this block */
    unsigned char b_lock;       /* 0 - ok, 1 -locked */
    struct task_struct * b_wait;
    struct buffer_head * b_prev;
    struct buffer_head * b_next;
    struct buffer_head * b_prev_free;
    struct buffer_head * b_next_free;
};

struct request request[NR_REQUEST];

struct blk_dev_struct blk_dev[NR_BLK_DEV] = {
    { NULL, NULL },     /* no_dev */
    { NULL, NULL },     /* dev mem */
    { NULL, NULL },     /* dev fd */
    { NULL, NULL },     /* dev hd */
    { NULL, NULL },     /* dev ttyx */
    { NULL, NULL },     /* dev tty */
    { NULL, NULL }      /* dev lp */
};

```
说明：

* 设备的数据结构blk_dev_struct中，包含了请求处理函数指针request_fn，和请求队列current_request。每一种设备都有自己的请求队列（数据）和自己的处理方式（方法），通过宏定义将它们绑定在一起，使得“对象”只用自己的方法访问自己的数据。

* 所有的块设备都集中放置在一个数组blk_dev里，可以用主设备号进行访问。0号空置。blk_dev[major]和major+blk_dev都会拿到一个dev指针。

* 所有的request集中放置在一个数组request[]里。但是它们的next指针指向同一设备的下一个请求。这样的方式使得request的数量是受控的，而且放置的位置固定，在运行时不需要临时申请内存。（如果像用户程序一样，要一个申请一个，那是不可想象的；内核程序的一般做法是，规规矩矩地规划好，摆好地盘，限制好数量，一切都在控制内。另外，所有的内存申请都是危险的，因为可能触发缺页异常。而所有中断代码因为不可以被调度，所以不能陷入睡眠，不能触发缺页异常。）

* bh和req之间的互相转换。bh内几乎包含了一个请求需要的全部信息（除了rw标志），比如缓冲区、设备号、读写块等等。注意bh里没有包含进程信息task_struct，但这是隐含的。因为对bh的调用存在于某一进程的内核态，而不是中断代码那样不代表进程。（事实上，这时候如果需要，是可以利用宏取得进程句柄的；同时，可以调用sleep_on让进程进入睡眠。）
***

**核心代码：**
```c

static void make_request()
{
    struct requset *req;
    req=request+NR_REQUEST;
    //linux0.11中，1个block对应2个扇区
    req->sector=bh->b_blocknr<<1;
    add_request(major+blk_dev,req);
}

static void add_request(struct blk_dev_struct *dev, struct request *req)
{ 
    struct requset *tmp=dev->current_request;
    req->next=NULL; 
    cli(); //关中断(互斥)
    //电梯算法，构建电梯队列
    for(;tmp->next;tmp=tmp->next)
        if( (IN_ORDER(tmp,req)||!IN_ORDER(tmp,tmp->next)) && IN_ORDER(req,tmp->next) ) 
            break;
    req->next=tmp->next; tmp->next=req; 
    sti();
}


#define IN_ORDER(s1, s2) \
    ((s1)->dev<(s2)->dev)||((s1)->dev == (s2)->dev && (s1)->sector<(s2)->sector))

```

**全局审视：**

遇到这种比较复杂的模块，可以找一张大纸，在其中画出两个图：函数调用的流程图；数据结构图。

统一接口：
* ll_rw_blk是处于buffer和底层硬件如hd中间的一个中间层，它代表着所有的块设备，向上层提供统一的接口。这是它存在的根本原因。

* 因为它的存在，buffer对所有块设备的访问，只需要传入bh，在其中说明设备号（注意设备指针是自己拿到的）、block号（在文件系统解析时计算得到的），这是非常简明的。

* 每一种设备都有自己的处理方式、接口，也都会有自己的请求队列，它们是完全可以并行的。这里巧妙的使用了宏定义，实现了面向对象思想——

面向对象：
* 如果我们将每一个设备看作一个对象，当上层的请求通过统一的接口到来后，我们就需要根据它请求的对象的真实类型（设备的主设备号），来选择相对应的处理方式。

* 在这里，就是用MAJOR获得请求的设备号，在对象数组中找到对象，将req添加到列表里，之后用这个对象的私有函数request_fn()进行处理。

* 在buffer来看，底下全是“块设备”；但是在实际处理里，却区分成了以major号为标志的不同类型对象。这是非常巧妙的。

数据结构：
* 在数据结构上，将所有的“设备对象”和“请求对象”，分别放置在里两个有限制的数组里。这种方式应该是内核程序的通用方式，提前布置好确定的内存，不需要申请，不会溢出。如果太多了就报警，或者睡眠等待。
* 数组与链表交叉的方式也很有意思。request数组的形式，是方便内存管理；而其中next链接是逻辑上的处理要求。两者并不冲突。我确信之后会在内核中看到很多这样的形式：逻辑上需要有链表的形式，但同一类型的对象都集中放置在一个连续的区域内，统一管理。

整体流程：
* 这个流程的设计是清晰、简单、自然的。当接收到一个块设备请求，那需要找到对应的设备对象，作各种状态判定，根据传入的参数构建内部使用的要求；如果设备空闲，就直接满足请求；如果忙，就放入等待队列里。
* 至于具体的实现细节，比如是电梯队列还是FIFO，这仅仅是算法上的选择，不影响整体流程的设计和理解。
* 值得特别提出的一点是，这都是不阻塞的；也就是说，块设备的申请都是异步的。我们需要注册回调函数，在中断处理函数中调用。真正的同步是在上层，在buffer中做的，通过某些标志位实现同步。


***
### hd.c

hd.c，包含了硬盘系统底层的工作原理。

**核心流程：**

* 对象注册。
  * 在初始化时，将request_fn设置为do_hd_request。

* do_hd_request()
  * 请求处理函数。所有的请求从这里开始。
  * 流程：一些检查与判定，标志信号的处理；如果current为null，直接返回；从block计算得到磁盘读写底层参数，sec/head/cyl等；调用hd_out()后返回。

* hd_out()
  * 用out_p指令向硬盘控制器发送底层命令。设置回调函数do_hd，这会在中断处理函数中被调用。
  * 非阻塞，发完就返回。

* write_intr()/read_intr()
  * 回调函数，其中一个会被赋值给do_hd指针，在中断处理程序中被调用。
  * 流程（以read_intr）为例：判断数据是否有效；将数据从硬盘控制器缓冲区复制到内存缓冲区；判断是否读完，如果没读完，将do_hd重置为read_intr，返回结束；如果已经读完，先调用end_request()，并再次调用do_hd_request()，开始处理下一个请求。

* end_request()
  * 唤醒所有等待缓冲区数据的进程；唤醒所有等待request空位的进程。
  * 将自己的dev置为-1，表示不再存在；将current指向next。

***
**讨论：**

异步与中断：
* 对于io的处理是异步的，do_hd_request不会阻塞，它向硬盘控制器发完命令就返回了。
* 中间进行串联的是中断处理程序：它会进行检查，复制数据，并根据请求是否完成安排下一步工作。
* 所以，所谓的“中断处理程序”，并不是特指某一行代码，而是处于“中断处理过程中的程序”，比如这个例子中的do_hd_request，它既可以运行在进程的内核态，又可能运行在中断处理过程中。
* 所有可能在中断过程中被调用的函数，都是不能陷入等待的；因为此时计算机无法调度（具体实现还不清楚，如果没有屏蔽中断的话定时器中断是可以进来的，但可能不再调用调度程序），从而导致死机。

全局流程：
* 整体流程是很清晰的：设备开始运行起来以后，就要把所有可做的任务都做完；而进程本身只负责提交请求，后来的执行逻辑就不管了；在其中进行将执行逻辑连接起来的，就是中断处理程序。
* 从某一个进程的视角来看，它申请读磁盘，陷入内核态，提交request，然后等待buffer；之后的各种硬盘控制器与中断处理程序之间的互动都和该进程无关（事实上和任何进程都无关，中断处理程序不代表任何进程），直到这个buffer准备好了，它收到某个唤醒信号；它验证buffer的lock标志位，继续往下执行，似乎这是一眨眼的事。

***

**基于宏的面向对象实现：**
```c

#ifdef MAJOR_NR

/*
 * Add entries as needed. Currently the only block devices
 * supported are hard-disks and floppies.
 */

#if (MAJOR_NR == 1)
/* ram disk */
#define DEVICE_NAME "ramdisk"
#define DEVICE_REQUEST do_rd_request
#define DEVICE_NR(device) ((device) & 7)
#define DEVICE_ON(device) 
#define DEVICE_OFF(device)

#elif (MAJOR_NR == 2)
/* floppy */
#define DEVICE_NAME "floppy"
#define DEVICE_INTR do_floppy
#define DEVICE_REQUEST do_fd_request
#define DEVICE_NR(device) ((device) & 3)
#define DEVICE_ON(device) floppy_on(DEVICE_NR(device))
#define DEVICE_OFF(device) floppy_off(DEVICE_NR(device))

#elif (MAJOR_NR == 3)
/* harddisk */
#define DEVICE_NAME "harddisk"
#define DEVICE_INTR do_hd
#define DEVICE_REQUEST do_hd_request
#define DEVICE_NR(device) (MINOR(device)/5)
#define DEVICE_ON(device)
#define DEVICE_OFF(device)

#elif
/* unknown blk device */
#error "unknown blk device"


#define CURRENT (blk_dev[MAJOR_NR].current_request)
#define CURRENT_DEV DEVICE_NR(CURRENT->dev)


extern inline void end_request(int uptodate)
{
    DEVICE_OFF(CURRENT->dev);
    if (CURRENT->bh) {
        CURRENT->bh->b_uptodate = uptodate;
        unlock_buffer(CURRENT->bh);
    }
    if (!uptodate) {
        printk(DEVICE_NAME " I/O error\n\r");
        printk("dev %04x, block %d\n\r",CURRENT->dev,
            CURRENT->bh->b_blocknr);
    }
    wake_up(&CURRENT->waiting);
    wake_up(&wait_for_request);
    CURRENT->dev = -1;
    CURRENT = CURRENT->next;
}

```
说明：
* extern inline产生的代码类似于宏，可以包含在头文件中，但又不与库文件中的同名函数冲突。
* 还有部分代码在后面的笔记中（《操作系统公开课》笔记）。


**核心代码**
```c

void do_hd_request(void)
{ 
    unsigned int block=CURRENT->sector;
    __asm__(“divl %4”
    :”=a”(block),”=d”(sec)
    :”0”(block),“1”(0),”r”(hd_info[dev].sect));

    __asm__(“divl %4”
    :”=a”(cyl),”=d”(head)
    :”0”(block),“1”(0),”r”(hd_info[dev].head));
    
    ...
    
    hd_out(dev,nsect,sec,head,cyl,WIN_WRITE,...);
    port_write(HD_DATA,CURRENT->buffer,256);
}

void hd_out(drive, nsect, sec, head, cyl, cmd...)
{ 
    port = HD_DATA; //数据寄存器端口(0x1f0)
    outb_p(nsect,++port); outb_p(sect,++port);
    outb_p(cyl,++port); outb_port(cyl>>8,++port);
    outb_p(0xA0|(drive<<4)|head, ++port);
    outb_p(cmd, ++port);
}

static void read_intr(void)
{ 
    end_request(1);//唤醒进程
    do_hd_request(); 
}
```
***


## 字符设备

前面两节的笔记来自于《操作系统公开课》笔记，后面来自于《Linux内核注释》笔记，算是对前面的补充完善。


### tty

**核心数据结构**
```c
//tty.h
struct tty_queue {
    unsigned long data;
    unsigned long head;
    unsigned long tail;
    struct task_struct * proc_list;
    char buf[TTY_BUF_SIZE];
};

struct tty_struct {
    struct termios termios;
    int pgrp;
    int stopped;
    void (*write)(struct tty_struct * tty);
    struct tty_queue read_q;
    struct tty_queue write_q;
    struct tty_queue secondary;
    };
```
说明：
* 每一个终端设备都可以看作一个tty对象，有自己的数据和方法。这就是tty_struct。其中有io控制对象termios，其中有4个标志集；有写的方法write，通常的调用方式是tty->wirte(tty)；有三个缓冲队列：读、写、备用列。

* tty_queue是循环队列。写的指针在右边，读的指针在左边，都向右移动，到尾就复位到开始；两者相等时就表示空了或者满了。

***
**tty_write()**
* 这是向上层提供的终端设备读写接口。需要传入设备的channel号，用户数据buf地址，以及需要写的数量nr。
* 先通过channel在tty_table[]中找到tty对象，将数据从buf写入tty自己的缓存区write_q里，并调用tty->write(tty)。
* tty对象可能有多种，比如终端对象或者串口终端对象。如果是终端对象，write指向con_write()；如果是串口对象，则指向rs_write()，它是异步通讯机制，发出后会直接返回，在串口中断里进行处理，移动队列指针。所以write是有可能陷入睡眠等待的，sleep_if_full(&tty->write_q)。

***
**tty_read()**
* 读比写有意思一些。这也是向上层提供的终端通用接口。会传入buf，数量等信息。

* tty_read()读的不是read_q，而是secondary队列。这是经过规范化处理的队列。如果sec队列空了，但是还没有读够，就陷入睡眠等待sleep_if_empty(&tty->secondary)。（其中有比较复杂的处理规则，细节就不管了）
* 键盘的中断处理程序负责其余的工作：
  * 中断程序入口是keyboard_interrupt()，在其中会经过table转换之类的，调用到put_queue()和do_tty_interrupt()。
  * put_queue()中，将字符写入read_q队列；
  * do_tty_interrupt()中，调用内部函数copy_to_cooked()；
  * copy_to_cooked()中，通过将字符放入write_q，并调用tty_write()进行回显；并将字符规范化后，放入secondary队列；并唤醒等待该缓冲区的队列wake_up(&tty->secondary.proc_list)。


***
**一些有意思的讨论：**

进程与终端的关系
* 终端并不属于某个进程，但任一时刻，都有且只有一个进程持有该终端（暂时要打个问号……）。也就是说进程会持有终端，但不会拥有终端。

* 终端和进程之间是相对独立的，尤其是读的时候：比如进程需要读一行，那它会在tty_read()里陷入睡眠等待；此时我们敲击键盘输入一个字母，终端代码作为中断处理程序和我们进行交互——响应、回显……但是它并不代表进程，进程的逻辑流依然在睡眠之中。等到中断处理程序执行完，wakeup了进程，返回，重新调度。这时候回显已经完成，但进程可能根本没有醒来，甚至于之后的调度都轮不到它；即使它醒来，也只是简单的在tty_read里将这个字符从read_q搬运到buf，就继续等待了……直到我们敲下回车，进程再次被唤醒并成功调度，流程继续往下执行。

* 那到底什么是终端对象（tyy）？我们要透过屏幕去看它。显示器只是被显卡所控制的显示装置而已，并不代表tty对象的内存状态。事实上，我们只需要把要写的内容传递给显卡，然后即使cpu死机了，显卡仍然会正常工作，显示的内容仍然存在。显示器的控制窗口和键盘都是tty的延伸，都不是它本身。（要注意它的独立性）


***

## 文件系统

### 关键数据结构

**文件系统组织关系图：**

```c
struct buffer_head {
    char * b_data;          /* pointer to data block (1024 bytes) */
    unsigned long b_blocknr;    /* block number */
    unsigned short b_dev;       /* device (0 = free) */
    unsigned char b_uptodate;
    unsigned char b_dirt;       /* 0-clean,1-dirty */
    unsigned char b_count;      /* users using this block */
    unsigned char b_lock;       /* 0 - ok, 1 -locked */
    struct task_struct * b_wait;
    struct buffer_head * b_prev;
    struct buffer_head * b_next;
    struct buffer_head * b_prev_free;
    struct buffer_head * b_next_free;
};

struct m_inode {
    unsigned short i_mode;
    unsigned short i_uid;
    unsigned long i_size;
    unsigned long i_mtime;
    unsigned char i_gid;
    unsigned char i_nlinks;
    unsigned short i_zone[9];
/* these are in memory also */
    struct task_struct * i_wait;
    unsigned long i_atime;
    unsigned long i_ctime;
    unsigned short i_dev;
    unsigned short i_num;
    unsigned short i_count;
    unsigned char i_lock;
    unsigned char i_dirt;
    unsigned char i_pipe;
    unsigned char i_mount;
    unsigned char i_seek;
    unsigned char i_update;
};

struct file {
    unsigned short f_mode;
    unsigned short f_flags;
    unsigned short f_count;
    struct m_inode * f_inode;
    off_t f_pos;
};

struct super_block {
    unsigned short s_ninodes;
    unsigned short s_nzones;
    unsigned short s_imap_blocks;
    unsigned short s_zmap_blocks;
    unsigned short s_firstdatazone;
    unsigned short s_log_zone_size;
    unsigned long s_max_size;
    unsigned short s_magic;
/* These are only in memory */
    struct buffer_head * s_imap[8];
    struct buffer_head * s_zmap[8];
    unsigned short s_dev;
    struct m_inode * s_isup;
    struct m_inode * s_imount;
    unsigned long s_time;
    struct task_struct * s_wait;
    unsigned char s_lock;
    unsigned char s_rd_only;
    unsigned char s_dirt;
};


struct dir_entry {
    unsigned short inode;
    char name[NAME_LEN];
};

```
***


### buffer

**buffer.c**
* 这是文件系统里最核心的数据结构。buffer.c也是至关重要的一层。它是整个系统的基础。
* buffer层是内存对磁盘的缓存，这样设计的原因有很多：访问内存的速度远远快于访问磁盘；程序的执行往往具有局部性（时间局部、空间局部）。所以可以通过缓存的方式，大大缩短磁盘访问时间，带来运行效率的提升。


**实现机制**
* 在内存中划分一片区域作为缓存，其中的管理单元是“块”，与磁盘中定义的“块”的大小保持一致；
* 每次从磁盘读内容到内存时（注意，磁盘是无法直接通过cpu指令读写的，必须要先载入内存，这是常识但还是很容易忽略），一次性读一整块甚至更多，并且不直接读入用户buf，而是读到由操作系统统一管理的高速缓存区（用户可能一次性只需要读一点点）。
* 每次用户需要读文件内容时，都会先到缓存区中看看是否已经在了，在就直接使用；如果没有在，就从磁盘读到缓存区，再从缓存区拷贝到用户数据区。
* 每次用户需要写内容时，先将数据块从磁盘读到缓存区，再将用户数据写入到缓存区中，并标记为dirt。当缓存区进行同步时，所有dirt块都会写回磁盘。
* 所以，所有对于磁盘的读写都是经过buffer完成的，这使得buffer成为完整的一层——它可以完全代表磁盘系统，buffer中的内容一定是最新的，因为不会有其他程序越过buffer对磁盘做更改。
* 某种意义上，我们可以将buffer视为磁盘的一种修改接口，对它的修改就是对磁盘的修改，它们底层如何同步是上层所无需关心的。甚至于是否有底层上层也看不见。上层能看到的就是一片buffer，各个bh单元。

**buffer_head**
* buffer_head（简称bh）是最核心的数据结构。它是整个管理机制运行的核心。读取、写入、同步、锁、各种判断、管理，都是以它单位来实现的。
* bh字段中，包含了完整的映射信息：数据指针data，设备dev，设备逻辑块号blocknr，有效标志uptodate，脏标志dirt，引用数标志count，锁标志lock，以及等待进程队列wait，用于哈希管理的链表指针prev/next，以及用于空闲bh槽管理的prev_free/next_free。
* bh是完整的。它与磁盘之间是一种完全的映射关系，而非读写关系。当同步时，将data中的数据块，写入到dev->blocknr位置去。不需要外在的参数。（事实上调用的应该是ll_rw_blk(rw,bh)）
* bh有多个判断标志。它们之间的关系我还没有理清楚……反正读和写都是不加锁的，只会判定uptodate；在与磁盘进行同步时，会加锁（因为数据写入是在中断处理程序中完成的，它们随时会执行；如果同步时不加锁，其他人读到一半可能会发生数据同步，就会导致读到的数据前后不一致，产生数据错误）；在进行回收时，会验证count（正有人使用的bh是不能释放的）。
  
**组织**
* 所有的bh形成一个大数组，存放在buffer区的开头。同时它还有两个双向链表：哈希链表和空闲链表。
* 哈希表是为了快速查询。一个bh可以对应到唯一的dev->block，所以查询的索引就是dev+blocknr，哈希函数是mod一个质数，比如307，对应哈希数组的307个指针槽。进行查询时，将索引值传入哈希算法，得到槽value，并沿着链表一一比较dev/block是否对应。在加入一个新的页时，同样计算得到槽value，然后插入到哈希链表的前端。
* 空闲队列。事实上，所有的bh都会在空闲队列里，而不管它是否空闲。这个队列存在的意义，更多的是运用某种算法来实现换出顺序。在linux0.11里，就是在每一次读到新的块以后，都先把它移出哈希链表、空闲队列，再重新将它加入（因为dev/block可能发生变化）。在插入空闲队列时，加入队尾，也就是先进先出……
* 这里的链表实现、操作都很典型，值得亲自动手写一遍，不用怀疑，很容易出错。
* buffer.c提供了初始化接口buffer_init。主要的任务就是对bh内容进行初始化，置0置null；然后建立空闲列表，把所有bh都串进去。

**bread()**
* buffer.c向上层提供统一的接口bread()。bread()传入参数dev、block，一次读一块，返回一个bh给调用者自行使用。所以，bread()也可以看作一个内存申请函数，类似于alloc的调用者必须free，bread的调用者必须手动brelse，否则bh的count会一直大于0，导致无法回收。
* 在上层来看，bread掩盖了所有的底层细节：你只要传入设备号、块号，我就给你一个块的数据，其他你别管。你甚至于可以将bread就等同于磁盘的一个内置接口来使用。
* bread的执行流程中，先调用getblk(dev,clock)，获得一个对应的bh；再判断bh->data是否有效，如果无效，就调用底层块设备读写函数ll_rw_blk(READ,bh)。
* getblk有着复杂的验证逻辑，它的逻辑主线是，先在hash表中查找是否有缓存；如果没有就在空闲链表中寻找可以用的buffer块，找到以后对调用sync_dev进行同步（回写dirt块），并进行参数初始化，包括置位标志，更新队列位置等；如果没有找到，就睡眠等待。

**brelse()**
* bh是在bread中申请的，但是它的释放却不是。其中要非常注意，否则会导致资源泄漏。
* 持有bh的是任意调用bread()函数的程序（当然肯定是内核程序），谁申请了它，谁就要负责调用brelse()释放。
* brelse并没有真正释放掉bh，而是引用计数减一；如果引用计数为0，就会在getblk()中被判定为空闲，同时出发sync_dev()，完成数据同步后真正被释放掉。或者在系统调用sys_sync()中，所有buffer一起同步调。


***
### file、inode

* 磁盘的基本组织是块block，buffer的也是。这在底层进行管理时很好用，但是对用户程序并不友好。用户喜欢的方式，就是文件。（具体演化思想可以参考前面的笔记：从生磁盘到文件）这里的思想也是类似于内存的，分页是os底层管理所适用的，而分段（vma线性区机制）是进程管理所喜欢的。

* 什么是文件？文件是一个抽象的、整体的概念，它的数据形式是字符流。文件的数据在硬盘设备中以块的形式组织，文件可以包含多个不连续的块，并且由它自己维护一种映射结构，其中包含了文件的各种属性信息，这就是inode（类似于bh之于buffer）。
  * 所有的文件都一定有inode，事实上，在文件句柄数据结构里，核心就是inode，其他都是一些打辅助的，如mode、flag、count、pos之流。inode是任何种类文件的组织形式，不只是硬盘文件，常规文件。
  * inode描述了文件，类似于文件头，根据它我们可以得到文件的基本信息，并且得到读写文件数据的路径；文件句柄则是用它来找到文件的接口，是提供给用户程序使用的，它在某种意义上可以认为是内核开放给用户程序的，能代表文件的东东。
  * 对于非块设备，inode中的字段会被赋予不同的含义。比如对于字符设备，zone[0]中一般是设备号；对于pipe设备，size,zone[0],zone[1]分别是指向缓冲队列的三个指针（data/head/tail）。所以这种组织形式的出发还是块设备，但是在其他设备种类上做到了很好的兼容。

* 和其他数据不同的是，文件需要存储在硬盘里，可以断电保持，在下次启动时由操作系统载入。所以文件的基础信息，也就是inode同样需要保存在硬盘里。
  * 当操作系统载入一个文件时，会读取硬盘中的inode信息，并赋给内存中的inode，但是要注意，两者有着不同的数据结构，分别是：d_inode和m_inode。前者的信息基本就是我们用ls所看到的信息：归属用户、权限、类型、长度、修改时间、链接数等基本属性，还有标记了block位置的zone[]数组。而内存中的m_inode更加丰富，还有dev，imount，以及各种标识位count，dirt，lock，update，等待列表wait等等。
  * 值得注意的是，zone[]间接块并不属于文件的数据块，但是同样被它持有。事实上，只要申请的时候将位图置1；释放的时候将位图恢复就可以。并不需要所有的数据块都被zone[]映射。

* 文件系统、设备、文件、块
  * 将块以散列map的方式组织起来，就形成了文件，其中的映射管理结构，就是inode。一个设备有很多个块，它们可以组成多个文件，为了可以访问和控制这么多的文件，我们需要用一定的秩序去安排它们，形成一些彼此的约定，这就是文件系统了，管理的机构就包括了superblock，imap，bmap，inodetable。
  * 经过这样的组织以后，一个设备就是完整的。我们从它的入口出发，可以访问到它存在的所有文件，不会有遗漏，也不会有混乱。这就是文件系统的功效。所以在这个层级上，文件系统=设备>文件>块。
  * 我们有两种不同的视角去看设备中的数据。从硬件管理的视角来看，它们就是一个一个的块，按顺序排列，前面是一些管理机构，后面则是一个一个的数据块，分别对应设备递增的逻辑块号；从用户的角度来看，一个文件系统中包含了多个文件，每个文件拥有它自己的数据流。
  * 协同这两种视角的其中一个重要问题是，从文件的数据块号，转换为设备的逻辑块号。这就是bmp()所提供的。
  * linux提供了文件系统的自由挂载（mount），允许一个文件系统，任意挂载在某个目录文件上。这使得多个文件系统可以被轻松自然地组织在一起，而整个计算机可以有多个设备、多个子文件系统，但最终只有一个根文件系统。
  * 此时，根文件系统>设备=子文件系统>文件>块。

* mount
  * 什么是文件系统？文件系统向上提供的，就是从filename到inode的路径。而mount，就是让这种路径可以跨越不同的设备，将某个设备的root inode与另一个文件系统中的普通inode重合，从而让不同的设备归结到一条总的路径上。




***
### 文件操作脉络：open/close

以一个文件的打开，来看整个文件系统是如何运作，各个模块是如何相互配合的。

* 用户程序申请打开一个文件，传入filename；此时可能还会经过运行库的一次中转，最终进行系统调用open，通过int80的软中断陷入内核。

* 在系统调用处理函数sys_open()中，首先要做的是完成从文件名filename到文件句柄对象file的转换。file中核心的数据结构就是inode。
  * 对于每一个进程，都有一个打开的文件数组filp[]，其中元素是file* ；操作系统还会维护一个文件表file_table[]，其中元素是真正的file对象。所以第一步，就是要找空闲的槽位，如果没有则打开失败或者陷入等待。
  * 同时，os还维护一个inode_table，我们同样需要拿到一个槽。一般是通过get_empty_inode()实现的。
  * 做完这些准备工作，真正的工作是完成从filename到inode的转换，这个工作是由namei/open_namei()完成的。

* namei()
  * 如何从filename到inode？每一级目录都可以视为一个文件（类型为d），它的数据块中存放的数据是一个个整齐排列的de（dir_entry，其中是dirname-entry map对)。
  * 我们可以分析filename的路径，从根目录（/，current->root）或者当前目录（.，current->pwd）的inode开始找起。在每一级的查找中，我们都可以通过查找到dirname-entry对应的值对，找到下一级dir文件对应的inode。
  * 在这个过程里，需要循环调用两个函数：iget/find_entry，前者是用dev+inr，利用super_block信息，从磁盘中读出inode信息。后者则是用inode+name，找到下一级文件的entry。

* 如果没找到，则创建。
  * 首先，文件的上层目录必须是存在的，否则就会发生错误。如果文件本身不存在，且相关标志位不冲突，就需要创建该文件。
  * 如何创建一个文件？核心还是建立inode。另外还有完成inode的映射管理（包括从内存m_inode到磁盘d_inode，以及imap的更新），数据映射（block映射，从文件的数据块号到设备的逻辑块号）。
  * 如何创建inode？调用bitmap.c提供的函数new_inode(dir->dev)，将inode加入到inode_table，更新imap并设置其bh为dirt。因为在m_inode中保存有它的相关信息（dev，inr，dirt等），所以在sync时，os检测到它的dirt，就会调用write_inode，将它写回磁盘的对应位置。这完全不需要使用者操心。inode和buf一样，本身是完整的（其中有着浓厚的面向对象思想）。

* read_inode/write_indoe
  * 在iget中，如果在table中没有找到，就要调用read。在iput中，只会count--，等到同步时才会write。
  * 在read/write中，都需要先计算得到dev和blk，并调用bread将对应块读出来，然后直接更改bh-data，并将bh->dirt置位。之后就交给bh自己处理了。
  * 这在superblock，imap/bmap的处理是一样的。
  * 值得注意的是，imap/bmap中的元素是bh* ，也就是一个完整的buffer块。所以如果要申请新的inode，先需要通过sb拿到imap，再一一扫描找到0项，计算其inr值；然后将该位置1，将该bh置dirt，将dev+inr设置给新的inode，同时设置inode 其他参数，比如时间、所属者等。当然最重要的是，将该inode的dirt置1。

* inode的缓存
  * inode_table限制了系统同时打开inode的个数，同样提供了inode的缓存。当我们打开一个新的inode时，要查看它是否已经存在于table中了。
  * 当我们创建一个新文件时，这不需要考虑——因为文件系统都还不存在，缓存区中不可能存在。
  * 当我们打开一个已经存在的inode时，需要遍历该table，比较dev和inr，如果已经在了，就将其count++，表示持有。这是在iget中进行的；同样，类似于bh，我们返回该inode，用户在使用完之后，必须手动调用iput进行释放，其中会将count--。
  * 当我们再次调用iget，并触发get_empty_inode时，就会进行count判定，如果count为0就表示空闲；此时就可以占有它；如果它是dirt，就需要先写回。在写回时进程需要陷入等待（写回是通过调用ll_rw_blk那一路子所实现的），因为硬盘写的操作是异步的。
  * 在睡眠中，什么事情都有可能发生，比如在你醒来之前这个inode槽又被新插入的进程给占据了；所以需要把判定流程重复执行一次，直到不再需要写回，不再需要wait_on，一次通过。这里的实现使用了c语言中的一大忌讳goto，似乎还挺好用。
  * 上面的寻找-判断-写回-等待-goto流程，在内核中非常普遍，superblock，inode，bh，基本都是这样搞的，非常典型。在后面会贴上代码。

* 返回fd
  * 拿到返回到inode之后，进行一些判定，在file对象中填入其他的一些参数，比如flag，mode之类的，就可以返回fd了。
  * 至此sys_open()执行完，fd通过%eax返回给用户程序。

* inode，file，fd
  * 可以对应到三张表：inodetable，filetable，filp。用户程序只能持有fd，fd实际上就是filp的索引。当我们要通过fd访问到inode时，大概的过程是这样的：filp[fd]->inode。
  * 当然这个过程只有内核才能做到，因为这三个数据结构都是在内核里。其中inode_table/file_table都是内核自己维护，每个进程私有的filp则存在于task_struct中，也是在内核空间（和内核栈在同一页）。
  * 这三个表有很多类似点，比如都有长度限定（废话，当然有），都没有什么链表的特殊处理（毕竟短，顺序遍历不费事）。
  * 它们也有不同的地方，比如inode和file都有count属性，也就是说，多个file可以指向同一个inode；多个fd可以有同样的file* 指针。
  * 前者在iget中，count++的时候发生的。后者则更多是不同进程之间可以持有同一个文件，比如fork/do_exec，创建并重新载入一个子程序的时候，没有设置close_on_exec的文件都会被继承，最典型的就是终端文件0/1/2。

* close文件
  * 用户程序通过filename打开文件，获得句柄；通过传入fd，调用sys_close关闭文件。
  * 在close中会先执行file->count--，如果count不为0，表示还有人持有，就不管；否则表示这个文件不再有人持有，需要释放掉它所占用的inode，因此调用iput(file->inode)。
  * 在iput中，先要对inode的类型进行判定，因为不同类型的文件，处理方式是不同的。比如pipe文件，终端文件，常规文件就很不同。
  * 对于pipe文件，如果还有另外的持方，则要尝试唤醒；如果没有人再持有了，就要释放掉它所占用的内存（用了一个bh作为数据缓存），并且将其复位。
  * 对于常规文件，要判断是否还有人持有；要判断它的link数，如果为0则意味着文件已经被删除了，需要释放掉所有的资源；还需要根据它的dirt为决定是否调用write_inode。


***
### 文件操作脉络：read/write

用户程序通过调用read/write对文件进行读写。底层发生了什么？以read为例。

* 首先触发系统调用sys_read(fd,buf,count)，用户会传进来文件句柄fd，用户缓存buf和需要读的字节count。至于从哪里读，在file文件中有字段pos，会标志出当前的位置。
  * 从这里也可以看出file和inode的关系：一个inode可以对应多个file，每个file都有自己的flag、count、mode、pos。如果说inode是静态的数据本身，file则是一个打开之后的动态数据流，是用户和数据交互之后的结果。
  * 在read里，需要根据设备的类型，来调用不同的处理函数（这是面向对象思想）。file->inode-mode，其中有设备的主设备号major和子设备号minor。在linux0.11，会根据major的不同，分别调用如下几种函数：block_read/rw_char/file_read/read_pipe（不要问为什么四个名字的read没有都在前面或者后面，我也不知道……）


* file_read()
  * 这是常规文件会选的路。需要传入参数inode，file，buf，count。
  * file_read的底层同样是块设备，所以它和block_read一样，都会调用buffer.c提供的接口bread()。
  * bread需要的参数是dev+block。所以需要先进行转换，将file->pos转换为文件的数据block号，再调用由inode.c提供的bmap()将数据block号转换为dev的逻辑block号，最终传给bread使用。
  * bread返回的是bh，所以还需要将bh中的数据复制到buf中，然后将其释放。在复制的时候要注意，bh在内核区，buf在用户区，复制的时候需要进行特殊的处理。
  * 这里还需要注意的是，如果一个文件已经被打开了，那么它的inode的count一定是大于0的，也就是说可以直接访问到，不存在需要在读写的时候还跑去磁盘找inode。


* file_write()
  * 写和读很相似。同样是根据设备的种类选择不同的处理函数。对于常规文件，则会进入filewrite。
  * 写的底层原理和读是一样的，先通过bread将对应block给读上来，再改写其data，置dirt。注意buffer.c并没有提供一个什么bwrite之类的接口。只有bread系列。
  * 写的时候会出现的一个问题是，它可能拓展文件的大小。也就是说磁盘之前这个block是未使用的，现在才分配给这个文件。这个时候就不调用bmap了，而是调用inode.c提供的create_block，再间接调用bitmap.c提供的new_block，在其中的处理和new_inode类似，通过super.c提供的get_super()拿到sb，在它的bmap中找0位，置1，置dirt；修改inode的zone，增加新的block映射，置dirt，将这个block读到bh，清0，准备写入……
  * 对应的还有free_block。如果一个文件被修改后需要重新写入，也有一种可能的方式是全部重写（比如对于文本文件）。


* 其他设备
  * 字符设备和文件系统几乎没什么关系，它不涉及到底层的buffer，只是利用inode和file完成文件概念。它的读写是从rw_char出发，根据设备类型再进行分支（根据一个table），比如tty_write或者rw_memory之类的。（这一块没搞太懂，需要补充）
  * 管道设备pipe的操作定义在pipe.c里，主要函数是read_pipe/write_pipe/sys_pipe。分别是读/写/建立。管道设备在建立时，会申请两个文件，打开的模式分别为读和写，都指向这个pipe_inode，并在内核中申请一个空闲页作为队列，利用inode的一些字段作为队列的头尾数据指针。在读写时，两个进程可以分别持有两个文件，并且在读写的过程里花式同步，相互等待和唤醒，配合得非常不错。



* 4个标志位：dirt，count，update，lock
  * 标志位四大天王，比如bh，inode，superblock之类的都有其中的。
  * update表征数据是否有效。比如我拿到一个bh，我需要看看它的update位，如果无效则需要重新读；count表征数据的引用数量，以此判断它是否空闲以及是否可以回收，一般情况下每次用户申请资源都会造成count++，而用户每一次手动释放都会导致count--，而资源的回收是由内部管理的，比如在查找空闲对象的时候；lock则表征数据是否上锁，一般在需要同步内存和磁盘的数据时需要上锁，当我们要访问一个可能被上锁的对象的时候，都应该用wait_on()形式去访问，避免数据不一致；dirt则是脏标志位，主要在回收时用于判断是否需要写回磁盘。
  * 一个简单的例子：在要同步块数据到磁盘时，我们需要先将bh->lock置位（在make request时），再进行数据同步；在request处理完成后，在end_request中，将lock复位，并wakeup等待的进程，同时要根据数据是否有效设置update位。等待进程从wait_on_buffer()中醒来后，需要根据update来判断数据是否有效。


* 竞争和保护：非抢占式内核
  * linux0.11是非抢占式内核。这意味着内核代码在执行中不会被调度——会将其控制路径打断的，只有中断处理程序。这有着非常深远的意义。
  * 如果我们已知在执行时不会发生调度，那么对全局变量的使用就只需要关注是否和使用了这些变量的中断发生冲突，其他进程我们不用考虑，因为除非我们自己使用完了主动放弃cpu，它们是得不到执行机会的。这也就意味着，只要在引发中断操作之前，将中断会更改的全局变量上锁，内核就完成了基础的保护。
  * 因为，此时一个内核控制流在执行时，一旦它可以穿过锁，就说明暂时没有程序会引发可能上锁的中断；以后也不会，因为其他程序没有机会执行。
  * 比如，在每一次读写磁盘时，需要对buf上锁。此时任意访问该buf的进程都会陷入等待，直到这个写操作彻底完成，不会再有中断到来；而内核代码只要发现自己可以跃过这个锁，就可以放心地读写了，而不需要给它上锁。
  * 典型代码如下： 
```C
    while (inode < NR_INODE+inode_table) {
        if (inode->i_dev != dev || inode->i_num != nr) {
            inode++;
            continue;
        }
        wait_on_inode(inode);
        if (inode->i_dev != dev || inode->i_num != nr) {
            inode = inode_table;
            continue;
        }
        inode->i_count++;
        ......
    }
```

**其他**

* 系统调用：在open.c，namei.c,super.c等文件里，还定义了大量的和文件系统相关的系统调用，比如创建、删除、更改目录，修改文件的用户、权限，挂载和卸载文件系统等。

* 错误处理与返回： 如果是重大错误，就直接panic()死机；如果是执行失败，就会返回错误码。错误码一般是内核提供给外界的信息（比如系统调用返回），在内核内部返回的方式更加灵活随意。



***
## 综合探讨

### tty0与fd0/1/2

当进程1通过系统调用open打开tty0的时候，它以tty0的inode为核心，建立了一个文件，放在file_table里，同时在进程的filp[0]里，存放了这个文件的指针file* ；当它通过dup()复制两份时，实际上并没有复制文件或者inode，而仅仅是让filp[1]和filp[2]也都存放这个指针。

所以，此时fd0/1/2，全部指向这一个文件，其中的inode指向tty0。

在这里我们必须要厘清的一个概念是：之前我认为，fd0/1/2，是对应三个不同的文件，有三个不同的inode，比如0对应键盘文件，1对应console输出文件，2再对应个什么；或者认为它们对应三个文件，但包含同一个inode指针，只是各自的打开方式、位置不同。

但是tty设备不是块设备而是字符设备，我不知道它是否可以被多个文件包含（初步想一下会觉得很奇怪）。但是作为一个文件打开（所以需要以读写模式打开），在不同的上下文里被调用tty_read/write，这是可行的。

如果重定向呢？那就是更改filp[]，让fd项指向了不同的file，而不会改变file和inode的关系。比如我们将fd2重置为某个打开的文本文件testfile。这样做的好处是编程时可以只面向fd，而在实际执行时再来更改fd与file之间的连接，有很大的灵活性。

***
### 内存管理：内核区、缓存区、主存区

内核将物理内存分为了两个部分，以low_men为分界，第一部分为内核代码/数据部分，第二部分为主内存区。在linux0.11中，如果总内存为16M，**那低1M为作为内核的代码/数据部分，永远驻留在内存中，它们不受mem_map[]管理，在载入时就已经确定位置和布局**；剩下的15M中，3M作为系统缓存，在mem_map[]初始化时设置为used，受mem_map管理，但也长期占用；剩下的12M，才是真正交给内存管理分配释放的区域。

所以，get_free_page()所返回的地址，一定是从4M-12M的区域中的某一页。

那当我们新运行一个进程，在内核中就需要新申请一页作为它的内核栈，同时存放它的task_struct，这时候我们拿到的页在哪里？还属于内核吗？

首先，拿回的页的物理位置是不确定的，可能在16M物理内存中的任何位置。同时这并不影响它属于内核。因为内核在gdt中的数据段、代码段范围都是从0-16M，同时在内存的开始处初始化了4个页表项对应的页表，也就是说可以直接访问到所有的16M内存。而内核之外的进程又不可能通过页表映射到这些物理页面去，那它为什么不是属于内核的呢？

所以，在物理内存上，前面的1M是内核原始数据，但并不是说后面的物理内存就属于用户了。内核和用户的分界并不是在物理内存上，而是整个分段、分页机制所决定的。尤其是分段机制，gdt、ldt表这些东西。


***
### 特权级

当内核放弃主宰权，跳跃到进程0，运行同样的代码，但是从内核态变成了用户态，从“内核”变成了“进程”。

“内核”和“用户”的显著标志，就是cs/ds的段选择符，前者是0x08,0x10，后者是0x0f，0x17。前者使用gdt表项，dpl为0；后者使用ldt表项，dpl为3。

事实上，内核从主宰态跳跃到进程0，esp、eflags、eip都没变，就是通过iret把cs、ss从前者变为了后者，之后又手动将0x17赋给了其他寄存器，ds、es、fs、gs。

在进程0的ldt中，代码段和数据段都是0-640kb，dpl为3。这意味着它访问的范围包括了整个的内核原始模块，但不包括缓存区和主内存区。而访问这些内核原始模块的特权级是3！也就是用户态下的进程0就可以自由访问。

从这里我们看出，特权级的概念，更多的是落在分段机制上，落在段表（gdt、ldt、idt）这些Gate上，而无关实际的线性地址、物理地址、页表、物理内存、代码、数据。它是作用于逻辑地址的，控制逻辑地址到线性地址转换的这一环，在这个层级上将各个进程进行分隔，将用户与内核进行分隔。

所以，同样是内核原始模块，经过同一个页表访问，其中有着关键的内核代码和数据，它们既可以在gdt中被定义为dpl0，也可以在进程0的ldt中被定义为dpl3。


***
### 虚拟内存与页表映射

我们可以从进程0、进程1的ldt构建、页表、内存分布中探寻出很多有意思的点。

在linux0.11中，4g虚拟内存的分布方式很特别，所有进程在一个大的空间里，各自分一段64M，彼此不重合，所以完成分隔。其中进程0比较特殊，它是内核手动构建的，只有640kb，比内核能访问到的16要小不少。

但从进程1开始，就是各自有64M的虚拟空间了，其中有代码段、数据段、堆区、栈区。事实上，现在还没有正式分区，堆区是从数据区向上增长的，仅仅用brk标志其范围；而栈区从顶往下扩展，其中还包括了参数和环境变量区。所以对于堆区的使用，就是向上拓展brk的值，一次申请一长段，再在运行库中分成一小块一小块地给用户程序使用。


**task 0**
当内核跃入进程0，从gdt切换为ldt时，我们可以自然地想通其中的过程：毕竟只是从逻辑地址转换到线性地址的Gate变了，而它们除了表项位置不同、权限不同之外，base是一样的。

所以对于一个简单的地址访问，比如 mov eax, 0x10000，即为64K位置，假设为系统栈所在。在跳跃之前，它的访问过程也许是：
* 使用默认的ds寄存器，段选择符为0x10，指向内核数据段（gdt表项）。
* 在gdt中得到段基址，完成从逻辑地址到线性地址的转换（事实上，对于普通的代码/数据段，base为0；而且gdt数据段表项会被缓存在ds寄存器的隐藏部分中），比如结果也为64K。
* 通过线性地址64K，查询页表，落在第一个页目录项，第一个页表上，第十七项，偏移为0。最后转换为物理地址64K。
* 将64K（0x10000）放到地址线上，访问到物理内存。

跳跃之后呢，则可能是：
* 使用默认的ds寄存器，段选择符为0x17，指向用户数据段（ldt表项）。
* 在ldt中得到段基址，完成从逻辑地址到线性地址的转换，base同样为0，得到线性地址64K（0x10000）。
* 通过线性地址64K，查询页表，落在第一个页目录项，第一个页表上，第十七项，偏移为0。最后转换为物理地址64K。
* 将64K（0x10000）放到地址线上，访问到物理内存。

因为两者虽然通过段寄存器查询的段表不同，经过的门不同，但是会得到同样的线性地址，所以经过的页表项、物理地址都是一样。不会有任何冲突。

**task 1**
当在进程0创建进程1，在fork系统调用中，会初始化它的task_struct，设置tss、ldt，复制页表。其中的ldt中的base，从0变为了64M；而在复制页表时，内核原始模块段的处理也会很不一样。我们看一看memory.c中的代码：
```C
int copy_page_tables(unsigned long from,unsigned long to,long size)
{
    //……
    //省略其他代码
        for ( ; nr-- > 0 ; from_page_table++,to_page_table++) {
            this_page = *from_page_table;
            if (!(1 & this_page))
                continue;
            this_page &= ~2;
            *to_page_table = this_page;
            if (this_page > LOW_MEM) {
                *from_page_table = this_page;
                this_page -= LOW_MEM;
                this_page >>= 12;
                mem_map[this_page]++;
            }
    //……        
    invalidate();
    return 0;
}

void un_wp_page(unsigned long * table_entry)
{
    unsigned long old_page,new_page;

    old_page = 0xfffff000 & *table_entry;
    if (old_page >= LOW_MEM && mem_map[MAP_NR(old_page)]==1) {
        *table_entry |= 2;
        invalidate();
        return;
    }
    if (!(new_page=get_free_page()))
        oom();
    if (old_page >= LOW_MEM)
        mem_map[MAP_NR(old_page)]--;
    *table_entry = new_page | 7;
    invalidate();
    copy_page(old_page,new_page);
}   

```
在复制页表时，一定会将新页表的页表项设置为只读；对于老页表，只有当物理地址不处于内核模块区（>LOW_MEM），才会设置为只读。所以，在fork之后，如果进程0继续执行且改变内存（比如调用函数），就会造成直接对进程1造成影响；反过来，如果进程1执行，改变了内存（比如继承自进程0的位于内核代码区的用户栈），那会触发page fault，进而执行un_wp_page()。

在up_wp_page()处理中，我们可以看到，只有位于非内核模块区，且物理引用数等于1的时候，才会直接更改页表项的读写位，然后返回之后再次去读。对于内核模块区的冲突，一定会使用get_free_page()重新分配一个页。这个新的页，就作为了进程1的用户栈。

（这个页如何释放？进程1不需要释放……那是否进程1还会改写其他的内核区数据，导致再次触发page fault？应该是没有的，它和其他所有普通的进程一样，都依赖于系统调用接口取得内核提供的系统服务，而不会自己在dpl=3的状态下直接取用内核数据，虽然这在理论上是它可以做到的；同时它之所以要能访问到内核模块，是因为它自己的代码本身就在内核模块中；但是我们不妨把它当做普通的进程来看待，它的权限级别、运行方式都和普通进程一样，不同的仅仅是为了能取得自己的代码，所以它会被映射到内核模块中）

对于tss，和普通的fork过程一样，tss会被初始化为父进程用户态的“断面”，也就是进程0在调用fork时的瞬态。所以它仍然是在main中，eip指向fork系统调用的下一条指令，esp指向进程0手动指令的用户栈，cs和ss则分别是0x0f和0x17。

当fork()返回时，如果进程0的时间片还没有到0，它就会继续执行，进入死循环里。如果fork()是以包装好的函数调用的形式提供，那么它从int80的系统调用中返回后，它会落在fork()中，而在ret时，会从栈中弹出返回地址并跳转；如果pause是以函数形式提供，它还会有函数调用，比如将返回地址压栈等——这一些关于函数的操作都会改变进程0的用户栈，在进程1还没有得到机会执行的时候，就已经将栈数据弄乱了，对于进程1，栈是数据不一致的。

所以进程0中的fork/pause，不能以函数调用的形式提供，因为写时复制对于被复制方而言是无效的。linux0.11中实现的方式是用内敛函数的形式提供，类似于宏，直接使用代码。

当等到进程1第一次被调度到时，它会直接进入断面开始执行——一般而言都是对返回值的一个是否等于0的判断。毫无疑问，它同样处于用户态。那么它此时的内存访问又是怎么样的呢？还是以进程0的假定用户栈地址64K，0x10000为例：
* 使用默认的ds寄存器，段选择符为0x17，指向用户数据段（ldt表项）。
* 在ldt中得到段基址，完成从逻辑地址到线性地址的转换，base同样为64M，得到线性地址64M+64K（0x4010000）。
* 通过线性地址64M+64K（0x4010000），查询页表，落在第17个页目录项，第十七个页表上，第十七项，偏移为0。最后转换为物理地址64K。
* 将64K（0x10000）放到地址线上，访问到物理内存。

到这里还没有结束：
* 访问物理内存时，发现是只读，触发page fault，进入异常处理流程。
* 自动陷入内核，在un_wp_page()中，通过get_free_page()申请新的一页，比如在4M+64K（0x410000），并重新设置页表。之后返回。
* 再次访问线性地址64m+64k，查询页表，还是在第17个页目录项，第十七个页表，第十七项，偏移为0。得到物理地址4M+64K（0x410000）。
* 将4M+64K（0x410000）放到地址线，访问到物理内存。

所以，进程1可以通过<ldt - 页表>直接访问到自己的代码（在内核模块中）和数据。


***

## 附录

### 初始化代码

以下是linux0.11中，各种init代码的集合。

```C

#define move_to_user_mode() \
__asm__ ("movl %%esp,%%eax\n\t" \
    "pushl $0x17\n\t" \
    "pushl %%eax\n\t" \
    "pushfl\n\t" \
    "pushl $0x0f\n\t" \
    "pushl $1f\n\t" \
    "iret\n" \
    "1:\tmovl $0x17,%%eax\n\t" \
    "movw %%ax,%%ds\n\t" \
    "movw %%ax,%%es\n\t" \
    "movw %%ax,%%fs\n\t" \
    "movw %%ax,%%gs" \
    :::"ax")


void main(void)     /* This really IS void, no error here. */
{           /* The startup routine assumes (well, ...) this */
/*
 * Interrupts are still disabled. Do necessary setups, then
 * enable them
 */
    ROOT_DEV = ORIG_ROOT_DEV;
    drive_info = DRIVE_INFO;
    memory_end = (1<<20) + (EXT_MEM_K<<10);
    memory_end &= 0xfffff000;
    if (memory_end > 16*1024*1024)
        memory_end = 16*1024*1024;
    if (memory_end > 12*1024*1024) 
        buffer_memory_end = 4*1024*1024;
    else if (memory_end > 6*1024*1024)
        buffer_memory_end = 2*1024*1024;
    else
        buffer_memory_end = 1*1024*1024;
    main_memory_start = buffer_memory_end;
#ifdef RAMDISK
    main_memory_start += rd_init(main_memory_start, RAMDISK*1024);
#endif
    mem_init(main_memory_start,memory_end);
    trap_init();
    blk_dev_init();
    chr_dev_init();
    tty_init();
    time_init();
    sched_init();
    buffer_init(buffer_memory_end);
    hd_init();
    floppy_init();
    sti();
    move_to_user_mode();
    if (!fork()) {      /* we count on this going ok */
        init();
    }
/*
 *   NOTE!!   For any other task 'pause()' would mean we have to get a
 * signal to awaken, but task0 is the sole exception (see 'schedule()')
 * as task 0 gets activated at every idle moment (when no other tasks
 * can run). For task0 'pause()' just means we go check if some other
 * task can run, and if not we return here.
 */
    for(;;) pause();
}

void init(void)
{
    int pid,i;

    setup((void *) &drive_info);//sys_setup() - mount_root()
    (void) open("/dev/tty0",O_RDWR,0);
    (void) dup(0);
    (void) dup(0);
    printf("%d buffers = %d bytes buffer space\n\r",NR_BUFFERS,
        NR_BUFFERS*BLOCK_SIZE);
    printf("Free mem: %d bytes\n\r",memory_end-main_memory_start);
    if (!(pid=fork())) {
        close(0);
        if (open("/etc/rc",O_RDONLY,0))
            _exit(1);
        execve("/bin/sh",argv_rc,envp_rc);
        _exit(2);
    }
    if (pid>0)
        while (pid != wait(&i))
            /* nothing */;
    while (1) {
        if ((pid=fork())<0) {
            printf("Fork failed in init\r\n");
            continue;
        }
        if (!pid) {
            close(0);close(1);close(2);
            setsid();
            (void) open("/dev/tty0",O_RDWR,0);
            (void) dup(0);
            (void) dup(0);
            _exit(execve("/bin/sh",argv,envp));
        }
        while (1)
            if (pid == wait(&i))
                break;
        printf("\n\rchild %d died with code %04x\n\r",pid,i);
        sync();
    }
    _exit(0);   /* NOTE! _exit, not exit() */
}





void mem_init(long start_mem, long end_mem)
{
    int i;

    HIGH_MEMORY = end_mem;
    for (i=0 ; i<PAGING_PAGES ; i++)
        mem_map[i] = USED;
    i = MAP_NR(start_mem);
    end_mem -= start_mem;
    end_mem >>= 12;
    while (end_mem-->0)
        mem_map[i++]=0;
}

void trap_init(void)
{
    int i;

    set_trap_gate(0,&divide_error);
    set_trap_gate(1,&debug);
    set_trap_gate(2,&nmi);
    set_system_gate(3,&int3);   /* int3-5 can be called from all */
    set_system_gate(4,&overflow);
    set_system_gate(5,&bounds);
    set_trap_gate(6,&invalid_op);
    set_trap_gate(7,&device_not_available);
    set_trap_gate(8,&double_fault);
    set_trap_gate(9,&coprocessor_segment_overrun);
    set_trap_gate(10,&invalid_TSS);
    set_trap_gate(11,&segment_not_present);
    set_trap_gate(12,&stack_segment);
    set_trap_gate(13,&general_protection);
    set_trap_gate(14,&page_fault);
    set_trap_gate(15,&reserved);
    set_trap_gate(16,&coprocessor_error);
    for (i=17;i<48;i++)
        set_trap_gate(i,&reserved);
    set_trap_gate(45,&irq13);
    outb_p(inb_p(0x21)&0xfb,0x21);
    outb(inb_p(0xA1)&0xdf,0xA1);
    set_trap_gate(39,&parallel_interrupt);
}


void blk_dev_init(void)
{
    int i;

    for (i=0 ; i<NR_REQUEST ; i++) {
        request[i].dev = -1;
        request[i].next = NULL;
    }
}


void chr_dev_init(void)
{
}

void tty_init(void)
{
    rs_init();
    con_init();
}


static void time_init(void)
{
    struct tm time;

    do {
        time.tm_sec = CMOS_READ(0);
        time.tm_min = CMOS_READ(2);
        time.tm_hour = CMOS_READ(4);
        time.tm_mday = CMOS_READ(7);
        time.tm_mon = CMOS_READ(8);
        time.tm_year = CMOS_READ(9);
    } while (time.tm_sec != CMOS_READ(0));
    BCD_TO_BIN(time.tm_sec);
    BCD_TO_BIN(time.tm_min);
    BCD_TO_BIN(time.tm_hour);
    BCD_TO_BIN(time.tm_mday);
    BCD_TO_BIN(time.tm_mon);
    BCD_TO_BIN(time.tm_year);
    time.tm_mon--;
    startup_time = kernel_mktime(&time);
}

void sched_init(void)
{
    int i;
    struct desc_struct * p;

    if (sizeof(struct sigaction) != 16)
        panic("Struct sigaction MUST be 16 bytes");
    set_tss_desc(gdt+FIRST_TSS_ENTRY,&(init_task.task.tss));
    set_ldt_desc(gdt+FIRST_LDT_ENTRY,&(init_task.task.ldt));
    p = gdt+2+FIRST_TSS_ENTRY;
    for(i=1;i<NR_TASKS;i++) {
        task[i] = NULL;
        p->a=p->b=0;
        p++;
        p->a=p->b=0;
        p++;
    }
/* Clear NT, so that we won't have troubles with that later on */
    __asm__("pushfl ; andl $0xffffbfff,(%esp) ; popfl");
    ltr(0);
    lldt(0);
    outb_p(0x36,0x43);      /* binary, mode 3, LSB/MSB, ch 0 */
    outb_p(LATCH & 0xff , 0x40);    /* LSB */
    outb(LATCH >> 8 , 0x40);    /* MSB */
    set_intr_gate(0x20,&timer_interrupt);
    outb(inb_p(0x21)&~0x01,0x21);
    set_system_gate(0x80,&system_call);
}

void buffer_init(long buffer_end)
{
    struct buffer_head * h = start_buffer;
    void * b;
    int i;

    if (buffer_end == 1<<20)
        b = (void *) (640*1024);
    else
        b = (void *) buffer_end;
    while ( (b -= BLOCK_SIZE) >= ((void *) (h+1)) ) {
        h->b_dev = 0;
        h->b_dirt = 0;
        h->b_count = 0;
        h->b_lock = 0;
        h->b_uptodate = 0;
        h->b_wait = NULL;
        h->b_next = NULL;
        h->b_prev = NULL;
        h->b_data = (char *) b;
        h->b_prev_free = h-1;
        h->b_next_free = h+1;
        h++;
        NR_BUFFERS++;
        if (b == (void *) 0x100000)
            b = (void *) 0xA0000;
    }
    h--;
    free_list = start_buffer;
    free_list->b_prev_free = h;
    h->b_next_free = free_list;
    for (i=0;i<NR_HASH;i++)
        hash_table[i]=NULL;
}   

void floppy_init(void)
{
    blk_dev[MAJOR_NR].request_fn = DEVICE_REQUEST;
    set_trap_gate(0x26,&floppy_interrupt);
    outb(inb_p(0x21)&~0x40,0x21);
}

void hd_init(void)
{
    blk_dev[MAJOR_NR].request_fn = DEVICE_REQUEST;
    set_intr_gate(0x2E,&hd_interrupt);
    outb_p(inb_p(0x21)&0xfb,0x21);
    outb(inb_p(0xA1)&0xbf,0xA1);
}

void con_init(void)
{
    //略……
}

void rs_init(void)
{
    set_intr_gate(0x24,rs1_interrupt);
    set_intr_gate(0x23,rs2_interrupt);
    init(tty_table[1].read_q.data);
    init(tty_table[2].read_q.data);
    outb(inb_p(0x21)&0xE7,0x21);
}



//在sys_setup()中被调用，只会触发一次
//sys_setup会在init()中最开始被调用，形式为setup()
void mount_root(void)
{
    int i,free;
    struct super_block * p;
    struct m_inode * mi;

    if (32 != sizeof (struct d_inode))
        panic("bad i-node size");
    for(i=0;i<NR_FILE;i++)
        file_table[i].f_count=0;
    if (MAJOR(ROOT_DEV) == 2) {
        printk("Insert root floppy and press ENTER");
        wait_for_keypress();
    }
    for(p = &super_block[0] ; p < &super_block[NR_SUPER] ; p++) {
        p->s_dev = 0;
        p->s_lock = 0;
        p->s_wait = NULL;
    }
    if (!(p=read_super(ROOT_DEV)))
        panic("Unable to mount root");
    if (!(mi=iget(ROOT_DEV,ROOT_INO)))
        panic("Unable to read root i-node");
    mi->i_count += 3 ;  /* NOTE! it is logically used 4 times, not 1 */
    p->s_isup = p->s_imount = mi;
    current->pwd = mi;
    current->root = mi;
    free=0;
    i=p->s_nzones;
    while (-- i >= 0)
        if (!set_bit(i&8191,p->s_zmap[i>>13]->b_data))
            free++;
    printk("%d/%d free blocks\n\r",free,p->s_nzones);
    free=0;
    i=p->s_ninodes+1;
    while (-- i >= 0)
        if (!set_bit(i&8191,p->s_imap[i>>13]->b_data))
            free++;
    printk("%d/%d free inodes\n\r",free,p->s_ninodes);
}

//hd.c
int sys_setup(void * BIOS)
{
    static int callable = 1;
    int i,drive;
    unsigned char cmos_disks;
    struct partition *p;
    struct buffer_head * bh;

    if (!callable)
        return -1;
    callable = 0;
// 很多代码，主要涉及到硬盘的参数获取、初始化等

    rd_load();
    mount_root();
    return (0);
}

```