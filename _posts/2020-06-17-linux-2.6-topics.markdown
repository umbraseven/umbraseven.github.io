---
layout: post
title:  "Linux：综合主题漫谈"
date:   2020-06-17 16:30:00 +0800
categories: Linux
tags: Linux 
description: 
---

*  目录
{:toc}

***

## Hello,World!

编写一个c程序，输出“Hello,world!”到控制台。这个中间发生了一些什么？我们可以谈到到哪些话题？

### 准备

**编写一个c程序文件**

* hello.c文件：vfs，ext2/3，super_block、inode、dentry、file对象，各种对象的ops，通用接口read/write/open等的实现……

* 进程的io等待：睡眠、唤醒、等待队列；调度，时间片，优先级，进程状态……

* 磁盘的页高速缓存：sb的缓冲区高速缓存，脏页的写回。

* 磁盘的访问：通用块层、逻辑块设备、io调度、request、电梯算法、块设备驱动、设备驱动模型、sysfs、设备文件……

* 磁盘中断：中断处理函数，irq，内核的执行逻辑流，中断的通用处理流程，同步与保护，上半部和下半部……

* 内存：inode等对象的slab缓存，页高速缓存的页申请，伙伴系统，页框回收……

***
**编译**

（了解得很浅，简单过一下）

* 预处理：纯文本处理，其中应该同样有语法分析、词法分析等过程，从广义而言应该也是属于一次编译，只是输入输出均为文本。

* 词法分析：从字符流变为token集合。原理可以是有限状态自动机……

* 语法分析：建立抽象语法树，方法大概有两类，从左至右的回溯推导，和从右至左的移进规约，两者都可以加入前看符号来尽可能剪枝。还有一个隐含的主题是语法定义。另外通过表驱动，规定一定的语法输入格式，能写一个可以生成语法分析程序的程序……

* 语义分析：摆脱源码，基于ast分析，典型的就有类型检查。这一步似乎可以做很多事。

* 中间代码生成和各种优化：比如某些表达式的值静态推导，某些路径的优化等等。

* 汇编代码生成：

* 目标文件生成：除了code正文之外，还有许多其他的段，比如data、bss、符号表、重定位表等等。这是一个完整的elf文件，有文件头，段表等。

* 链接成可执行目标文件：静态链接，符号决议，重定位，生成elf文件。

***
### 执行

**开始执行**

* shell中输入命令hello，开始查找可执行的目标文件（其中有大量的文件系统、io操作）。

* fork()。创建一个子进程，父进程等待它执行完成。初始化必要的数据结构，比如task_struct、mm和页表等等，写时复制，共享页框。

* 子进程被调度。开始执行exec()。清理很多从父进程继承的资源，装载目标文件，重建虚拟地址空间，初始化各种线性区，建立执行文件的内存映射，修改用户态返回地址，布置堆栈等等。

* 共享库装载。动态链接程序，共享库的线性区映射，动态重定位，动态链接等，一切准备就绪，返回用户态，从入口点开始执行。

* 缺页异常。各种检查，线性区nopage方法，读文件，inode的address_space缓存，页表项修改，再次访问。（文件读，再一次穿过vfs、filesystem、cache、块设备处理层、io调度层、设备驱动、端口、总线、设备……）

***
**一条指令**

比如mov %eax ,(0xfffffff)（瞎写的，大意是从内存某个地址读一个int到eax寄存器）

寻址过程：

* 逻辑地址到线性地址：输入为逻辑地址，默认的段寄存器为ds，根据其中的段选择子，访问对应的gdt表项，做权限判断，得到段基址，加上逻辑地址偏移，生成线性地址。

* 线性地址到物理地址：开启分页模式后，所有的地址访问都是基于线性地址，通过mmu单元完成的。pg0地址存放在cr控制寄存器中。线性地址通过页表的逐级转换，最后变为物理地址，放到地址总线，访问内存。中间还有tlb加速页表项的寻址处理。


执行过程：

* 分阶段：

* 流水线：

* 状态机本质：

* 堆栈、寄存器、cpu的配合：

***
## 页框回收

想回收页框，要问的第一个问题是：在任意时刻，内存是如何被使用的？如何分布的？它可能被哪些对象占用？可以被释放吗？对象将来还需要使用它吗？内存是如何被内核组织和管理起来的？入口在哪里？如果要释放，需要修改哪些数据结构才能维持自洽？

所以页框回收是一个综合性问题。

***
### 内存分布

**内核如何使用内存？**

内核需要维持大量的数据结构，来管理进程，管理各种资源，典型的有进程描述符、mm和线性区描述符、页表、信号、中断子系统、调度子系统、文件系统等等，这些数据都是由内核管理，存放在内核空间。这些数据也许可以分为两类：

* 一类是独立于某个进程而存在的，典型的是设备和文件、物理内存page描述符、中断等。
* 一类是描述进程和进程所拥有的资源的，它们会随着进程的创建和终止而动态变化。

内核对内存的使用方式主要有两种，一种是页，一种是slab对象。页的分配是由伙伴系统来管理的，slab缓存同样是从伙伴系统里拿页的。

从页的最终去向上，可能已经被slab层拿来装对象了，可能还在slab中没有使用，可能在页高速缓存了，另外还有静态分配的内核代码和数据，还有内核的保留页和特别专用页，哪些页可以被回收？

* 怎么才叫回收？如果页是空的，或者信息无效，直接丢弃就可以。如果页中的信息有效，且能对应到磁盘，那就只需要写回磁盘，释放脏页。如果页中的信息有效，且没有磁盘文件对应，那就不能同步也不能丢弃了，但是可以交换到磁盘，本质上就是强行弄个文件给它装着，需要的时候再读进来。

* 在内核中，作为slab对象分配出去的内存，其中装的是各种内核数据结构，说不定什么时候就要使用，而且在很多内核执行流里，是不允许触发缺页异常的，自然也不能临时走io去磁盘读，阻塞当前的执行流，所以这一部分内存是不能交换出去，不能回收的。

* 另外的内核专用的，比如保留页、内核态堆栈页，比如内核的静态数据和代码区等，页都不能回收。

* 页高速缓存中的页呢？应该大都是可以同步和回收的，比如其中映射到文件，或者映射到块设备缓冲区的，都可以将数据同步到磁盘，然后释放该页。

* slab中还没有使用的页呢？直接拿去用就好。还有其他的一些缓存，比如目录项、inode的应该也可以回收。

***
**进程空间**

因为进程可能长时间睡眠，而且进程数量可能开很多，所以从原则上来说，进程所占用的页应该全部可以回收。

进程会通过哪些方式占用内存？进程地址空间是通过线性区来管理的，线性区有匿名区和映射区，对于映射区，可以同步后释放，比如可执行文件、共享库等的内存映射区；对于匿名区，可以交换后释放，典型的就是堆栈数据。

***
**关键点**

不管是内核还是进程，要想动态拿页，不管通过哪个入口、何种方式，不管中间多么曲折，最终都是要向伙伴系统申请的，它的基本单位一定是页。

比如进程，分配页的时机有哪些？

* 映射区访问，页表项无效，触发缺页异常，在内核的异常处理函数中，申请一个页（可能是共享），读进数据，然后将页表项映射过去。

* 匿名区扩展。增加堆线性区边界，等到访问时，缺页异常但地址合法，于是申请一个空白页。

所以，都是经过缺页异常，从伙伴系统手里，一次一个页拿到的。

***
内核呢？分配页的时机有哪些？

* 一种是直接的拿整页，甚至一堆整页，然后自行规划，比如page描述符大数组，或者载入比较大的模块时，应该是要拿到连续页框的。又比如页高速缓存，那就是一次一个页了。

* 更多的一种呢，是各种内核数据结构的申请。在用户态进程中，如果在堆中申请这种对象，其实并不会触发一次内存分配，而仅仅是线性地址分配。在内核中没有人提供这些服务，所以就需要有slab缓存了，而slab在空间不够时，会一次从伙伴系统里拿多个页。

当然在内核初始化时，要确定哪些页可以交给伙伴系统管理，还要各种分区（zone），确定水位、保留区等等，这样的细节非常多。

但是总体图景大概如上讨论，这样随着系统的运行，随着进程的创建和终止，文件的打开和关闭，我们也许能大概地有一个把握，内存中有哪些东西出现，有哪些东西消失，内存的大致分布，其中各种成分的拥有者、比例、性质、将来的预期……

至于具体的页框回收策略、原则、算法、流程等，在对内存管理有一定整体把握的情况下，应该是不难理解的。

一个具体的lru算法，仅仅是设计的结果，我们读它只能知其然。但它为何如何设计？设计者在设计它时所面对的、基于的是什么？他做了哪些思考？在我们能看到的选项A的背后，是否曾丢弃了选项BCDEF呢？设计者在每一步、每一个细节里，都面临了哪些选择，又为何会如此选择？

我想这些思考，对深入研究一个复杂系统而言是必须的。当然我现在还远远没有能力去深究这些，所以还只是纸上谈兵，做一些有趣的瞻望。

***
### 反向映射

如果知道了内存页的当前状态，那如何才能找到它们并完成释放？有两条路径：一种是先找到物理页的使用者，从它们手里拿回来再释放掉；一种是从物理页出发，释放可以释放的，再想办法告知使用者。

因为使用者形态各异，如果是统一处理，从物理页本身出发似乎更好：在page描述符中加入足够的字段，以描述页当前的状态，从这些状态中我们可以判断出它能否被回收、应该如何回收。这样在回收时，我们只需要基于page描述符去处理，而不用管是谁在使用它，它在逻辑上存在于何处。如果判断它可以回收，我们再通过某种方式获得使用者，减去它的持有，然后将页回收掉。为了能从page找到持有者（比如页表），我们需要建立的就是反向映射。

反向映射的具体分类、处理流程、细节我不想过多描述（直接参考书本或者源码最好），反向映射的出发点是值得关注的，这种机制简化了我们在管理内存时的操作和层次，使得页框回收只需要关注页框本身。

同时内存可以很大，page描述符数组项数可以很多，如果在回收时去遍历它，无疑会非常耗费时间。怎么办？通过链表将不同状态的页组织起来，在处理的时候顺着链表往下遍历就好，非常省工夫。

lru算法里，就有活动页和非活动页两个链表，页描述符里有三个重要的标志，lru、active、reference，描述了页的状态。在我们每一次访问页的时候，都会对这几个状态进行合理的更新，同时会将页在这几个链表中进行移动。

这种提前组织也反应了某些计算机科学的理念：首先是平摊代价问题，通过在单次访问做一些操作，使得在执行回收时大大减小，相当于代价被均匀掉了，而在计算机系统里，去峰是非常重要的，因为很多时候决定一个东西行不行的，不是平均起来行不行，而是在最差情况下能否在deadline限制之内。同时，这也类似于一个无序数组，如果我们要经常查找它，那随时保持其有序效率是最高的，会避免很多重复操作。


另外一个值得关注的点是，在计算机领域里有很多经验性问题，这些问题往往没有一个绝对的标准答案，而只能试一下，根据结果再来优化。当最后得到满意的结果时，我们往往也没法给出完整的理由，只能说在这种环境下，实际结果说明就是这个方法、这个参数好……

而页框回收就是这种问题之一，如何调节其中的参数，获得比较好的性能，这是一个工程问题……

另外还碰到一个有趣的小问题，既然页都在active或者inactive链表里了，那干嘛还要active标志？道理是这么个道理，但是当你拿到一个page描述符时，你怎么知道它在哪个链表里？怎么完成自我确认？所以标志还是要的……


***
## 进程间通信


### 管道

讨论管道有助于我们理解文件系统。

一个最简易版的管道可以如何实现？

管道的基本运行流程是：

* 我们需要两个文件，一个的打开方式是读，一个的打开方式是写，两个进程分别持有。两个文件指向同一个inode，inode有一片数据缓冲区buffer，可以循环使用。读者和写者密切配合，互相等待和睡眠，直到数据传输完成。

管道可以看作是一个特殊的文件，同样有打开、关闭、读、写操作，同样有fd、file、inode，但是它不对应磁盘文件，它的数据区也比较特殊，不支持随机读写，而只能顺序读数据流。

第一个问题是，管道作为一个文件，需要文件系统吗？

* 如果说文件是数据流，那文件系统是什么？文件系统可以有效地组织文件，使得我们可以通过文件名找到对应文件的inode，同时还可以管理文件系统整体中的数据，使得文件与文件之间不发生数据混乱……总之，文件系统可以视为一种对文件的组织管理。

* 那么文件系统对于任何文件都是必须的吗？似乎不然。如果我就是一个单独的文件，我有办法可以生成它（文件系统当然可以提供这个功能，但是绕开文件系统，我手动实现也是ok的吧？），我对它的读写也不会影响到其他任何东西，那我就是一个单独的文件，向外满足文件通用接口，那我似乎不用对它搞什么组织管理，我也可以不用文件系统？

* 所以，管道作为一个独立文件，只通过父子关系继承，不安装到系统目录，似乎可以独立存在，不搞文件系统？


《深入理解linux内核》中说，为了加速对于pipe的处理，实现了pipefs文件系统，但是没有安装到系统目录。为何可以加速？因为vfs的特殊处理吗？不太清楚。

***
通过pipefs，我们也可以看到文件系统的几种容易混淆的操作：注册、创建、安装。

* 注册对应的是文件系统类型，是将一种文件系统类型添加到内核中，这样才能在需要时创建，这就类似于我们在创建一个对象之前，先要声明对象的类。

* 创建则是创建一个确定类型的文件系统，要为它分配数据结构，要将它放到内核的文件系统管理链表里。核心数据结构是什么？通过get_sb()拿到的super_block。

* 安装呢？则是将该文件系统挂载在哪个位置，linux在这一方面极为灵活。像pipefs，就是没有安装点的，所以不存在命名空间里，也就无法被普通进程所访问。

能不能创建两个pipefs？当然可以，只要你开心，完全可以在init_pipe_fs()里，调用完register_filesystem()后，手动调用do_kern_mount()两次，传入不同的名字……（没试过，大概是可以的吧)

***
类似的一个问题是，当我们插入一个安装了ext3文件系统磁盘，系统建立一个设备文件，让我们可以访问到它。但是只有当我们用ext3将它安装到某个系统目录，我们才可以访问它其内的普通文件。

在这之前之后，系统中发生了什么变化？

* 当我们插入一个块设备时，系统将它作为一个块设备，加入了bdev特殊文件系统，同时建立了设备文件，让我们可以从系统目录中访问到它。但此时它是“设备文件”，也就是将它整体作为一个数据序列，是一个大文件，而没有展开为一个文件系统以及其内的各种目录和文件。

* 我们如何展开它呢？用对应的数据组织方式去解释它，也就是用某种文件系统规则去适配它、解析它，让其内的数据具有各自的含义——有的是元数据，有的是数据。于是它们有了更深一层的含义，成为了一个自洽的文件系统，其中的数据部分能重新组合牵连成线，组成新的数据序列，成为一个一个独立的文件。

内核如何具体地完成这个过程？

* 当我们执行mount -t ext2 /dev/fd0 /myfs，mount()系统调用会经过层层中转，到do_kern_mount(fstype, flags, name, data)，在它内部会调用该fstype所定义的get_sb，比如ext2_get_sb()。

* ext2_get_sb()只有一行代码，get_sb_bdev(fs_type, flags, dev_name, data, ext2_fill_super)，在其中，通过块设备读取sb块，再间接调用ext2自己定义的fill_super，最后得到一个完整的sb。为什么可以这么做？因为对各种文件系统，超级块总是在第二个块（第一个块是引导块），所以可以用通用的方式读出来，然后由文件系统自己定义的fill函数来解释（这个函数一般都很长……）

* 然后从这个安装点开始，新的目录树开始建立起来了，可以通过文件名在其中搜索到对应的目录项和inode，建立新的数据序列……

***
### IPC

（这一块还很模糊，也没有跟源码，简单过一下）

两个进程如何通信？信号机制是其中的一种，但是信号只有标志，不能携带数据。所以还要更多的机制来支持进程之间的数据交换。

这种数据交换有两个基本问题是必须考虑的：首先两者要能访问到同一块数据，我们可以称之为资源；其次两者要有一种同步机制，以保证一致性。

在内核中，同样有多个、多种执行流，同样需要做同步，比如自旋锁、信号量之类的。对于用户进程，信号量毫无疑问同样非常有吸引力——如果我们能定义一块能被多个进程访问的数据块，那我们是否也可以用信号量进行保护呢？每次访问都尝试持有，不成功则可以睡眠等待；释放的人则负责唤醒……

但是进程信号量还有一些麻烦，比如进程可能在持有信号量时被杀死，这样就永远无法释放了，比如在等待的时候被杀死，这样就没法up了等等。所以还需要提供一些异常情况下的回滚机制……

这种信号量如何实现？肯定需要内核的支持。内核如何支持？

***
内核定义了一套ipc资源体系，统一提供不同的ipc机制，比如信号量、消息、共享内存区，它们都是“资源”，其中都内嵌了kern_ipc_perm，可以认为它们都是是ipc体系中“资源接口”的派生，而对于资源管理的模式是通用的。


共享内存区一个很有意思的点是，它是基于vfs实现的，当两个进程要共享一片内存区时，可以建立一个vm_file，两者都创建一个线性区并映射到这个文件，而这个文件的数据页会放在页高速缓存里，也就是对一片共享内存区的访问，两个进程是通过一个虚拟文件的内存映射来完成的。

对于这个特殊的文件系统，它的file_operations只需要定义mmap，也就是如何完成内存映射，因为它从来都不会被直接read和write，对它的访问都是通过直接的线性地址访问实现的……


***
消息队列。

它的数据结构定义很有意思，正文接在数据头之后，在数据头里存有正文的长度，这样就可以支持数据长度可变了……

***

## 内核内存管理

### 非连续内存区

关于高端内存我现在没必要深究，因为在64位机器上根本就没这个问题。但是非连续内存区它作为一种映射高端内存的方式，意义并不在于如何使用高端内存，而是在于可以将非连续的物理页映射到连续的线性地址段，所以是值得被讨论的。

这种方式有什么好处和坏处？我们基于64位系统，也就是内核线性地址足够的前提下来讨论这个问题。

好处是可以避免外碎片，也就是内核在使用物理内存时，可以将碎屑也利用起来，避免将物理内存切割得过于零碎。

坏处呢？如果内核地址空间足够，我们完全可以将每一个物理地址页都直接映射到一个固定的线性地址，这样内核页表可以永远不变。但是这样做的话，就必须让连续的线性地址映射到连续的物理地址，有外碎片问题。

那如果我们用非连续内存区的方式呢？在避免外碎片的同时，我们也打乱了内核页表。当我们分配一个非连续内存区时，是需要修改内核页表的，这又会导致tlb刷新等问题。如果访问太频繁，还可能导致颠簸，不停地换进换出。


所以对非连续内存区的使用加以限制似乎就是自然的：我们应该只将内核访问不频繁的部分，放进非连续内存区里，比如模块的加载。


如何实现非连续内存区呢？

它的描述符数据结构是vm_struct，非常像进程地址空间中的vma……实际上它们从概念上而言真的很像。

当我们要使用一个非连续内存区时，要先查找内核的线性地址空间，确认有合适的线性地址区间可供使用，并返回vm_struct描述符，值得注意的是，它本身是用kmalloc分配的……

现在线性地址区间已经拿到了，类似于已经分配到了一个线性区，接下来呢？一次访问的关键有三：线性地址、物理页框、页表映射关系。

下一步就是调用alloc_page，一次一个页地从伙伴系统里拿物理页框，并且保存到自己的pages描述符数组里。

然后修改页表，将pages写到内核页表里去，这样就大功告成了。


和进程线性区不同的是，进程线性区，比如映射区，只说明了如何获得（比如定义了nopage方法）而没有实际载入，当然也不分配实际页框；内核的非连续内存区则是一次到位，把页框都拿到了，不过是先自己存起来，再填到页表中去。


***
### 关于模块

为何要搞模块？这样可以让内核尽可能地小，可以动态地增减内核功能等等，好处肯定是很多的。

它是如何实现的？当我们insmod时发生了什么？去看源代码。

关键有什么？

分配内存，将模块的elf文件加载到内存中，并映射到内核页表。

符号表。支持符号的导入导出。内核本身可能就持有了一个符号表，可以供所有模块使用，同时模块还可以使用其他模块导出的符号。这和装载、链接时的重定位过程应该是一致的。

而且，因为一个模块只会被载入一次，只会被内核直接使用，所以只需要对应一个线性地址，所以还不像用户程序的动态重定位那样要搞什么间接处理，直接根据重定位表，修改代码段应该就可以。处理过程应该是比较简单的。

***
## 睡眠、异常等

### 内核睡眠与异常

睡眠的本质就是被调度，进程不再继续执行，所以进程切换也是不允许的，也就是说，自旋锁的设计原则就是等待时间短，等待时间短其实也就是意味着每一个执行流的处理时间都很短。如果被调度，那就是不可控的无限等待时间了。

值得注意的是，在内核里，某些操作可能引起异常，从而陷入io等待，导致进程睡眠，那就是缺页异常。

内核本身从理论上来说是不会缺页的（比如slab没有页了就直接找伙伴系统拿，伙伴系统没有页了就直接释放，都不需要等待），但是当内核访问用户地址空间的时候，比如调用copy_to/from_user()的时候，就可能触发缺页异常了。

但实际上内核本身应该也会缺页，因为对内核页表的更改仅仅修改了init_mm，也就是task0的页表，其他进程的内核空间，都有自己的pgd（pud就是共用了），但是这种方式使得进程对内核地址空间的高端内存区的访问，或者是非连续内存区的访问（总之就是内核页表可能有更改区域的访问），就有可能触发缺页异常。

缺页异常是什么？就是要访问的线性地址所对应的页表项无效。

如果是用户态，如果地址落在合法的线性区内，那就要触发一次内存分配，如果是匿名区，直接分配一个空白页；如果是映射区，可能还有磁盘文件的读，这就涉及到io操作，可能导致当前进程的睡眠等待。

如果是内核态呢？在内核态的缺页异常里，需要睡眠吗？应该是不需要的。它只需要检查init_mm，更新页表项，然后重新访问就可以了，不需要涉及io操作，也不需要睡眠等待。


中断处理过程中，是不允许睡眠的，因为它本身不是一个调度单位。所以在中断处理过程里，是一定不能触发用户空间的缺页异常的，copy_to/from_user肯定是不能用的。那内核空间的缺页异常呢？我不确定……


回到自旋锁。自旋锁某些属性和中断类似，它同样不能陷入睡眠，不能被抢占、被调度，不能返回用户空间，自然，在它保护的区域里，用户空间的缺页异常也是不允许的。

自旋锁需要禁用中断吗？自旋锁对于抢占的禁用是自然完成的，但中断呢？

理论上来说是不需要的。因为中断的原则同样是快速进入快速退出而不会陷入睡眠，所以相当于在处理过程中增加了一小段，并不会影响当前的执行流。而且因为禁用掉了抢占，在中断进出时也并不会导致抢占的发生。

当然，如果保护的数据需要和中断共享，那就一定要禁用本地中断了，否则会导致死锁。


关于自旋锁还有一个很有意思但容易被忽略的点。

当我们试图访问一个自旋锁，但是陷入自旋等待时，说明什么？一定有另外一个cpu占有了它！因为本地cpu是不可能两次访问同一个自旋锁的。而根据自旋锁的设计原则，它的处理过程很短，自然它的等待时间也应该很短（只要没有大量的并发线程同时试图访问一个自旋锁，导致一直排队），所以我们可以期待，在不久的将来，对方cpu就会释放该自旋锁，让我方cpu可以继续往下。

其中的关键是：自旋锁忙时，占用者一定是正在处理的对方cpu！


本地cpu不可能两次访问同一个自旋锁，那有可能访问不同的自旋锁吗？比如在内核异常里试图持有一个自旋锁，但是陷入了忙等待，在这个时候中断发生了，该中断和其他中断共享了数据，所以它禁止了本地中断，并试图持有另一个自旋锁……

这应该是有可能发生的，甚至可能是常态。该中断甚至还有可能也要自旋等待，等对方cpu释放自旋锁后才开始执行（这个一定很快，因为对方cpu同样会禁用中断），己方cpu处理完中断后，回到内核异常处理流，继续试图持有自旋锁……

还可能在中断处理过程中，对方cpu已经释放了自旋锁，但是不巧的是，又被其他的某个cpu被占用掉了。所以我方继续自旋等待。


那如果有多个cpu在等待同一个自旋锁，会如何分配呢？先来后到？我想大概是碰运气吧，它的底层应该是一个atomic原子操作，谁占住了归谁。


***
### 等待队列和进程的睡眠唤醒

等待队列似乎天然地就和睡眠、调度、唤醒这些关联在了一起，但实际上这并不是等待队列本身所必须的。

等待队列本身所提供的机制是：

* 将某个wait加入等待队列头所标志的等待队列。wait中有处理的func和privatedata。
* wakeup时，依次遍历该队列，依次调用func。

睡眠sleep_on(wq)的过程是怎么样的？(摘抄之前笔记）

第一部分：
* 进程主动调用sleep_on(q)，进入睡眠流程。
* 建立一个wait，将current和default_wakeup_function放进去。
* 将wait加入q。
* 更改自己的state为非0。
* 调用schedule()。
* 在内部调用deactive_task()，触发dequeue()将自己出列，不再存在于runlist中。
* 切换为其他其他进程。

第二部分：
* 某个其他进程调用wake_up(q)，进入唤醒流程。
* 从头到位扫描等待队列q，获得其中的wait，并依次调用try_to_wake_up(wait->task)。
* 在其中，先更改task->state，再调用active_task()。
* 在其中，先recalc_task_prio()，再enqueue()入列，进程再次被加入runlist中。

第三部分：
* 原睡眠进程再次被调度到，从schedule()返回，继续往下执行。
* 将自己移出等待队列。
* 从sleep_on()返回，继续往下执行。

可以看到，它构造wait，将func赋值为唤醒函数，再将wait加入wq；之后是自己设置state，自己调用schedule()，完成进程的睡眠和切换。这些操作是和唤醒函数相呼应的。在被唤醒之后，还要自己将自己移出wq，完成善后工作。

所以等待队列只是为进程的睡眠等待唤醒提供了基础，但我们也可以用另外的方式使用它。比如手动初始化wait，func不设置为唤醒函数，而是某些需要做的工作do_work；wakeup时，会调用该函数，执行某些工作，当然也不涉及到进程的唤醒……

等待队列所面向的基本单元是某种“任务”，也就是func，而非某个进程。



那wait_event呢？其内部流程是怎么样的？

* 其接口为wait_event(wq, condition)，核心就是增加了条件判断。
* 必须确认condition为假才能进入睡眠，否则可能没人来唤醒。
* 因为从condition变为真、进程被唤醒，到进程得到调度、继续往下执行，之间会有一段时间，这个condition可能又变为假，所以在继续执行时要再次判断，如果为假则要继续睡眠等待。所以这里需要一个循环，直到继续执行时条件为真。

***
### 内核资源的申请和释放

在进程空间中，有哪些资源需要手动释放？

栈上的临时变量是不需要的，它会自动回收。堆上的则需要，否则就会导致内存泄漏。另外还有向操作系统申请的各种资源，同样需要手动释放。

这些资源虽然大部分（比如内存）都会在进程退出时自动归还给操作系统，但并不意味着我们就无需在意它的泄漏了。因为一个服务程序可能根本不会退出，那这种泄漏就会不断侵占系统资源，直至死机。


在内核中呢？

内核中的资源要更加小心，因为没有人再替它负责了，它任何的资源泄漏都是永久的。有哪些需要手动释放呢？比如像slab申请的（包括用kmalloc申请的）内存，就一定是要手动释放的。另外还有某些对象如inode，内核会负责释放它，但是需要使用者自行完成计数，也就是必须手动调用put。

有哪些资源不用管呢？

栈上的临时变量会随着函数的调用和返回自动清理，不需要管；全局变量、静态变量会一直存在，不需要也不能被回收，不用管……


***
## 文件系统的挂载

### 文件系统mount

在ext2中，如何mount一个新的文件系统？

从mount系统调用开始，转到sys_mount，再转到do_mount。

```c
long do_mount(char * dev_name, char * dir_name, char *type_page,
		  unsigned long flags, void *data_page)
{
    retval = path_lookup(dir_name, LOOKUP_FOLLOW, &nd);
    retval = do_new_mount(&nd, type_page, flags, mnt_flags,
                      dev_name, data_page);
    return retval;
}

/*
 * 分两步，创建该文件系统，根目录为mnt;将该文件系统挂载到nd指定的目录。
 */
static int do_new_mount(struct nameidata *nd, char *type, int flags,
			int mnt_flags, char *name, void *data)
{
    struct vfsmount *mnt;

    mnt = do_kern_mount(type, flags, name, data);

    return do_add_mount(mnt, nd, mnt_flags, NULL);
}


/*
 * 第一步：创建超级块对象，构建mnt对象
 */
struct vfsmount *
do_kern_mount(const char *fstype, int flags, const char *name, void *data)
{
    struct file_system_type *type = get_fs_type(fstype);
    struct vfsmount *mnt = alloc_vfsmnt(name);
    
    /* 调用文件系统的get_sb方法 */
    struct super_block *sb = type->get_sb(type, flags, name, data);

    mnt->mnt_sb = sb;
    mnt->mnt_root = dget(sb->s_root);
    mnt->mnt_mountpoint = sb->s_root;
    mnt->mnt_parent = mnt;
    mnt->mnt_namespace = current->namespace;
    up_write(&sb->s_umount);
    put_filesystem(type);
    return mnt;
}

/*
 * ext2:传入fill_super指针，走块设备通用流程
 */
static struct super_block *ext2_get_sb(struct file_system_type *fs_type,
	int flags, const char *dev_name, void *data)
{
    return get_sb_bdev(fs_type, flags, dev_name, data, ext2_fill_super);
}

/*
 * 块设备通用流程：
 * 打开块设备，通过sget获取超级块对象s；如果s_root字段为空，则调用fill_super初始化。
 * 为何这样？因为一个sb可能被挂载多次，我们不能重复创建。调用sget时，会返回一个sb对象，
 * 它可能是在系统中找到的，也可能是新创建的，我们可以根据sb->s_root字段是否为空来判断，
 * 如果它不为空，说明sb可以直接使用；如果为空，则用fill_super来初始化它。
 */
struct super_block *get_sb_bdev(struct file_system_type *fs_type,
	int flags, const char *dev_name, void *data,
	int (*fill_super)(struct super_block *, void *, int))
{
    struct block_device *bdev;
    struct super_block *s;
    int error = 0;

    bdev = open_bdev_excl(dev_name, flags, fs_type);
    /* 查找或者创建一个super_block对象，注意，它不负责填充字段 */
    s = sget(fs_type, test_bdev_super, set_bdev_super, bdev);

    if (s->s_root) {
        close_bdev_excl(bdev);
        return s;
    } else {
        /* 脏活累活，fill_super为文件系统自己定义的 */
        error = fill_super(s, data, flags & MS_VERBOSE ? 1 : 0);
        bdev_uevent(bdev, KOBJ_MOUNT);
    }
    return s;
}

/*
 * 寻找或者创建一个sb对象。
 * 对于每一种type，会通过type->fs_supers保存该类型下的所有sb，所以可以通过遍历它，
 * 调用test方法，来判断需要的sb是否已经存在了。对于块设备，test方法很简单，就是比较
 * 是否为同一个设备。
 * 如果sb已经存在，则直接返回；否则，申请一个新的sb对象，填入bdev等基本信息，加入到
 * type的supers管理链表，返回。此时super_block中的字段，几乎都是空的，还未从磁盘读取
 */
struct super_block *sget(struct file_system_type *type,
			int (*test)(struct super_block *,void *),
			int (*set)(struct super_block *,void *),
			void *data)
{
    struct super_block *s = NULL;
    struct list_head *p;
    int err;

    spin_lock(&sb_lock);
    if (test) list_for_each(p, &type->fs_supers) {
        struct super_block *old;
        old = list_entry(p, struct super_block, s_instances);
        if (!test(old, data))
            continue;
        return old;
    }
    if (!s) {
        spin_unlock(&sb_lock);
        s = alloc_super();
    }

    err = set(s, data);

    s->s_type = type;
    strlcpy(s->s_id, type->name, sizeof(s->s_id));
    list_add_tail(&s->s_list, &super_blocks);
    list_add(&s->s_instances, &type->fs_supers);
    spin_unlock(&sb_lock);

    return s;
}

static int set_bdev_super(struct super_block *s, void *data)
{
    s->s_bdev = data;
    s->s_dev = s->s_bdev->bd_dev;
    return 0;
}

static int test_bdev_super(struct super_block *s, void *data)
{
    return (void *)s->s_bdev == data;
}


/*
 * 一个非常长的函数。
 */
static int ext2_fill_super(struct super_block *sb, void *data, int silent)
{
    struct buffer_head * bh;
    struct ext2_sb_info * sbi;
    struct ext2_super_block * es;
    struct inode *root;
    unsigned long block;

    /* 初始化时可能需要多次从磁盘读数据块，会用到私有数据data */
    unsigned long sb_block = get_sb_block(&data);

    sbi = kmalloc(sizeof(*sbi), GFP_KERNEL);

    /* 注意，sb预留了文件系统私有数据指针s_fs_info */
    sb->s_fs_info = sbi;
    memset(sbi, 0, sizeof(*sbi));

    bh = sb_bread(sb, logic_sb_block)
    es = (struct ext2_super_block *) (((char *)bh->b_data) + offset);
    sbi->s_es = es;
    sb->s_magic = le16_to_cpu(es->s_magic);

    /* 重要任务之设置sbi的很多很多字段 */
    sbi->s_inode_size = EXT2_GOOD_OLD_INODE_SIZE;
    sbi->s_first_ino = EXT2_GOOD_OLD_FIRST_INO;

    memset(sbi->s_debts, 0, sbi->s_groups_count * sizeof(*sbi->s_debts));
    for (i = 0; i < db_count; i++) {
        block = descriptor_loc(sb, logic_sb_block, i);
        sbi->s_group_desc[i] = sb_bread(sb, block);
    }

    sbi->s_gdb_count = db_count;
    get_random_bytes(&sbi->s_next_generation, sizeof(u32));
    spin_lock_init(&sbi->s_next_gen_lock);


    /* 重要任务之设置s_op */
    sb->s_op = &ext2_sops;
    sb->s_export_op = &ext2_export_ops;
    sb->s_xattr = ext2_xattr_handlers;

    /* 重要任务之初始化root，返回时root已经ok了*/
    root = iget(sb, EXT2_ROOT_INO);
    sb->s_root = d_alloc_root(root);

    ext2_setup_super (sb, es, sb->s_flags & MS_RDONLY);

    return 0;
}


/*
 * 第二步：将文件系统挂载到目录树上去，将mnt和nd关联起来
 */
int do_add_mount(struct vfsmount *newmnt, struct nameidata *nd,
		 int mnt_flags, struct list_head *fslist)
{
    int err;

    down_write(&current->namespace->sem);

    newmnt->mnt_flags = mnt_flags;
    /* 其内会调用attach_mnt() */
    err = graft_tree(newmnt, nd);

    up_write(&current->namespace->sem);
    mntput(newmnt);
    return err;
}


static void attach_mnt(struct vfsmount *mnt, struct nameidata *nd)
{
    mnt->mnt_parent = mntget(nd->mnt);
    mnt->mnt_mountpoint = dget(nd->dentry);

    //注意，每次都会插入链表头部，所以后安装的会隐藏之前的
    list_add(&mnt->mnt_hash, mount_hashtable+hash(nd->mnt, nd->dentry));
    list_add_tail(&mnt->mnt_child, &nd->mnt->mnt_mounts);

    //标志该目录，在遍历的时候可以用于判断，如果是安装点，就通过哈希表寻找
    nd->dentry->d_mounted++;
}

/*
 * 搜索辅助数据结构nameidata
 */
struct nameidata {
    struct dentry	*dentry;
    struct vfsmount *mnt;
    struct qstr	last;
    unsigned int	flags;
    int		last_type;
    unsigned	depth;
    char *saved_names[MAX_NESTED_LINKS + 1];

    /* Intent data */
    union {
        struct open_intent open;
    } intent;
};

/*
 * 文件系统挂载数据结构
 */
struct vfsmount
{
    struct list_head mnt_hash;
    struct vfsmount *mnt_parent;	/* fs we are mounted on */
    struct dentry *mnt_mountpoint;	/* dentry of mountpoint */
    struct dentry *mnt_root;	/* root of the mounted tree */
    struct super_block *mnt_sb;	/* pointer to superblock */
    struct list_head mnt_mounts;	/* list of children, anchored here */
    struct list_head mnt_child;	/* and going through their mnt_child */
    atomic_t mnt_count;
    int mnt_flags;
    int mnt_expiry_mark;		/* true if marked for expiry */
    char *mnt_devname;		/* Name of device e.g. /dev/dsk/hda1 */
    struct list_head mnt_list;
    struct list_head mnt_fslink;	/* link in fs-specific expiry list */
    struct namespace *mnt_namespace; /* containing namespace */
};

struct dentry {
    atomic_t d_count;
    unsigned int d_flags;		/* protected by d_lock */
    spinlock_t d_lock;		/* per dentry lock */
    struct inode *d_inode;		/* Where the name belongs to - NULL is
                     * negative */
    /*
     * The next three fields are touched by __d_lookup.  Place them here
     * so they all fit in a 16-byte range, with 16-byte alignment.
     */
    struct dentry *d_parent;	/* parent directory */
    struct qstr d_name;

    struct list_head d_lru;		/* LRU list */
    struct list_head d_child;	/* child of parent list */
    struct list_head d_subdirs;	/* our children */
    struct list_head d_alias;	/* inode alias list */
    unsigned long d_time;		/* used by d_revalidate */
    
    /* 目录项操作 */
    struct dentry_operations *d_op;
    struct super_block *d_sb;	/* The root of the dentry tree */
    void *d_fsdata;			/* fs-specific data */
    struct rcu_head d_rcu;
    struct dcookie_struct *d_cookie; /* cookie, if any */
    struct hlist_node d_hash;	/* lookup hash list */	
    
    /* 注意，挂载是发生在内存中的，和磁盘中的文件系统无关 */
    int d_mounted;
    
    /* 长度不定 */
    unsigned char d_iname[DNAME_INLINE_LEN_MIN];	/* small names */
};

```

***
**nameidata**

打开一个文件的过程里，核心的就是从filename到inode，因为文件是树状分布的，所以这是一个递归向下的过程。如果不做任何特殊处理，这可能导致多次访问磁盘（每次要从目录文件中获得该文件夹内文件的name - inode映射，都需要先从磁盘读取目录文件的数据块）。

所以引入dentry是自然的，将这些映射关系存放在内存的目录项缓存中，可以极大地加速这个过程。

但另外一个问题是，在文件系统的搜索遍历之中，dentry足够吗？因为在vfs的支持下，文件系统是可以互相挂载的，所以我们不仅仅需要指导当前正在处理的dentry，还需要知道它所属的文件系统。描述已经挂载的文件系统的数据结构就是vfs_mount。

而nameidata是一种辅助数据结构，在遍历过程里，可以保存当前已经到达的dentry以及它所属的vfsmount。所以随着遍历的进行，随着文件路径的展开，这两者都是不断变化的。

比如对于一个路径：

/mnt/myfs/myfils/file

根文件系统为vfsmount1，myfs是我们挂载的文件系统vfsmount2。

最开始的时候，在nameidata里，分别是vfsmount1和root；之后访问到mnt，dentry变为mnt；访问到myfs时，发现它是一个挂载点，其中挂载了vfsmount2，所以会自动切换到vfsmount2的root2，nameidata就变为了vfsmount2和root2，而非myfs了；再往下，dentry会变为myfils和file，最后确定为vfsmount2和file。

***
**vfsmount**

在vfsmount中有一些什么字段？和sb有什么关系？

对于一个已经创建并挂载的文件系统，两者都是必须的。sb描述了文件系统本身的数据组织情况，而vfsmount则描述挂载信息。两者的关系有些类似于inode和file。

所在在vfsmount的字段中，除了有和它相关的sb、root等基本信息之外，还有mount的层次关系、namespace等信息。

一个inode可以对应多个file，同样，一个sb可以对应多个vfsmount。file可以认为包含了交互信息，是用户打开了的文件对象；而vfsmount也可以认为包含了某种交互信息，是被挂载了的文件系统。

***
### 根节点root


root节点其实是一个很神奇的存在。

在mount过程中，当我们attach，将newmnt和nd（vfsmount、dentry）关联起来时，是将该文件系统挂载到了这个目录下，之后访问该目录时，会自动跳转到新文件系统的root节点去。

root节点代表了什么？它是一个文件系统的根部，每一个sb对象都必须有它，它是访问该文件系统文件的必经路径，它天然地就是一个目录。

要仔细体会这些概念之间的关系：文件系统类型、文件系统、超级块对象、vfsmount对象、根节点……

尤其神奇的是，一个文件系统的根节点，也可以拿来挂载其他的文件系统，还可以多次挂载。这样的话，原本文件系统的所有文件都被遮蔽了，会直接访问到最新挂载的文件系统，看上去好像就只有它一样。linux的根文件系统初始化就是这样干的。好玩的是，这是遮蔽而不是覆盖。所以当挂载的文件系统被一一卸载，被遮蔽的就会逐渐显现。



```c

/*
 * 上一节提到，创建root有两步：
 * 调用iget()拿到root inode；
 * 调用d_alloc_root()初始化root的dentry。
 */


/*
 * 第一步，iget，查找或者创建一个inode。因为内核为inode建立了缓存，所以需要先到hash表里查，
 * 有就直接使用，没有再alloc一个新的，触发sb的read_inode，比如从磁盘读。
 * 同时，该操作还会增加inode的计数，这是通过底层调用的__iget()实现的。
 */
static inline struct inode *iget(struct super_block *sb, unsigned long ino)
{
	struct inode *inode = iget_locked(sb, ino);
	
	if (inode && (inode->i_state & I_NEW)) {
		sb->s_op->read_inode(inode);
		unlock_new_inode(inode);
	}

	return inode;
}

struct inode *iget_locked(struct super_block *sb, unsigned long ino)
{
    /* hash的日常使用方式，通过hash(sb,ino)计算得到slot index，拿到head */
    struct hlist_head *head = inode_hashtable + hash(sb, ino);
    struct inode *inode;

    /* 可以想象到，这是一个遍历链表，比较ino的过程 */
    inode = ifind_fast(sb, head, ino);
    if (inode)
        return inode;
    
    /* get一个新的inode，因为可能睡眠，所以要判断是否重复创建了 */
    return get_new_inode_fast(sb, head, ino);
}

/* 
 * 初始化root目录项对象。
 * 设置其名字为“/”，设置其sb。
 * 设置其parent为自身，这样就不能再往上查找了，除非是穿越挂载点。
 * 通过调用d_instantiate关联inode与dentry，因为一个sb可以被多次挂载，对应到多个
 * vfsmount，那一个root inode当然就可能对应到多个dentry，函数的名字为“实例化”……
 */
struct dentry * d_alloc_root(struct inode * root_inode)
{
    struct dentry *res = NULL;

    if (root_inode) {
        static const struct qstr name = { .name = "/", .len = 1 };

        res = d_alloc(NULL, &name);
        if (res) {
            res->d_sb = root_inode->i_sb;
            res->d_parent = res;
            d_instantiate(res, root_inode);
        }
    }
    return res;
}

/*
 * 从中可以看出inode与dentry的关系：
 * inode可以对应多个dentry，inode有i_dentry list；dentry有d_inode字段。
 * 由dentry维护目录的上下级关系，对应到磁盘上，目录文件数据块中的dentry。
 * 注意！inode本身的字段中，是没有维护文件层次关系的！
 */
void d_instantiate(struct dentry *entry, struct inode * inode)
{
	if (!list_empty(&entry->d_alias)) BUG();
	spin_lock(&dcache_lock);
	if (inode)
		list_add(&entry->d_alias, &inode->i_dentry);
	entry->d_inode = inode;
	spin_unlock(&dcache_lock);
	security_d_instantiate(entry, inode);
}


```

***



