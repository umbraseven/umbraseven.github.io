---
layout: post
title:  "Linux2.6：综合主题漫谈"
date:   2020-06-17 16:30:00 +0800
categories: Linux
tags: Linux 
description: 
---

*  目录
{:toc}

***

## Hello,World!

编写一个c程序，输出“Hello,world!”到控制台。这个中间发生了一些什么？我们可以谈到到哪些话题？

### 准备

**编写一个c程序文件**

* hello.c文件：vfs，ext2/3，super_block、inode、dentry、file对象，各种对象的ops，通用接口read/write/open等的实现……

* 进程的io等待：睡眠、唤醒、等待队列；调度，时间片，优先级，进程状态……

* 磁盘的页高速缓存：sb的缓冲区高速缓存，脏页的写回。

* 磁盘的访问：通用块层、逻辑块设备、io调度、request、电梯算法、块设备驱动、设备驱动模型、sysfs、设备文件……

* 磁盘中断：中断处理函数，irq，内核的执行逻辑流，中断的通用处理流程，同步与保护，上半部和下半部……

* 内存：inode等对象的slab缓存，页高速缓存的页申请，伙伴系统，页框回收……

***
**编译**

（了解得很浅，简单过一下）

* 预处理：纯文本处理，其中应该同样有语法分析、词法分析等过程，从广义而言应该也是属于一次编译，只是输入输出均为文本。

* 词法分析：从字符流变为token集合。原理可以是有限状态自动机……

* 语法分析：建立抽象语法树，方法大概有两类，从左至右的回溯推导，和从右至左的移进规约，两者都可以加入前看符号来尽可能剪枝。还有一个隐含的主题是语法定义。另外通过表驱动，规定一定的语法输入格式，能写一个可以生成语法分析程序的程序……

* 语义分析：摆脱源码，基于ast分析，典型的就有类型检查。这一步似乎可以做很多事。

* 中间代码生成和各种优化：比如某些表达式的值静态推导，某些路径的优化等等。

* 汇编代码生成：

* 目标文件生成：除了code正文之外，还有许多其他的段，比如data、bss、符号表、重定位表等等。这是一个完整的elf文件，有文件头，段表等。

* 链接成可执行目标文件：静态链接，符号决议，重定位，生成elf文件。

***
### 执行

**开始执行**

* shell中输入命令hello，开始查找可执行的目标文件（其中有大量的文件系统、io操作）。

* fork()。创建一个子进程，父进程等待它执行完成。初始化必要的数据结构，比如task_struct、mm和页表等等，写时复制，共享页框。

* 子进程被调度。开始执行exec()。清理很多从父进程继承的资源，装载目标文件，重建虚拟地址空间，初始化各种线性区，建立执行文件的内存映射，修改用户态返回地址，布置堆栈等等。

* 共享库装载。动态链接程序，共享库的线性区映射，动态重定位，动态链接等，一切准备就绪，返回用户态，从入口点开始执行。

* 缺页异常。各种检查，线性区nopage方法，读文件，inode的address_space缓存，页表项修改，再次访问。（文件读，再一次穿过vfs、filesystem、cache、块设备处理层、io调度层、设备驱动、端口、总线、设备……）

***
**一条指令**

比如mov %eax ,(0xfffffff)（瞎写的，大意是从内存某个地址读一个int到eax寄存器）

寻址过程：

* 逻辑地址到线性地址：输入为逻辑地址，默认的段寄存器为ds，根据其中的段选择子，访问对应的gdt表项，做权限判断，得到段基址，加上逻辑地址偏移，生成线性地址。

* 线性地址到物理地址：开启分页模式后，所有的地址访问都是基于线性地址，通过mmu单元完成的。pg0地址存放在cr控制寄存器中。线性地址通过页表的逐级转换，最后变为物理地址，放到地址总线，访问内存。中间还有tlb加速页表项的寻址处理。


执行过程：

* 分阶段：

* 流水线：

* 状态机本质：

* 堆栈、寄存器、cpu的配合：

***
## 页框回收

想回收页框，要问的第一个问题是：在任意时刻，内存是如何被使用的？如何分布的？它可能被哪些对象占用？可以被释放吗？对象将来还需要使用它吗？内存是如何被内核组织和管理起来的？入口在哪里？如果要释放，需要修改哪些数据结构才能维持自洽？

所以页框回收是一个综合性问题。

***
### 内存分布

**内核如何使用内存？**

内核需要维持大量的数据结构，来管理进程，管理各种资源，典型的有进程描述符、mm和线性区描述符、页表、信号、中断子系统、调度子系统、文件系统等等，这些数据都是由内核管理，存放在内核空间。这些数据也许可以分为两类：

* 一类是独立于某个进程而存在的，典型的是设备和文件、物理内存page描述符、中断等。
* 一类是描述进程和进程所拥有的资源的，它们会随着进程的创建和终止而动态变化。

内核对内存的使用方式主要有两种，一种是页，一种是slab对象。页的分配是由伙伴系统来管理的，slab缓存同样是从伙伴系统里拿页的。

从页的最终去向上，可能已经被slab层拿来装对象了，可能还在slab中没有使用，可能在页高速缓存了，另外还有静态分配的内核代码和数据，还有内核的保留页和特别专用页，哪些页可以被回收？

* 怎么才叫回收？如果页是空的，或者信息无效，直接丢弃就可以。如果页中的信息有效，且能对应到磁盘，那就只需要写回磁盘，释放脏页。如果页中的信息有效，且没有磁盘文件对应，那就不能同步也不能丢弃了，但是可以交换到磁盘，本质上就是强行弄个文件给它装着，需要的时候再读进来。

* 在内核中，作为slab对象分配出去的内存，其中装的是各种内核数据结构，说不定什么时候就要使用，而且在很多内核执行流里，是不允许触发缺页异常的，自然也不能临时走io去磁盘读，阻塞当前的执行流，所以这一部分内存是不能交换出去，不能回收的。

* 另外的内核专用的，比如保留页、内核态堆栈页，比如内核的静态数据和代码区等，页都不能回收。

* 页高速缓存中的页呢？应该大都是可以同步和回收的，比如其中映射到文件，或者映射到块设备缓冲区的，都可以将数据同步到磁盘，然后释放该页。

* slab中还没有使用的页呢？直接拿去用就好。还有其他的一些缓存，比如目录项、inode的应该也可以回收。

***
**进程空间**

因为进程可能长时间睡眠，而且进程数量可能开很多，所以从原则上来说，进程所占用的页应该全部可以回收。

进程会通过哪些方式占用内存？进程地址空间是通过线性区来管理的，线性区有匿名区和映射区，对于映射区，可以同步后释放，比如可执行文件、共享库等的内存映射区；对于匿名区，可以交换后释放，典型的就是堆栈数据。

***
**关键点**

不管是内核还是进程，要想动态拿页，不管通过哪个入口、何种方式，不管中间多么曲折，最终都是要向伙伴系统申请的，它的基本单位一定是页。

比如进程，分配页的时机有哪些？

* 映射区访问，页表项无效，触发缺页异常，在内核的异常处理函数中，申请一个页（可能是共享），读进数据，然后将页表项映射过去。

* 匿名区扩展。增加堆线性区边界，等到访问时，缺页异常但地址合法，于是申请一个空白页。

所以，都是经过缺页异常，从伙伴系统手里，一次一个页拿到的。

***
内核呢？分配页的时机有哪些？

* 一种是直接的拿整页，甚至一堆整页，然后自行规划，比如page描述符大数组，或者载入比较大的模块时，应该是要拿到连续页框的。又比如页高速缓存，那就是一次一个页了。

* 更多的一种呢，是各种内核数据结构的申请。在用户态进程中，如果在堆中申请这种对象，其实并不会触发一次内存分配，而仅仅是线性地址分配。在内核中没有人提供这些服务，所以就需要有slab缓存了，而slab在空间不够时，会一次从伙伴系统里拿多个页。

当然在内核初始化时，要确定哪些页可以交给伙伴系统管理，还要各种分区（zone），确定水位、保留区等等，这样的细节非常多。

但是总体图景大概如上讨论，这样随着系统的运行，随着进程的创建和终止，文件的打开和关闭，我们也许能大概地有一个把握，内存中有哪些东西出现，有哪些东西消失，内存的大致分布，其中各种成分的拥有者、比例、性质、将来的预期……

至于具体的页框回收策略、原则、算法、流程等，在对内存管理有一定整体把握的情况下，应该是不难理解的。

一个具体的lru算法，仅仅是设计的结果，我们读它只能知其然。但它为何如何设计？设计者在设计它时所面对的、基于的是什么？他做了哪些思考？在我们能看到的选项A的背后，是否曾丢弃了选项BCDEF呢？设计者在每一步、每一个细节里，都面临了哪些选择，又为何会如此选择？

我想这些思考，对深入研究一个复杂系统而言是必须的。当然我现在还远远没有能力去深究这些，所以还只是纸上谈兵，做一些有趣的瞻望。

***
### 反向映射

如果知道了内存页的当前状态，那如何才能找到它们并完成释放？有两条路径：一种是先找到物理页的使用者，从它们手里拿回来再释放掉；一种是从物理页出发，释放可以释放的，再想办法告知使用者。

因为使用者形态各异，如果是统一处理，从物理页本身出发似乎更好：在page描述符中加入足够的字段，以描述页当前的状态，从这些状态中我们可以判断出它能否被回收、应该如何回收。这样在回收时，我们只需要基于page描述符去处理，而不用管是谁在使用它，它在逻辑上存在于何处。如果判断它可以回收，我们再通过某种方式获得使用者，减去它的持有，然后将页回收掉。为了能从page找到持有者（比如页表），我们需要建立的就是反向映射。

反向映射的具体分类、处理流程、细节我不想过多描述（直接参考书本或者源码最好），反向映射的出发点是值得关注的，这种机制简化了我们在管理内存时的操作和层次，使得页框回收只需要关注页框本身。

同时内存可以很大，page描述符数组项数可以很多，如果在回收时去遍历它，无疑会非常耗费时间。怎么办？通过链表将不同状态的页组织起来，在处理的时候顺着链表往下遍历就好，非常省工夫。

lru算法里，就有活动页和非活动页两个链表，页描述符里有三个重要的标志，lru、active、reference，描述了页的状态。在我们每一次访问页的时候，都会对这几个状态进行合理的更新，同时会将页在这几个链表中进行移动。

这种提前组织也反应了某些计算机科学的理念：首先是平摊代价问题，通过在单次访问做一些操作，使得在执行回收时大大减小，相当于代价被均匀掉了，而在计算机系统里，去峰是非常重要的，因为很多时候决定一个东西行不行的，不是平均起来行不行，而是在最差情况下能否在deadline限制之内。同时，这也类似于一个无序数组，如果我们要经常查找它，那随时保持其有序效率是最高的，会避免很多重复操作。


另外一个值得关注的点是，在计算机领域里有很多经验性问题，这些问题往往没有一个绝对的标准答案，而只能试一下，根据结果再来优化。当最后得到满意的结果时，我们往往也没法给出完整的理由，只能说在这种环境下，实际结果说明就是这个方法、这个参数好……

而页框回收就是这种问题之一，如何调节其中的参数，获得比较好的性能，这是一个工程问题……

另外还碰到一个有趣的小问题，既然页都在active或者inactive链表里了，那干嘛还要active标志？道理是这么个道理，但是当你拿到一个page描述符时，你怎么知道它在哪个链表里？怎么完成自我确认？所以标志还是要的……


***
## 进程间通信


### 管道

讨论管道有助于我们理解文件系统。

一个最简易版的管道可以如何实现？

管道的基本运行流程是：

* 我们需要两个文件，一个的打开方式是读，一个的打开方式是写，两个进程分别持有。两个文件指向同一个inode，inode有一片数据缓冲区buffer，可以循环使用。读者和写者密切配合，互相等待和睡眠，直到数据传输完成。

管道可以看作是一个特殊的文件，同样有打开、关闭、读、写操作，同样有fd、file、inode，但是它不对应磁盘文件，它的数据区也比较特殊，不支持随机读写，而只能顺序读数据流。

第一个问题是，管道作为一个文件，需要文件系统吗？

* 如果说文件是数据流，那文件系统是什么？文件系统可以有效地组织文件，使得我们可以通过文件名找到对应文件的inode，同时还可以管理文件系统整体中的数据，使得文件与文件之间不发生数据混乱……总之，文件系统可以视为一种对文件的组织管理。

* 那么文件系统对于任何文件都是必须的吗？似乎不然。如果我就是一个单独的文件，我有办法可以生成它（文件系统当然可以提供这个功能，但是绕开文件系统，我手动实现也是ok的吧？），我对它的读写也不会影响到其他任何东西，那我就是一个单独的文件，向外满足文件通用接口，那我似乎不用对它搞什么组织管理，我也可以不用文件系统？

* 所以，管道作为一个独立文件，只通过父子关系继承，不安装到系统目录，似乎可以独立存在，不搞文件系统？


《深入理解linux内核》中说，为了加速对于pipe的处理，实现了pipefs文件系统，但是没有安装到系统目录。为何可以加速？因为vfs的特殊处理吗？不太清楚。

***
通过pipefs，我们也可以看到文件系统的几种容易混淆的操作：注册、创建、安装。

* 注册对应的是文件系统类型，是将一种文件系统类型添加到内核中，这样才能在需要时创建，这就类似于我们在创建一个对象之前，先要声明对象的类。

* 创建则是创建一个确定类型的文件系统，要为它分配数据结构，要将它放到内核的文件系统管理链表里。核心数据结构是什么？通过get_sb()拿到的super_block。

* 安装呢？则是将该文件系统挂载在哪个位置，linux在这一方面极为灵活。像pipefs，就是没有安装点的，所以不存在命名空间里，也就无法被普通进程所访问。

能不能创建两个pipefs？当然可以，只要你开心，完全可以在init_pipe_fs()里，调用完register_filesystem()后，手动调用do_kern_mount()两次，传入不同的名字……（没试过，大概是可以的吧)

***
类似的一个问题是，当我们插入一个安装了ext3文件系统磁盘，系统建立一个设备文件，让我们可以访问到它。但是只有当我们用ext3将它安装到某个系统目录，我们才可以访问它其内的普通文件。

在这之前之后，系统中发生了什么变化？

* 当我们插入一个块设备时，系统将它作为一个块设备，加入了bdev特殊文件系统，同时建立了设备文件，让我们可以从系统目录中访问到它。但此时它是“设备文件”，也就是将它整体作为一个数据序列，是一个大文件，而没有展开为一个文件系统以及其内的各种目录和文件。

* 我们如何展开它呢？用对应的数据组织方式去解释它，也就是用某种文件系统规则去适配它、解析它，让其内的数据具有各自的含义——有的是元数据，有的是数据。于是它们有了更深一层的含义，成为了一个自洽的文件系统，其中的数据部分能重新组合牵连成线，组成新的数据序列，成为一个一个独立的文件。

内核如何具体地完成这个过程？

* 当我们执行mount -t ext2 /dev/fd0 /myfs，mount()系统调用会经过层层中转，到do_kern_mount(fstype, flags, name, data)，在它内部会调用该fstype所定义的get_sb，比如ext2_get_sb()。

* ext2_get_sb()只有一行代码，get_sb_bdev(fs_type, flags, dev_name, data, ext2_fill_super)，在其中，通过块设备读取sb块，再间接调用ext2自己定义的fill_super，最后得到一个完整的sb。为什么可以这么做？因为对各种文件系统，超级块总是在第二个块（第一个块是引导块），所以可以用通用的方式读出来，然后由文件系统自己定义的fill函数来解释（这个函数一般都很长……）

* 然后从这个安装点开始，新的目录树开始建立起来了，可以通过文件名在其中搜索到对应的目录项和inode，建立新的数据序列……

***
### IPC

（这一块还很模糊，也没有跟源码，简单过一下）

两个进程如何通信？信号机制是其中的一种，但是信号只有标志，不能携带数据。所以还要更多的机制来支持进程之间的数据交换。

这种数据交换有两个基本问题是必须考虑的：首先两者要能访问到同一块数据，我们可以称之为资源；其次两者要有一种同步机制，以保证一致性。

在内核中，同样有多个、多种执行流，同样需要做同步，比如自旋锁、信号量之类的。对于用户进程，信号量毫无疑问同样非常有吸引力——如果我们能定义一块能被多个进程访问的数据块，那我们是否也可以用信号量进行保护呢？每次访问都尝试持有，不成功则可以睡眠等待；释放的人则负责唤醒……

但是进程信号量还有一些麻烦，比如进程可能在持有信号量时被杀死，这样就永远无法释放了，比如在等待的时候被杀死，这样就没法up了等等。所以还需要提供一些异常情况下的回滚机制……

这种信号量如何实现？肯定需要内核的支持。内核如何支持？

***
内核定义了一套ipc资源体系，统一提供不同的ipc机制，比如信号量、消息、共享内存区，它们都是“资源”，其中都内嵌了kern_ipc_perm，可以认为它们都是是ipc体系中“资源接口”的派生，而对于资源管理的模式是通用的。


共享内存区一个很有意思的点是，它是基于vfs实现的，当两个进程要共享一片内存区时，可以建立一个vm_file，两者都创建一个线性区并映射到这个文件，而这个文件的数据页会放在页高速缓存里，也就是对一片共享内存区的访问，两个进程是通过一个虚拟文件的内存映射来完成的。

对于这个特殊的文件系统，它的file_operations只需要定义mmap，也就是如何完成内存映射，因为它从来都不会被直接read和write，对它的访问都是通过直接的线性地址访问实现的……


***
消息队列。

它的数据结构定义很有意思，正文接在数据头之后，在数据头里存有正文的长度，这样就可以支持数据长度可变了……

***





